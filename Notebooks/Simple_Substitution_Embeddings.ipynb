{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Substitution\n",
    "`w266 Final Project: Crosslingual Word Embeddings`\n",
    "\n",
    "The code in this notebook was used to develop an algorithm to generate crosslingual word embeddings by training on a monolingual corpus and substituting translations at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set base paths depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Maya's paths\n",
    "BASE = '/home/mmillervedam/' #'/Users/mmillervedam/Documents/MIDS/w266' #\n",
    "PROJ = '/home/mmillervedam/ProjectRepo' #'/Users/mmillervedam/Documents/MIDS/w266/FinalProject'#\n",
    "\n",
    "## Roseanna's paths\n",
    "BASE = '/home/rhopper/'\n",
    "PROJ = '/home/rhopper/W266-Fall-2017-Final-Project'\n",
    "\n",
    "## Mona's local paths\n",
    "#BASE = '/Users/mona/OneDrive/repos/Data' #'/home/mmillervedam/Data'\n",
    "#PROJ = '/Users/mona/OneDrive/repos/final_proj/W266-Fall-2017-Final-Project'#'/home/mmillervedam/ProjectRepo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Repo paths\n",
    "FPATH_EN = BASE + '/Data/test/wiki_en_10K.txt' # first 10000 lines from wiki dump\n",
    "FPATH_ES = BASE + '/Data/test/wiki_es_10K.txt' # first 10000 lines from wiki dump\n",
    "EN_ES_DICT = PROJ +'/XlingualEmb/data/dicts/en.es.panlex.all.processed'\n",
    "EN_IT_DICT  = PROJ +'/XlingualEmb/data/dicts/en.it.panlex.all.processed'\n",
    "EN_IT_RAW = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'\n",
    "\n",
    "## Large datasets\n",
    "FULL_EN_ES = \"/home/miwamoto/shuffled_files/en_es_shuf.txt\"\n",
    "FULL_EN_IT = \"/home/miwamoto/shuffled_files/en_it_shuf.txt\"\n",
    "FULL_EN = BASE + '/Data/en/full.txt'\n",
    "FULL_ES = BASE + '/Data/es/full.txt'\n",
    "FULL_IT = BASE + '/Data/it/full.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory to save pickled embeddings\n",
    "SAVE_TO = PROJ + '/Notebooks/trained'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Preprocess Data\n",
    "__`ORIGINAL AUTHORS SAY:`__ \"Normally, the monolingual word embeddings are trained on billions of words. However, getting that much of monolingual data for a low-resource language is also challenging. That is why we only select the top 5 million sentences (around 100 million words) for each language.\" - _Section 5.1, Duong et. al._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import Corpus, BilingualVocabulary, batch_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load corpus\n",
    "en_it_data = Corpus(EN_IT_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20000  430887 3746786 /home/mmillervedam/ProjectRepo/XlingualEmb/data/mono/en_it.shuf.10k\r\n"
     ]
    }
   ],
   "source": [
    "# Corpus Stats\n",
    "!wc {EN_IT_RAW}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179520"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many batches is 20 epochs? (w/ batch size 48)\n",
    "430887 // 48 * 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`i.e.:`__ 20K sentences (10K in each language) with ~430K tokens\n",
    "> So this must not be their full data For now, I'm just going to look at the top 20K words and see what happens. In reality we should probably modify the Vocab class so that it explicily collects the top words for each language separately and then concatenates the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading english-italian dictionary\n",
    "pld = pd.read_csv(EN_IT_DICT, sep='\\t', names = ['en', 'it'], dtype=str)\n",
    "en_set = set(pld.en.unique())\n",
    "it_set = set(pld.it.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: 266450\n",
      "IT: 258641\n"
     ]
    }
   ],
   "source": [
    "# dictionary vocab lengths:\n",
    "print('EN:', len(en_set))\n",
    "print('IT:', len(it_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary for ease of runtime translation\n",
    "# WARNING this takes a sec to run\n",
    "bi_dict = pld.groupby(['en'])['it'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add other direction\n",
    "# WARNING this takes another sec to run\n",
    "bi_dict.update(pld.groupby(['it'])['en'].unique().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it_aggirare', 'it_andai', 'it_andara', 'it_andare',\n",
       "       'it_andare_avanti'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo en to it\n",
    "bi_dict['en_go'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en_adieu', 'en_bye-bye', 'en_bye', 'en_cheerio', 'en_ciao'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo it to en\n",
    "bi_dict['it_ciao'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train multilingual Vocabulary\n",
    "en_it_vocab = BilingualVocabulary(en_it_data.gen_tokens(), \n",
    "                                  languages = ('en','it'), \n",
    "                                  size = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48579"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of corpus vocabulary\n",
    "en_it_vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BilingualVocabulary' object has no attribute 'lang1_start_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-fa558b6b15ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# transition between words in the bilingual vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0midx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_it_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang1_start_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0midx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_it_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang2_start_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0men_it_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_it_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BilingualVocabulary' object has no attribute 'lang1_start_idx'"
     ]
    }
   ],
   "source": [
    "# transition between words in the bilingual vocab\n",
    "idx1 = en_it_vocab.lang1_start_idx\n",
    "idx2 = en_it_vocab.lang2_start_idx\n",
    "print(idx1 - 1 , en_it_vocab.index[idx1 - 1])\n",
    "print(idx1, en_it_vocab.index[idx1])\n",
    "print(idx2 - 1 , en_it_vocab.index[idx2 - 1])\n",
    "print(idx2, en_it_vocab.index[idx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13447"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overlap with dictionary vocabulary\n",
    "len([w for w in en_it_vocab.types if w in bi_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Question for the group`__*Seems low?* Will we limit our vocab to the words in the dict? (MI)\n",
    "\n",
    "#### Sample of orphaned words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_manuscripts\n",
      "en_syro-hittite\n",
      "en_western-hemisphere\n",
      "en_migrating\n",
      "en_conraua\n",
      "en_aretē\n",
      "en_tukey\n",
      "en_trustees\n",
      "en_240\n",
      "en_241\n",
      "en_privileges\n",
      "en_arab-israeli\n",
      "en_resembles\n",
      "en_koyuk\n",
      "en_schüttler\n",
      "en_threatened\n",
      "en_22.3\n",
      "en_22.2\n",
      "en_animātiō\n",
      "en_gamm\n"
     ]
    }
   ],
   "source": [
    "def print_orphans(vocab, bi_dict):\n",
    "    x = 1\n",
    "    for w in vocab:\n",
    "        if w not in bi_dict:\n",
    "            print(w)\n",
    "            x += 1\n",
    "        if x > 20:\n",
    "            break\n",
    "            \n",
    "print_orphans(en_it_vocab.types, bi_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary with full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train multilingual Vocabulary\n",
    "# NOTE: use FULL_EN_IT if on the instance\n",
    "en_it_data = Corpus(EN_IT_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_it_vocab = Vocabulary(en_it_data.gen_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48579"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of corpus vocabulary\n",
    "en_it_vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24176"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overlap with dictionary vocabulary\n",
    "len([w for w in en_it_vocab.types if w in bi_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_spunti\n",
      "it_integrali\n",
      "en_manuscripts\n",
      "it_giostrando\n",
      "en_syro-hittite\n",
      "it_subsessile\n",
      "it_[[878172]]\n",
      "it_[[879051]]\n",
      "it_\")\n",
      "it_raffiguranti\n",
      "it_promettenti\n",
      "it_raffigurante\n",
      "it_margaria\n",
      "en_western-hemisphere\n",
      "it_gallizio\n",
      "en_migrating\n",
      "en_conraua\n",
      "it_\".\n",
      "en_aretē\n",
      "it_macchiano\n"
     ]
    }
   ],
   "source": [
    "# sample of orphaned words\n",
    "print_orphans(en_it_vocab.types, bi_dict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Data Generator\n",
    "__`CHECK PAPER for HYPERPARAMS!`__: I can't seem to find where they talk about the context window size, embedding size and batch size they use -- it may actually be in the Vulic and Moens paper instead of the Duong one.\n",
    "\n",
    "__`RLH Update`__: Duong et al. section 6, footnote 4: \"Default learning rate of 0.025, negative sampling with 25 samples, subsampling rate of value 1e−4, embedding dimension d = 200, window size cs = 48 and run for 15 epochs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "WINDOW_SIZE = 4\n",
    "MAX_EPOCHS = 1 # fail safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batched_data = batch_generator(en_it_data, \n",
    "                               en_it_vocab, \n",
    "                               BATCH_SIZE, \n",
    "                               WINDOW_SIZE, \n",
    "                               MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Format Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 1: recreate sentence & batch generators so they're insynch\n",
    "token_gen = en_it_data.gen_tokens()\n",
    "batch_gen = batch_generator(en_it_data, en_it_vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATA:\n",
      "['it_[[877881]]', 'it_[[879362]]', 'it_in', 'it_un', 'it_remoto']\n",
      "CONTEXT IDS: [[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 34, 15624, 1584, 309], [0, 0, 0, 20, 15624, 1584, 309, 5533], [0, 0, 20, 34, 1584, 309, 5533, 7]]\n",
      "CONTEXT: [['<s>', '<s>', '<s>', '<s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', '<s>', '<s>', '<s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'it_un', 'it_remoto', 'it_passato', 'it_aveva'], ['<s>', '<s>', '<s>', 'it_in', 'it_remoto', 'it_passato', 'it_aveva', 'it_progettato'], ['<s>', '<s>', 'it_in', 'it_un', 'it_passato', 'it_aveva', 'it_progettato', 'it_,']]\n",
      "LABEL IDS: [43790, 24849, 20, 34, 15624]\n",
      "LABELS: ['it_[[877881]]', 'it_[[879362]]', 'it_in', 'it_un', 'it_remoto']\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: have a look\n",
    "print(\"ORIGINAL DATA:\")\n",
    "print([next(token_gen) for _ in range(BATCH_SIZE)])\n",
    "\n",
    "for context, label in batch_gen:\n",
    "    print(\"CONTEXT IDS:\", context[:5])\n",
    "    print(\"CONTEXT:\", [en_it_vocab.to_words(c) for c in context[:5]])\n",
    "    print(\"LABEL IDS:\", label[:5])\n",
    "    print(\"LABELS:\", en_it_vocab.to_words(label[:5]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun Validation Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 46, 23465, 25963]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_it_vocab.to_ids(['en_the','en_first', 'it_nuovo', 'it_parola'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it_della', 'it_gli', 'it_i', 'it_il', 'it_la', 'it_l\\xc3\\xa0',\n",
       "       'it_le', 'it_lo', 'it_ma'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['en_the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it_anteriore', 'it_anteriormente', 'it_antico', 'it_anzitutto',\n",
       "       'it_anzi_tutto', 'it_avvio', 'it_dapprima', 'it_davanti',\n",
       "       'it_di_fronte', 'it_il_primo', 'it_in', 'it_in_cima', 'it_inizio',\n",
       "       'it_innanzitutto', 'it_innanzi_tutto', 'it_in_primis',\n",
       "       'it_in_primo_luogo', 'it_la_prima', 'it_per_la_prima_volta',\n",
       "       'it_per_primo', 'it_precedente', 'it_prima', 'it_prima_di_tutto',\n",
       "       'it_primariamente', 'it_primario', 'it_prime', 'it_primieramente',\n",
       "       'it_primiero', 'it_primo', 'it_principale', 'it_principio'], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['en_first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en_fresh', 'en_green', 'en_latter-day', 'en_new', 'en_novel',\n",
       "       'en_raw', 'en_recent', 'en_renewed', 'en_unexampled', 'en_unused',\n",
       "       'en_young'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['it_nuovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en_drake', 'en_language', 'en_mot', 'en_parole', 'en_promise',\n",
       "       'en_-shaped', 'en_speech', 'en_term', 'en_tongue', 'en_verb',\n",
       "       'en_vocable', 'en_word_of_honor', 'en_word'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['it_parola']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model - no word sub yet!\n",
    "__`CODE NOTES:`__ To get this running I had to hard code the context length (set to 2) inside `BuildCoreGraph()` where we generate `self.input_` in line 102. That should really be inferred from the `self.context_` itself but it doesn't seem to like the placeholder dimension (we don't have a span length until runtime). Does tensorflow not have a vectorized average? Something to fix (later). I also had to hard code the number of samples for softmax (I had originally put this as a `tf.placeholder_with_default` thinking we could pass it in to the training function (since its a training parameter) but TF kicked out an error message asking for an integer so for now I'll just give it what it wants. I need to think more about why TF doesn't want this changing from batch to batch. (or if there is another reason it wants an int)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "WINDOW_SIZE = 1\n",
    "MAX_EPOCHS = 15 # fail safe\n",
    "\n",
    "batched_data = batch_generator(en_it_data, en_it_vocab, BATCH_SIZE, \n",
    "                               WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V\n",
    "\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# create model\n",
    "model = BiW2V(index = en_it_vocab.index, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model.BuildCoreGraph()\n",
    "model.BuildTrainingGraph()\n",
    "model.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "__`IMPORTANT!`__ right now the model only works with a window of 1 because the feed dict can't handle context windows of different lengths. We'll either need to figure out how to have a variable length dimension or else add extra padding to the sentences to account for the window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(50709, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(50709, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(50709,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 9.23656622569e-05\n",
      "   [it_,] sim words:  it_piazzati, it_un'iscrizione, it_sessanta, it_verità, it_luo, it_chieri, it_sancita, it_disastrosamente,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_cambio, it_chomsky, it_catalana, it_meccanismi, it_aiutare, it_agricola, it_incaricato, it_drawing,\n",
      "   [it_manoscritto] sim words:  it_proietta, it_anticrisi, it_autolinee, it_sv, it_arroscia, it_forze, it_popolato, it_detenuti,\n",
      "... STEP 30000 : Average Loss : 0.560785405807\n",
      "... STEP 60000 : Average Loss : 0.353204154999\n",
      "   [it_,] sim words:  it_piazzati, it_un'iscrizione, it_sessanta, it_verità, it_luo, it_chieri, it_sancita, it_conseguenza,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_cambio, it_chomsky, it_catalana, it_la, it_meccanismi, it_aiutare, it_agricola, it_incaricato,\n",
      "   [it_manoscritto] sim words:  it_proietta, it_anticrisi, it_autolinee, it_sv, it_arroscia, it_forze, it_popolato, it_a,\n",
      "... STEP 90000 : Average Loss : 0.26959254044\n",
      "... STEP 120000 : Average Loss : 0.20879321057\n",
      "   [it_,] sim words:  it_piazzati, it_un'iscrizione, it_sessanta, it_verità, it_luo, it_chieri, it_sancita, it_conseguenza,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_la, it_cambio, it_chomsky, it_catalana, it_meccanismi, it_aiutare, it_agricola, it_incaricato,\n",
      "   [it_manoscritto] sim words:  it_a, it_proietta, it_anticrisi, it_autolinee, it_sv, it_arroscia, it_forze, it_popolato,\n",
      "... STEP 150000 : Average Loss : 0.172264112494\n",
      "... STEP 180000 : Average Loss : 0.141350095836\n",
      "   [it_,] sim words:  it_piazzati, it_un'iscrizione, it_sessanta, it_verità, it_luo, it_chieri, it_sancita, it_conseguenza,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_la, it_cambio, it_chomsky, it_catalana, it_meccanismi, it_aiutare, it_agricola, it_incaricato,\n",
      "   [it_manoscritto] sim words:  it_a, it_proietta, it_da, it_anticrisi, it_), it_autolinee, it_in, it_sv,\n",
      "... STEP 210000 : Average Loss : 0.117509773142\n",
      "... STEP 240000 : Average Loss : 0.102170003423\n",
      "   [it_,] sim words:  it_piazzati, it_un'iscrizione, <s>, it_sessanta, it_verità, it_luo, it_chieri, it_sancita,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_la, it_cambio, it_chomsky, it_catalana, it_meccanismi, it_aiutare, it_agricola, it_incaricato,\n",
      "   [it_manoscritto] sim words:  it_a, it_da, it_proietta, it_), it_in, it_anticrisi, it_autolinee, it_sv,\n",
      "... STEP 270000 : Average Loss : 0.0864160344615\n",
      "... STEP 300000 : Average Loss : 0.0791876024566\n",
      "   [it_,] sim words:  <s>, it_piazzati, it_un'iscrizione, it_sessanta, it_verità, it_luo, it_chieri, it_sancita,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_la, it_cambio, it_chomsky, it_catalana, it_con, it_meccanismi, it_aiutare, it_agricola,\n",
      "   [it_manoscritto] sim words:  it_a, it_da, it_proietta, it_), it_in, it_anticrisi, it_autolinee, it_sv,\n",
      "... Training Complete\n",
      "... 300000 batches trained in 450.537431955 seconds\n"
     ]
    }
   ],
   "source": [
    "# time\n",
    "start = time.time()\n",
    "\n",
    "# training parameters\n",
    "TEST_WORDS = [3, 84, 669, 6646] # en_the, en_first, it_nuovo, it_parole\n",
    "nBATCHES = 300000 # ~ 14 epochs\n",
    "DATA_GENERATOR = batched_data\n",
    "\n",
    "# training call\n",
    "model.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = 0.15)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES:__ This is just a context of 1 (ie. window = 3) and there's no bilingual signal. When I ran it w/ the default learning rate there was mad overfitting for `the`'s neighbors but `first` had some much better results (eg. `third` and `only`). It would be interesting to really tune the hyperparamters to see how good we could do (this is essentially monolingual word2vec with two languages)... as a point of comparison for the bilingual versions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.60977550e-04,   6.87121879e-04,  -6.07755675e-04, ...,\n",
       "          1.04074273e-03,  -7.94071006e-04,  -4.09811037e-04],\n",
       "       [  6.01218257e-04,  -1.73640612e-04,   1.50801498e-04, ...,\n",
       "          9.76910815e-04,   4.34648187e-04,   1.50857595e-04],\n",
       "       [  4.21131008e-05,   1.43417434e-04,  -4.42015793e-04, ...,\n",
       "         -2.61730253e-04,   2.63188384e-04,  -3.19738436e-04],\n",
       "       ..., \n",
       "       [ -5.24837349e-04,   7.47645172e-05,  -3.87505628e-04, ...,\n",
       "          3.38180165e-04,   1.87452009e-04,  -4.23476391e-04],\n",
       "       [ -2.05034637e-04,  -1.74783956e-04,   2.54430954e-04, ...,\n",
       "         -1.96897527e-04,  -4.46256425e-04,  -4.10091743e-04],\n",
       "       [  4.34216403e-04,   9.33892716e-06,  -4.76779445e-04, ...,\n",
       "          1.30503067e-05,  -3.12790798e-05,  -4.18215437e-04]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the embeddings\n",
    "model.context_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__`Hmmmm...`__ These don't look normalized to me. Something to return to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Random Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "WINDOW_SIZE = 4\n",
    "MAX_EPOCHS = 20 # fail safe\n",
    "\n",
    "batched_data = batch_generator(en_it_data, en_it_vocab, BATCH_SIZE, \n",
    "                               WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_random\n",
    "\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# create model\n",
    "model2 = BiW2V_random(bi_dict, vocab = en_it_vocab, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model2.BuildCoreGraph()\n",
    "model2.BuildTrainingGraph()\n",
    "model2.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "TEST_WORDS = [3, 46, 23465, 25963] # en_the, en_first, it_nuovo, it_parole\n",
    "nBATCHES = 100000 # ~ 10 epochs on small en_it dataset\n",
    "DATA_GENERATOR = batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(48579, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(48579, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(48579,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.000644030761719\n",
      "   [en_the] sim words:  it_1832, it_paura, en_reduction, en_afroasiatic, it_professionistico, it_primi, en_prosecuted, en_29,689,\n",
      "   [en_first] sim words:  it_nobiliare, en_child-star, en_protists, it_pensato, en_2:1, it_limitazione, en_theirs, en_countrywide,\n",
      "   [it_nuovo] sim words:  en_imitation, en_modernize, it_maestri, it_gestito, it_atv, it_contenesse, it_casella, en_observatories,\n",
      "   [it_parola] sim words:  it_anti, en_fuel, en_babylonians, it_legarsi, en_unspecified, it_reclutato, en_abnormal, it_blockbuster,\n",
      "... STEP 10000 : Average Loss : 3.72829451993\n",
      "... STEP 20000 : Average Loss : 3.07080396673\n",
      "   [en_the] sim words:  en_,, en_of, en_., en_to, en_in, en_and, <s>, en_a,\n",
      "   [en_first] sim words:  en_child-star, it_nobiliare, en_protists, en_2:1, it_limitazione, en_countrywide, it_pensato, en_annie,\n",
      "   [it_nuovo] sim words:  en_imitation, en_modernize, <s>, </s>, it_maestri, it_gestito, it_atv, it_contenesse,\n",
      "   [it_parola] sim words:  it_anti, en_fuel, en_babylonians, en_unspecified, it_legarsi, it_reclutato, en_abnormal, it_blockbuster,\n",
      "... STEP 30000 : Average Loss : 2.82630164856\n",
      "... STEP 40000 : Average Loss : 2.70269337368\n",
      "   [en_the] sim words:  en_,, en_., en_of, en_in, en_to, en_a, en_and, en_that,\n",
      "   [en_first] sim words:  it_nobiliare, en_child-star, en_protists, <s>, en_2:1, it_limitazione, en_countrywide, en_annie,\n",
      "   [it_nuovo] sim words:  </s>, en_imitation, <s>, en_modernize, it_maestri, it_gestito, it_atv, it_del,\n",
      "   [it_parola] sim words:  it_anti, en_fuel, en_babylonians, en_unspecified, it_legarsi, it_reclutato, en_abnormal, it_blockbuster,\n",
      "... STEP 50000 : Average Loss : 2.59730833451\n",
      "... STEP 60000 : Average Loss : 2.52518722055\n",
      "   [en_the] sim words:  en_., en_,, en_of, en_in, en_to, en_a, en_and, en_that,\n",
      "   [en_first] sim words:  <s>, it_nobiliare, en_child-star, en_protists, it_limitazione, en_2:1, en_countrywide, en_of,\n",
      "   [it_nuovo] sim words:  <s>, </s>, en_imitation, en_modernize, it_del, it_maestri, it_gestito, it_atv,\n",
      "   [it_parola] sim words:  it_anti, en_fuel, en_babylonians, en_unspecified, it_legarsi, it_reclutato, en_abnormal, it_blockbuster,\n",
      "... STEP 70000 : Average Loss : 2.46071511587\n",
      "... STEP 80000 : Average Loss : 2.41157856221\n",
      "   [en_the] sim words:  en_., en_,, en_of, en_a, en_in, en_to, en_and, en_that,\n",
      "   [en_first] sim words:  <s>, en_., en_of, en_to, it_nobiliare, en_child-star, it_limitazione, en_countrywide,\n",
      "   [it_nuovo] sim words:  <s>, </s>, en_imitation, en_modernize, it_del, it_maestri, it_gestito, it_atv,\n",
      "   [it_parola] sim words:  it_anti, en_fuel, en_babylonians, it_legarsi, en_unspecified, it_reclutato, en_abnormal, it_blockbuster,\n",
      "... STEP 90000 : Average Loss : 2.3666239041\n",
      "... STEP 100000 : Average Loss : 2.32929420652\n",
      "   [en_the] sim words:  en_., en_a, en_of, en_in, en_,, en_to, en_and, en_that,\n",
      "   [en_first] sim words:  en_., <s>, en_to, en_of, it_nobiliare, it_limitazione, en_child-star, en_countrywide,\n",
      "   [it_nuovo] sim words:  <s>, it_del, </s>, en_imitation, en_modernize, it_maestri, it_gestito, it_a,\n",
      "   [it_parola] sim words:  it_anti, en_fuel, en_babylonians, it_legarsi, en_unspecified, it_reclutato, en_abnormal, it_blockbuster,\n",
      "... Training Complete\n",
      "... 100000 batches trained in 278.36611414 seconds\n"
     ]
    }
   ],
   "source": [
    "# training call\n",
    "start = time.time()\n",
    "model2.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = 0.15)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTES:`__ Same words look reasonable for 'the' but are subpar for the others. I'd be interesting in training this longer and experimenting with the learning rate. [UPDATE: alpha = 0.55 seemed to perform a little better but mixed results (at least in terms of the example words) with 0.85 and 0.65... going to wait for more data before spending more time on this point.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving final embeddings in case we want to do more stuff later\n",
    "filename = SAVE_TO + '../embeddings/en_it_rand_600K_cw1_V_dec15.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model2.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "filename = SAVE_TO + '../embeddings/en_it_rand_600K_cw1_U_dec15.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model2.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.42820040e-02,  -1.08593737e-03,  -9.77827888e-03, ...,\n",
       "         -7.43078999e-03,  -9.79084056e-04,  -7.75979646e-03],\n",
       "       [  6.15398725e-03,   3.99457384e-03,  -7.21636403e-04, ...,\n",
       "         -4.45156882e-04,   4.76947054e-03,   4.01509507e-03],\n",
       "       [  1.42526999e-03,  -2.62570567e-03,   6.83171733e-04, ...,\n",
       "         -2.14850740e-03,  -6.21526386e-04,   8.00127018e-05],\n",
       "       ..., \n",
       "       [ -4.83615768e-05,  -7.35577720e-04,   2.69850646e-03, ...,\n",
       "         -1.72964076e-03,   2.55509047e-03,   6.92204339e-04],\n",
       "       [  3.46941641e-03,   5.76911087e-04,   7.60798051e-04, ...,\n",
       "          3.49387084e-03,   3.47503019e-03,  -1.87479728e-03],\n",
       "       [ -7.38707022e-04,  -1.68911624e-03,  -2.75655207e-03, ...,\n",
       "         -2.23345775e-03,  -2.73358473e-03,   1.52106478e-03]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm:\n",
    "filename = SAVE_TO + '/en_it_rand_600K_cw1_U_dec15.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    C_embedding = pickle.load(f)\n",
    "    \n",
    "C_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Translation with Larger Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`WARNING:`__ the code below was run on an old version of the model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "WINDOW_SIZE = 8\n",
    "MAX_EPOCHS = 30 # fail safe\n",
    "\n",
    "batched_data = batch_generator(en_it_data, en_it_vocab, BATCH_SIZE, \n",
    "                               WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_random\n",
    "\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# create model\n",
    "model3 = BiW2V_random(('en', 'it'), bi_dict, en_it_vocab.to_ids,\n",
    "                      index = en_it_vocab.index, \n",
    "                      H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model3.BuildCoreGraph()\n",
    "model3.BuildTrainingGraph()\n",
    "model3.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(48579, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(48579, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(48579,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.000122350947062\n",
      "   [en_the] sim words:  en_diakonoff, it_abraham, it_iuta, en_coups, en_griffith, en_alter, en_upward, en_1770s,\n",
      "   [en_first] sim words:  it_firmato, en_boil, it_fiocina, it_vga, it_[[876681]], en_featural, it_risparmio, en_run,\n",
      "   [it_nuovo] sim words:  it_sabana, it_dell’antimateria, en_stabbing, en_probes, it_hetman, it_rof, it_roseto, en_restarted,\n",
      "   [it_parola] sim words:  it_d'artificio, it_rimanesse, en_infantile, it_branca, it_permesso, en_materialism, en_reunited, en_non-syndromal,\n",
      "... STEP 60000 : Average Loss : 3.43592488782\n",
      "... STEP 120000 : Average Loss : 2.32015497759\n",
      "   [en_the] sim words:  en_counteracts, en_antiquated, en_tropics, it_1829, en_spearheading, it_ambientati, en_nvt, it_amburgo,\n",
      "   [en_first] sim words:  it_arresero, it_pozzo, en_boil, en_unique, it_retrocedendo, it_acquisiti, it_risparmio, en_reading,\n",
      "   [it_nuovo] sim words:  it_hetman, it_sabana, en_probes, en_stabbing, it_rof, it_dell’antimateria, en_prudhoe, it_roseto,\n",
      "   [it_parola] sim words:  it_d'artificio, it_rimanesse, en_infantile, it_branca, en_non-syndromal, it_permesso, it_specie, en_materialism,\n",
      "... STEP 180000 : Average Loss : 1.96030490431\n",
      "... STEP 240000 : Average Loss : 1.76807048526\n",
      "   [en_the] sim words:  it_ambientati, en_this, it_frassino, it_alghe, it_mf, en_governors, en_nvt, it_l'orchestra,\n",
      "   [en_first] sim words:  it_arresero, it_pozzo, it_retrocedendo, en_patents, it_incanti, en_boil, en_tossing, it_ghignante,\n",
      "   [it_nuovo] sim words:  it_hetman, it_sabana, en_prudhoe, en_probes, en_stabbing, it_rof, it_sottogruppo, it_dell’antimateria,\n",
      "   [it_parola] sim words:  it_d'artificio, it_rimanesse, en_infantile, en_non-syndromal, it_branca, it_permesso, it_miracoli, en_reunited,\n",
      "... STEP 300000 : Average Loss : 1.62638815477\n",
      "... STEP 360000 : Average Loss : 1.508576124\n",
      "   [en_the] sim words:  en_counteracts, en_guava, en_spontaneity, it_bakr, en_assistance, en_mid, it_scaduto, en_tropics,\n",
      "   [en_first] sim words:  it_arresero, en_largest, it_pozzo, it_incanti, en_patents, en_tossing, it_ghignante, it_retrocedendo,\n",
      "   [it_nuovo] sim words:  it_hetman, it_sabana, en_prudhoe, en_probes, it_dorfe, it_rof, it_sottogruppo, en_criminal,\n",
      "   [it_parola] sim words:  it_d'artificio, en_non-syndromal, en_infantile, it_rimanesse, it_branca, it_miracoli, it_permesso, en_reunited,\n",
      "... STEP 420000 : Average Loss : 1.40801186306\n",
      "... STEP 480000 : Average Loss : 1.3143499378\n",
      "   [en_the] sim words:  it_frassino, en_spontaneity, en_fernando, it_bagno, en_antiquated, en_1,427, en_counteracts, it_riformista,\n",
      "   [en_first] sim words:  en_largest, it_arresero, it_berbera, it_incanti, it_pozzo, en_2009, en_tossing, it_vignali,\n",
      "   [it_nuovo] sim words:  it_hetman, it_jorge, it_dorfe, it_myślibórz, en_anthem, it_sabana, en_probes, en_criminal,\n",
      "   [it_parola] sim words:  it_d'artificio, en_non-syndromal, en_infantile, it_rimanesse, en_reunited, it_branca, it_permesso, it_miracoli,\n",
      "... STEP 540000 : Average Loss : 1.22699242282\n",
      "... STEP 600000 : Average Loss : 1.14706762033\n",
      "   [en_the] sim words:  en_assistance, en_spontaneity, en_antiquated, en_actium, en_a, it_frassino, it_scaduto, en_selfless,\n",
      "   [en_first] sim words:  en_largest, it_berbera, en_figurative, it_arresero, it_pozzo, en_patents, en_melts, en_2009,\n",
      "   [it_nuovo] sim words:  it_jorge, it_hetman, en_anthem, it_dorfe, it_myślibórz, en_probes, it_sottogruppo, en_prudhoe,\n",
      "   [it_parola] sim words:  it_d'artificio, en_non-syndromal, en_infantile, it_rimanesse, en_reunited, it_permesso, it_miracoli, it_divo,\n",
      "... Training Complete\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "nBATCHES = 600000 # ~ 25 epochs\n",
    "DATA_GENERATOR = batched_data\n",
    "TEST_WORDS = [3, 84, 669, 6646]\n",
    "\n",
    "# training call\n",
    "model3.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES:__ Interesting, the larger context seems to hurt the training performance. Is this because of the extra padding? Or will a smart adjustment of the learning rate redeem this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrds = \"en_the en_a en_this en_'s en_an en_their en_its en_these en_his \\\n",
    "       en_first en_on en_in en_for en_to en_with en_are en_. en_all \\\n",
    "       it_nuovo it_di it_un it_, <s> it_i it_con it_è it_più it_parola \\\n",
    "       en_censorship en_minima it_profesional it_michail en_الأول \\\n",
    "       it_ritengono en_prevention it_mckenna en_third\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in [\"en_the\", \"en_first\", \"it_nuovo\", \"it_parole\"]:\n",
    "    wrds += list(bi_dict[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordset = set(en_it_vocab.to_ids(wrds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'TSNE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-87e85cb87b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_embeddings_in_2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mmillervedam/Documents/MIDS/w266/FinalProject/Notebooks/models.py\u001b[0m in \u001b[0;36mplot_embeddings_in_2D\u001b[0;34m(self, wordset)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must train the embeddings before plotting.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0mlow_dim_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'TSNE' is not defined"
     ]
    }
   ],
   "source": [
    "model2.plot_embeddings_in_2D(wordset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang=\"it\"\n",
    "GTT_BASE = '/home/rhopper/W266-Fall-2017-Final-Project/BaselineModels/data/ground_truth_translations/' #'/home/mmillervedam/ProjectRepo/BaselineModels/data/ground_truth_translations/'\n",
    "GTT_PATH = GTT_BASE + \"%s-%s-clean.txt\" % (source_lang, target_lang)\n",
    "gtt = pd.read_csv(GTT_PATH, names = [source_lang, target_lang], sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "sim shape (3, 48579)\n",
      "vocab size 48579\n",
      "word en_the\n",
      "half of vocab 24291\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 24291 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c1f8da9d651d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_translations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/rhopper/W266-Fall-2017-Final-Project/Notebooks/models.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, ground_truth_translations, sample, verbose)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;31m#source_lang = \"en\" #Hard-coding for testing; should be self.vocab.language[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;31m#target_lang = \"it\" #Hard-coding for testing; should be self.vocab.language[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mnearest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_translation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0mtotal_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_translation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rhopper/W266-Fall-2017-Final-Project/Notebooks/models.py\u001b[0m in \u001b[0;36mevaluate_prediction\u001b[0;34m(self, i, sim, top_k, word)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'half of vocab'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mnearest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Take the nearest from the second half of the matrix (target language is second half)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;31m#nearest = (-sim[(len(sim)/2)+i, :]).argsort()[1:top_k + 1] #Take the nearest from the second half of the matrix (target language is second half)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 24291 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "model2.evaluate(ground_truth_translations=gtt, sample=[3, 4, 5])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
