{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Substitution\n",
    "`w266 Final Project: Crosslingual Word Embeddings`\n",
    "\n",
    "The code in this notebook was used to develop an algorithm to generate crosslingual word embeddings by training on a monolingual corpus and substituting translations at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filepaths\n",
    "BASE = '/Users/mmillervedam/Documents/MIDS/w266' #'/home/mmillervedam/' \n",
    "PROJ = '/Users/mmillervedam/Documents/MIDS/w266/FinalProject'#'/home/mmillervedam/ProjectRepo'\n",
    "FPATH_EN = BASE + '/Data/test/wiki_en_10K.txt' # first 10000 lines from wiki dump\n",
    "FPATH_ES = BASE + '/Data/test/wiki_es_10K.txt' # first 10000 lines from wiki dump\n",
    "#FULL_EN = BASE + '/Data/en/full.txt'\n",
    "#FULL_ES = BASE + '/Data/es/full.txt'\n",
    "EN_ES_DICT = PROJ +'/XlingualEmb/data/dicts/en.es.panlex.all.processed'\n",
    "EN_IT_DICT  = PROJ +'/XlingualEmb/data/dicts/en.it.panlex.all.processed'\n",
    "EN_IT_RAW = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'\n",
    "EN_IT_RAW = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Preprocess Data\n",
    "__`ORIGINAL AUTHORS SAY:`__ \"Normally, the monolingual word embeddings are trained on billions of words. However, getting that much of monolingual data for a low-resource language is also challenging. That is why we only select the top 5 million sentences (around 100 million words) for each language.\" - _Section 5.1, Duong et. al._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import Corpus, Vocabulary, batch_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load corpus\n",
    "en_it_data = Corpus(EN_IT_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20000  430928 3746786 /Users/mmillervedam/Documents/MIDS/w266/FinalProject/XlingualEmb/data/mono/en_it.shuf.10k\r\n"
     ]
    }
   ],
   "source": [
    "# Corpus Stats\n",
    "!wc {EN_IT_RAW}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`i.e.:`__ 20K sentences (10K in each language) with ~430K tokens\n",
    "> So this must not be their full data For now, I'm just going to look at the top 20K words and see what happens. In reality we should probably modify the Vocab class so that it explicily collects the top words for each language separately and then concatenates the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading english-italian dictionary\n",
    "pld = pd.read_csv(EN_IT_DICT, sep='\\t', names = ['en', 'it'], dtype=str)\n",
    "en_set = set(pld.en.unique())\n",
    "it_set = set(pld.it.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: 266450\n",
      "IT: 258641\n"
     ]
    }
   ],
   "source": [
    "# dictionary vocab lengths:\n",
    "print('EN:', len(en_set))\n",
    "print('IT:', len(it_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train multilingual Vocabulary\n",
    "en_it_vocab = Vocabulary(en_it_data.gen_tokens(), size = 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Data Generator\n",
    "__`CHECK PAPER for HYPERPARAMS!`__: I can't seem to find where they talk abou the context window size, embedding size and batch size they use -- it may actually be in the Vulic and Moens paper instead of the Duong one.\n",
    "\n",
    "__`RLH Update`__: Duong et al. section 6, footnote 4: \"Default learning rate of 0.025, negative sampling with 25 samples, subsampling rate of value 1e−4, embedding dimension d = 200, window size cs = 48 and run for 15 epochs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "WINDOW_SIZE = 1\n",
    "MAX_EPOCHS = 1 # fail safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batched_data = batch_generator(en_it_data, \n",
    "                               en_it_vocab, \n",
    "                               BATCH_SIZE, \n",
    "                               WINDOW_SIZE, \n",
    "                               MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT IDS: [[0, 1], [0, 1], [0, 34], [20, 17318], [34, 1638]]\n",
      "LABEL IDS: [25668, 37957, 20, 34, 17318]\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "for context, label in batched_data:\n",
    "    print(\"CONTEXT IDS:\", context[:5])\n",
    "    print(\"LABEL IDS:\", label[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model\n",
    "__`CODE NOTES:`__ To get this running I had to hard code the context length (set to 2) inside `BuildCoreGraph()` where we generate `self.input_` in line 102. That should really be inferred from the `self.context_` itself but it doesn't seem to like the placeholder dimension (we don't have a span length until runtime). Does tensorflow not have a vectorized average? Something to fix (later). I also had to hard code the number of samples for softmax (I had originally put this as a `tf.placeholder_with_default` thinking we could pass it in to the training function (since its a training parameter) but TF kicked out an error message asking for an integer so for now I'll just give it what it wants. I need to think more about why TF doesn't want this changing from batch to batch. (or if there is another reason it wants an int)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import BiW2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BiW2V(index = en_it_vocab.index, H = EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.BuildCoreGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.BuildTrainingGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`IMPORTANT!`__ right now the model only works with a window of 1 because the feed dict can't handle context windows of different lengths. We'll either need to figure out how to have a variable length dimension or else add extra padding to the sentences to account for the window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(48579, 128) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(48579, 128) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(48579,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "Average loss at step  0 :  0.39325568676\n",
      "   Nearest to _en_the: _it_spitz, _en_diagnoses, _it_l'escrezione, _en_جابر, _it_latero, _it_doppiatore, _it_[[877696]], _en_mercedes-benz,\n",
      "   Nearest to _en_,: _it_fondatori, _it_qum, _it_invadendo, _en_tm-3, _en_fence, _en_outdated, _it_dall'altra, _it_dragan,\n",
      "   Nearest to _en_.: _it_rurale, _it_dovesse, _en_discuss, _it_oggetto, _it_progressiva, _it_ag4, _en_budgets, _it_strada,\n",
      "   Nearest to _en_of: _it_pva, _it_angioletto, _it_controproducente, _en_unexpected, _en_brooke, _it_porro, _it_rientrato, _it_espresso,\n",
      "   Nearest to _it_,: _en_founding, _en_horses, _it_dell'ungheria, _it_transcaspiana, _it_esistenza, _en_flow-on, _it_giorgio, _it_animati,\n",
      "Average loss at step  20 :  6.77429425716\n",
      "Average loss at step  40 :  6.59204256535\n",
      "   Nearest to _en_the: _it_spitz, _en_diagnoses, _it_l'escrezione, _en_mercedes-benz, _it_latero, _it_[[877696]], _it_doppiatore, _en_جابر,\n",
      "   Nearest to _en_,: _it_fondatori, _it_qum, _en_tm-3, _it_invadendo, _en_fence, _en_outdated, _it_dall'altra, _it_jakob,\n",
      "   Nearest to _en_.: _it_rurale, _it_dovesse, _en_discuss, _it_oggetto, _it_progressiva, _it_ag4, _en_viktor, _en_budgets,\n",
      "   Nearest to _en_of: _it_pva, _it_angioletto, _it_controproducente, _en_unexpected, _en_brooke, _it_porro, _it_rientrato, _it_espresso,\n",
      "   Nearest to _it_,: _en_founding, _en_horses, _it_dell'ungheria, _it_transcaspiana, _it_esistenza, _en_flow-on, _it_animati, _it_giorgio,\n",
      "Average loss at step  60 :  6.24995720387\n",
      "Average loss at step  80 :  5.92869446278\n",
      "   Nearest to _en_the: _it_spitz, _en_diagnoses, _it_l'escrezione, _it_latero, _en_mercedes-benz, _en_extravert, _it_uno, _en_جابر,\n",
      "   Nearest to _en_,: _it_fondatori, _it_qum, _it_invadendo, _en_tm-3, _en_fence, _it_jakob, _en_outdated, _it_dall'altra,\n",
      "   Nearest to _en_.: _it_rurale, _it_dovesse, _en_discuss, _it_oggetto, _it_progressiva, _it_ag4, _en_viktor, _it_parigi,\n",
      "   Nearest to _en_of: _it_angioletto, _it_pva, _it_controproducente, _en_unexpected, _en_brooke, _it_porro, _it_rientrato, _en_refer,\n",
      "   Nearest to _it_,: _en_founding, _en_horses, _it_dell'ungheria, _it_transcaspiana, _it_esistenza, _it_animati, _en_flow-on, _it_giorgio,\n",
      "Average loss at step  100 :  5.94137647152\n",
      "Average loss at step  120 :  5.83610277176\n",
      "   Nearest to _en_the: _it_spitz, _en_diagnoses, _it_l'escrezione, _it_latero, _en_extravert, _en_mercedes-benz, _it_uno, _en_جابر,\n",
      "   Nearest to _en_,: _it_fondatori, _it_qum, _it_invadendo, _en_fence, _en_tm-3, _en_outdated, _it_jakob, _it_dragan,\n",
      "   Nearest to _en_.: _it_rurale, _it_dovesse, _en_discuss, _it_oggetto, _it_progressiva, _it_ag4, _it_parigi, _en_viktor,\n",
      "   Nearest to _en_of: _it_pva, _it_angioletto, _it_controproducente, _en_unexpected, _en_brooke, _it_rientrato, _it_porro, _en_Ἰατρός,\n",
      "   Nearest to _it_,: _en_founding, _en_horses, _it_dell'ungheria, _it_transcaspiana, _it_esistenza, _it_animati, _en_flow-on, _it_giorgio,\n",
      "Average loss at step  140 :  5.89650864601\n",
      "Average loss at step  160 :  5.69141423702\n",
      "   Nearest to _en_the: _it_spitz, _en_diagnoses, _it_l'escrezione, _it_latero, _en_mercedes-benz, _en_extravert, _it_uno, _it_[[877696]],\n",
      "   Nearest to _en_,: _it_fondatori, _it_qum, _en_tm-3, _it_invadendo, _en_fence, _en_outdated, _it_dragan, _it_jakob,\n",
      "   Nearest to _en_.: _it_rurale, _it_dovesse, _it_oggetto, _en_discuss, _it_progressiva, _it_ag4, _it_parigi, _en_viktor,\n",
      "   Nearest to _en_of: _it_pva, _it_angioletto, _it_controproducente, _en_unexpected, _en_brooke, _it_porro, _it_rientrato, _en_refer,\n",
      "   Nearest to _it_,: _en_founding, _en_horses, _it_dell'ungheria, _it_transcaspiana, _it_animati, _it_esistenza, _it_giorgio, _en_flow-on,\n",
      "Average loss at step  180 :  5.55076870918\n",
      "Average loss at step  200 :  5.46932637691\n",
      "   Nearest to _en_the: _it_spitz, _en_diagnoses, _it_l'escrezione, _it_latero, _en_mercedes-benz, _it_uno, _en_extravert, _it_dell'operato,\n",
      "   Nearest to _en_,: _it_fondatori, _en_tm-3, _it_qum, _en_fence, _it_invadendo, _en_outdated, _it_jakob, _it_dragan,\n",
      "   Nearest to _en_.: _it_rurale, _it_dovesse, _en_discuss, _it_oggetto, _it_progressiva, _it_ag4, _it_parigi, _en_viktor,\n",
      "   Nearest to _en_of: _it_pva, _it_angioletto, _it_controproducente, _en_unexpected, _en_brooke, _it_porro, _it_rientrato, _en_refer,\n",
      "   Nearest to _it_,: _en_founding, _en_horses, _it_dell'ungheria, _it_transcaspiana, _it_animati, _it_esistenza, _it_giorgio, _en_flow-on,\n",
      "... Training Complete\n"
     ]
    }
   ],
   "source": [
    "model.train(200, batched_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
