{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Substitution\n",
    "`w266 Final Project: Crosslingual Word Embeddings`\n",
    "\n",
    "The code in this notebook was used to develop an algorithm to generate crosslingual word embeddings by training on a monolingual corpus and substituting translations at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Maya's paths\n",
    "BASE = '/Users/mmillervedam/Documents/MIDS/w266' #'/home/mmillervedam/' \n",
    "PROJ = '/Users/mmillervedam/Documents/MIDS/w266/FinalProject'#'/home/mmillervedam/ProjectRepo'\n",
    "\n",
    "## Roseanna's paths\n",
    "\n",
    "\n",
    "## Mona's local paths\n",
    "#BASE = '/Users/mona/OneDrive/repos/Data' #'/home/mmillervedam/Data'\n",
    "#PROJ = '/Users/mona/OneDrive/repos/final_proj/W266-Fall-2017-Final-Project'#'/home/mmillervedam/ProjectRepo'\n",
    "\n",
    "\n",
    "## Repo paths\n",
    "FPATH_EN = BASE + '/Data/test/wiki_en_10K.txt' # first 10000 lines from wiki dump\n",
    "FPATH_ES = BASE + '/Data/test/wiki_es_10K.txt' # first 10000 lines from wiki dump\n",
    "EN_ES_DICT = PROJ +'/XlingualEmb/data/dicts/en.es.panlex.all.processed'\n",
    "EN_IT_DICT  = PROJ +'/XlingualEmb/data/dicts/en.it.panlex.all.processed'\n",
    "EN_IT_RAW = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'\n",
    "EN_IT_RAW = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'\n",
    "FULL_EN = BASE + '/Data/en/full.txt'\n",
    "FULL_ES = BASE + '/Data/es/full.txt'\n",
    "FULL_IT = BASE + '/Data/it/full.txt'\n",
    "\n",
    "## Large datasets\n",
    "FULL_EN_ES = \"./shuffled_files/en_es_shuf.txt\"\n",
    "FULL_EN_IT = \"./shuffled_files/en_it_shuf.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save pickled embeddings\n",
    "SAVE_TO = PROJ + '/Notebooks/embeddings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Preprocess Data\n",
    "__`ORIGINAL AUTHORS SAY:`__ \"Normally, the monolingual word embeddings are trained on billions of words. However, getting that much of monolingual data for a low-resource language is also challenging. That is why we only select the top 5 million sentences (around 100 million words) for each language.\" - _Section 5.1, Duong et. al._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing import Corpus, Vocabulary, batch_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "en_it_data = Corpus(EN_IT_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20000  430928 3746786 /Users/mmillervedam/Documents/MIDS/w266/FinalProject/XlingualEmb/data/mono/en_it.shuf.10k\r\n"
     ]
    }
   ],
   "source": [
    "# Corpus Stats\n",
    "!wc {EN_IT_RAW}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`i.e.:`__ 20K sentences (10K in each language) with ~430K tokens\n",
    "> So this must not be their full data For now, I'm just going to look at the top 20K words and see what happens. In reality we should probably modify the Vocab class so that it explicily collects the top words for each language separately and then concatenates the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading english-italian dictionary\n",
    "pld = pd.read_csv(EN_IT_DICT, sep='\\t', names = ['en', 'it'], dtype=str)\n",
    "en_set = set(pld.en.unique())\n",
    "it_set = set(pld.it.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: 266450\n",
      "IT: 258641\n"
     ]
    }
   ],
   "source": [
    "# dictionary vocab lengths:\n",
    "print('EN:', len(en_set))\n",
    "print('IT:', len(it_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for ease of runtime translation\n",
    "# WARNING this takes a sec to run\n",
    "bi_dict = pld.groupby(['en'])['it'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add other direction\n",
    "# WARNING this takes another sec to run\n",
    "bi_dict.update(pld.groupby(['it'])['en'].unique().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it_aggirare', 'it_andai', 'it_andara', 'it_andare',\n",
       "       'it_andare_avanti'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo en to it\n",
    "bi_dict['en_go'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en_adieu', 'en_bye-bye', 'en_bye', 'en_cheerio', 'en_ciao'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo it to en\n",
    "bi_dict['it_ciao'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train multilingual Vocabulary\n",
    "en_it_vocab = Vocabulary(en_it_data.gen_tokens(), size = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of corpus vocabulary\n",
    "en_it_vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10607"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overlap with dictionary vocabulary\n",
    "len([w for w in en_it_vocab.types if w in bi_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Question for the group`__*Seems low?* Will we limit our vocab to the words in the dict? (MI)\n",
    "\n",
    "#### Sample of orphaned words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_spunti\n",
      "it_[[879051]]\n",
      "it_giostrando\n",
      "it_subsessile\n",
      "it_janue\n",
      "it_[[878172]]\n",
      "it_raffiguranti\n",
      "it_promettenti\n",
      "it_raffigurante\n",
      "it_margaria\n",
      "it_gallizio\n",
      "it_\".\n",
      "it_macchiano\n",
      "it_ripubblicate\n",
      "it_peppers\n",
      "it_mattonelle\n",
      "it_incendiata\n",
      "it_contemporanea\n",
      "it_senyor\n",
      "it_contemporanei\n"
     ]
    }
   ],
   "source": [
    "def print_orphans(vocab, bi_dict):\n",
    "    x = 1\n",
    "    for w in vocab:\n",
    "        if w not in bi_dict:\n",
    "            print(w)\n",
    "            x += 1\n",
    "        if x > 20:\n",
    "            break\n",
    "            \n",
    "print_orphans(en_it_vocab.types, bi_dict )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary with full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train multilingual Vocabulary\n",
    "# NOTE: use FULL_EN_IT if on the instance\n",
    "en_it_data = Corpus(EN_IT_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_it_vocab = Vocabulary(en_it_data.gen_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50709"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of corpus vocabulary\n",
    "en_it_vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10729"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overlap with dictionary vocabulary\n",
    "len([w for w in en_it_vocab.types if w in bi_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_spunti\n",
      "it_[[879051]]\n",
      "it_giostrando\n",
      "it_subsessile\n",
      "it_janue\n",
      "it_[[878172]]\n",
      "it_raffiguranti\n",
      "it_promettenti\n",
      "it_raffigurante\n",
      "it_margaria\n",
      "it_gallizio\n",
      "it_\".\n",
      "it_macchiano\n",
      "it_ripubblicate\n",
      "it_peppers\n",
      "it_mattonelle\n",
      "it_incendiata\n",
      "it_contemporanea\n",
      "it_senyor\n",
      "it_contemporanei\n"
     ]
    }
   ],
   "source": [
    "# sample of orphaned words\n",
    "print_orphans(en_it_vocab.types, bi_dict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Data Generator\n",
    "__`CHECK PAPER for HYPERPARAMS!`__: I can't seem to find where they talk abou the context window size, embedding size and batch size they use -- it may actually be in the Vulic and Moens paper instead of the Duong one.\n",
    "\n",
    "__`RLH Update`__: Duong et al. section 6, footnote 4: \"Default learning rate of 0.025, negative sampling with 25 samples, subsampling rate of value 1e−4, embedding dimension d = 200, window size cs = 48 and run for 15 epochs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "WINDOW_SIZE = 2\n",
    "MAX_EPOCHS = 1 # fail safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data = batch_generator(en_it_data, \n",
    "                               en_it_vocab, \n",
    "                               BATCH_SIZE, \n",
    "                               WINDOW_SIZE, \n",
    "                               MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT IDS: [[0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 34, 15624], [0, 20, 15624, 1584], [20, 34, 1584, 309]]\n",
      "CONTEXT: [['<s>', '<s>', '</s>', '</s>'], ['<s>', '<s>', '</s>', '</s>'], ['<s>', '<s>', 'it_un', 'it_remoto'], ['<s>', 'it_in', 'it_remoto', 'it_passato'], ['it_in', 'it_un', 'it_passato', 'it_aveva']]\n",
      "LABEL IDS: [43790, 24849, 20, 34, 15624]\n",
      "LABELS: ['it_[[877881]]', 'it_[[879362]]', 'it_in', 'it_un', 'it_remoto']\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "for context, label in batched_data:\n",
    "    print(\"CONTEXT IDS:\", context[:5])\n",
    "    print(\"CONTEXT:\", [en_it_vocab.to_words(c) for c in context[:5]])\n",
    "    print(\"LABEL IDS:\", label[:5])\n",
    "    print(\"LABELS:\", en_it_vocab.to_words(label[:5]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun Validation Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 84, 669, 6646]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_it_vocab.to_ids(['en_the','en_first', 'it_nuovo', 'it_parola'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it_della', 'it_gli', 'it_i', 'it_il', 'it_la', 'it_l\\xc3\\xa0',\n",
       "       'it_le', 'it_lo', 'it_ma'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['en_the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it_anteriore', 'it_anteriormente', 'it_antico', 'it_anzitutto',\n",
       "       'it_anzi_tutto', 'it_avvio', 'it_dapprima', 'it_davanti',\n",
       "       'it_di_fronte', 'it_il_primo', 'it_in', 'it_in_cima', 'it_inizio',\n",
       "       'it_innanzitutto', 'it_innanzi_tutto', 'it_in_primis',\n",
       "       'it_in_primo_luogo', 'it_la_prima', 'it_per_la_prima_volta',\n",
       "       'it_per_primo', 'it_precedente', 'it_prima', 'it_prima_di_tutto',\n",
       "       'it_primariamente', 'it_primario', 'it_prime', 'it_primieramente',\n",
       "       'it_primiero', 'it_primo', 'it_principale', 'it_principio'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['en_first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en_fresh', 'en_green', 'en_latter-day', 'en_new', 'en_novel',\n",
       "       'en_raw', 'en_recent', 'en_renewed', 'en_unexampled', 'en_unused',\n",
       "       'en_young'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['it_nuovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en_drake', 'en_language', 'en_mot', 'en_parole', 'en_promise',\n",
       "       'en_-shaped', 'en_speech', 'en_term', 'en_tongue', 'en_verb',\n",
       "       'en_vocable', 'en_word_of_honor', 'en_word'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['it_parola']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model - no word sub yet!\n",
    "__`CODE NOTES:`__ To get this running I had to hard code the context length (set to 2) inside `BuildCoreGraph()` where we generate `self.input_` in line 102. That should really be inferred from the `self.context_` itself but it doesn't seem to like the placeholder dimension (we don't have a span length until runtime). Does tensorflow not have a vectorized average? Something to fix (later). I also had to hard code the number of samples for softmax (I had originally put this as a `tf.placeholder_with_default` thinking we could pass it in to the training function (since its a training parameter) but TF kicked out an error message asking for an integer so for now I'll just give it what it wants. I need to think more about why TF doesn't want this changing from batch to batch. (or if there is another reason it wants an int)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "WINDOW_SIZE = 1\n",
    "MAX_EPOCHS = 15 # fail safe\n",
    "\n",
    "batched_data = batch_generator(en_it_data, en_it_vocab, BATCH_SIZE, \n",
    "                               WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V\n",
    "\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# create model\n",
    "model = BiW2V(index = en_it_vocab.index, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model.BuildCoreGraph()\n",
    "model.BuildTrainingGraph()\n",
    "model.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "__`IMPORTANT!`__ right now the model only works with a window of 1 because the feed dict can't handle context windows of different lengths. We'll either need to figure out how to have a variable length dimension or else add extra padding to the sentences to account for the window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(50709, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(50709, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(50709,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 9.23656622569e-05\n",
      "   [it_,] sim words:  it_piazzati, it_un'iscrizione, it_sessanta, it_verità, it_luo, it_chieri, it_sancita, it_disastrosamente,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_cambio, it_chomsky, it_catalana, it_meccanismi, it_aiutare, it_agricola, it_incaricato, it_drawing,\n",
      "   [it_manoscritto] sim words:  it_proietta, it_anticrisi, it_autolinee, it_sv, it_arroscia, it_forze, it_popolato, it_detenuti,\n",
      "... STEP 30000 : Average Loss : 0.560785405807\n",
      "... STEP 60000 : Average Loss : 0.353204154999\n",
      "   [it_,] sim words:  it_piazzati, it_un'iscrizione, it_sessanta, it_verità, it_luo, it_chieri, it_sancita, it_conseguenza,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_cambio, it_chomsky, it_catalana, it_la, it_meccanismi, it_aiutare, it_agricola, it_incaricato,\n",
      "   [it_manoscritto] sim words:  it_proietta, it_anticrisi, it_autolinee, it_sv, it_arroscia, it_forze, it_popolato, it_a,\n",
      "... STEP 90000 : Average Loss : 0.26959254044\n",
      "... STEP 120000 : Average Loss : 0.20879321057\n",
      "   [it_,] sim words:  it_piazzati, it_un'iscrizione, it_sessanta, it_verità, it_luo, it_chieri, it_sancita, it_conseguenza,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_la, it_cambio, it_chomsky, it_catalana, it_meccanismi, it_aiutare, it_agricola, it_incaricato,\n",
      "   [it_manoscritto] sim words:  it_a, it_proietta, it_anticrisi, it_autolinee, it_sv, it_arroscia, it_forze, it_popolato,\n",
      "... STEP 150000 : Average Loss : 0.172264112494\n",
      "... STEP 180000 : Average Loss : 0.141350095836\n",
      "   [it_,] sim words:  it_piazzati, it_un'iscrizione, it_sessanta, it_verità, it_luo, it_chieri, it_sancita, it_conseguenza,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_la, it_cambio, it_chomsky, it_catalana, it_meccanismi, it_aiutare, it_agricola, it_incaricato,\n",
      "   [it_manoscritto] sim words:  it_a, it_proietta, it_da, it_anticrisi, it_), it_autolinee, it_in, it_sv,\n",
      "... STEP 210000 : Average Loss : 0.117509773142\n",
      "... STEP 240000 : Average Loss : 0.102170003423\n",
      "   [it_,] sim words:  it_piazzati, it_un'iscrizione, <s>, it_sessanta, it_verità, it_luo, it_chieri, it_sancita,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_la, it_cambio, it_chomsky, it_catalana, it_meccanismi, it_aiutare, it_agricola, it_incaricato,\n",
      "   [it_manoscritto] sim words:  it_a, it_da, it_proietta, it_), it_in, it_anticrisi, it_autolinee, it_sv,\n",
      "... STEP 270000 : Average Loss : 0.0864160344615\n",
      "... STEP 300000 : Average Loss : 0.0791876024566\n",
      "   [it_,] sim words:  <s>, it_piazzati, it_un'iscrizione, it_sessanta, it_verità, it_luo, it_chieri, it_sancita,\n",
      "   [it_nelle] sim words:  it_solidale, it_percento, it_weiße, it_venerato, it_autori, it_moltissimi, it_organizzando, it_produrli,\n",
      "   [it_fare] sim words:  it_la, it_cambio, it_chomsky, it_catalana, it_con, it_meccanismi, it_aiutare, it_agricola,\n",
      "   [it_manoscritto] sim words:  it_a, it_da, it_proietta, it_), it_in, it_anticrisi, it_autolinee, it_sv,\n",
      "... Training Complete\n",
      "... 300000 batches trained in 450.537431955 seconds\n"
     ]
    }
   ],
   "source": [
    "# time\n",
    "start = time.time()\n",
    "\n",
    "# training parameters\n",
    "TEST_WORDS = [3, 84, 669, 6646] # en_the, en_first, it_nuovo, it_parole\n",
    "nBATCHES = 300000 # ~ 14 epochs\n",
    "DATA_GENERATOR = batched_data\n",
    "\n",
    "# training call\n",
    "model.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = 0.15)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES:__ This is just a context of 1 (ie. window = 3) and there's no bilingual signal. When I ran it w/ the default learning rate there was mad overfitting for `the`'s neighbors but `first` had some much better results (eg. `third` and `only`). It would be interesting to really tune the hyperparamters to see how good we could do (this is essentially monolingual word2vec with two languages)... as a point of comparison for the bilingual versions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.60977550e-04,   6.87121879e-04,  -6.07755675e-04, ...,\n",
       "          1.04074273e-03,  -7.94071006e-04,  -4.09811037e-04],\n",
       "       [  6.01218257e-04,  -1.73640612e-04,   1.50801498e-04, ...,\n",
       "          9.76910815e-04,   4.34648187e-04,   1.50857595e-04],\n",
       "       [  4.21131008e-05,   1.43417434e-04,  -4.42015793e-04, ...,\n",
       "         -2.61730253e-04,   2.63188384e-04,  -3.19738436e-04],\n",
       "       ..., \n",
       "       [ -5.24837349e-04,   7.47645172e-05,  -3.87505628e-04, ...,\n",
       "          3.38180165e-04,   1.87452009e-04,  -4.23476391e-04],\n",
       "       [ -2.05034637e-04,  -1.74783956e-04,   2.54430954e-04, ...,\n",
       "         -1.96897527e-04,  -4.46256425e-04,  -4.10091743e-04],\n",
       "       [  4.34216403e-04,   9.33892716e-06,  -4.76779445e-04, ...,\n",
       "          1.30503067e-05,  -3.12790798e-05,  -4.18215437e-04]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the embeddings\n",
    "model.context_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__`Hmmmm...`__ These don't look normalized to me. Something to return to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Random Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "WINDOW_SIZE = 1\n",
    "MAX_EPOCHS = 30 # fail safe\n",
    "\n",
    "batched_data = batch_generator(en_it_data, en_it_vocab, BATCH_SIZE, \n",
    "                               WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_random\n",
    "\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "# create model\n",
    "model2 = BiW2V_random(('en', 'it'), bi_dict, en_it_vocab.to_ids,\n",
    "                      index = en_it_vocab.index, \n",
    "                      H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model2.BuildCoreGraph()\n",
    "model2.BuildTrainingGraph()\n",
    "model2.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "TEST_WORDS = [3, 84, 669, 6646] # en_the, en_first, it_nuovo, it_parole\n",
    "nBATCHES = 600000 # ~ 25 epochs\n",
    "DATA_GENERATOR = batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(48579, 128) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(48579, 128) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(48579,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP  0 : Average Loss : 0.000111448200544\n",
      "   [en_the] sim words:  en_psychical, en_slogan, it_balbo, it_race, en_1/4, it_regina, it_causando, it_[[879322]],\n",
      "   [en_first] sim words:  it_interviú, it_fermarsi, it_tanfo, it_navate, en_delivers, en_sworn, it_avvocatura, it_fortificare,\n",
      "   [it_nuovo] sim words:  en_dismissal, it_bevve, en_septa, en_implements, it_telegiornale, it_marcello, it_acciaio, it_roxx,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_ritengono, it_michail, en_الأول, it_mckenna, en_prevention,\n",
      "... STEP  60000 : Average Loss : 4.30245931981\n",
      "... STEP  120000 : Average Loss : 3.33011244143\n",
      "   [en_the] sim words:  en_a, en_,, en_., en_in, en_and, en_'s, en_of, en_slogan,\n",
      "   [en_first] sim words:  en_in, en_on, it_interviú, it_tanfo, it_fermarsi, it_navate, it_avvocatura, en_sworn,\n",
      "   [it_nuovo] sim words:  en_septa, it_bevve, en_dismissal, it_telegiornale, it_i, it_marcello, it_di, it_acciaio,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_ritengono, it_michail, en_الأول, it_mckenna, en_prevention,\n",
      "... STEP  180000 : Average Loss : 2.96933935808\n",
      "... STEP  240000 : Average Loss : 2.73646217626\n",
      "   [en_the] sim words:  en_a, en_., en_'s, en_in, en_and, en_this, en_an, en_that,\n",
      "   [en_first] sim words:  en_on, en_in, en_to, en_for, en_,, en_with, en_are, en_is,\n",
      "   [it_nuovo] sim words:  it_di, it_un, it_i, it_,, <s>, en_septa, it_bevve, it_telegiornale,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_michail, it_ritengono, en_الأول, it_mckenna, en_prevention,\n",
      "... STEP  300000 : Average Loss : 2.56025394741\n",
      "... STEP  360000 : Average Loss : 2.41503330296\n",
      "   [en_the] sim words:  en_a, en_'s, en_., en_this, en_an, en_their, en_these, en_in,\n",
      "   [en_first] sim words:  en_on, en_in, en_for, en_to, en_with, en_are, en_,, en_was,\n",
      "   [it_nuovo] sim words:  it_di, it_un, it_,, <s>, it_i, en_septa, it_con, it_è,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_michail, it_ritengono, en_الأول, en_prevention, it_mckenna,\n",
      "... STEP  420000 : Average Loss : 2.2927963642\n",
      "... STEP  480000 : Average Loss : 2.18326287779\n",
      "   [en_the] sim words:  en_a, en_'s, en_this, en_an, en_., en_their, en_these, en_his,\n",
      "   [en_first] sim words:  en_on, en_in, en_for, en_to, en_with, en_are, en_was, en_.,\n",
      "   [it_nuovo] sim words:  it_di, it_un, it_,, <s>, it_i, it_con, it_è, en_septa,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_michail, en_الأول, it_ritengono, en_prevention, it_mckenna,\n",
      "... STEP  540000 : Average Loss : 2.0917483625\n",
      "... STEP  600000 : Average Loss : 2.00766153559\n",
      "   [en_the] sim words:  en_a, en_this, en_'s, en_an, en_their, en_its, en_these, en_his,\n",
      "   [en_first] sim words:  en_on, en_in, en_for, en_to, en_with, en_are, en_., en_all,\n",
      "   [it_nuovo] sim words:  it_di, it_un, it_,, <s>, it_i, it_con, it_è, it_più,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_michail, en_الأول, it_ritengono, en_prevention, it_mckenna,\n",
      "... Training Complete\n",
      "... 600000 batches trained in 837.840520144 seconds\n"
     ]
    }
   ],
   "source": [
    "# training call\n",
    "start = time.time()\n",
    "model2.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = 0.15)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTES:`__ Same words look reasonable in the English examples. I'd be interesting in training this longer to see if that helps but its probably worth fixing the context window issue first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving final embeddings in case we want to do more stuff later\n",
    "filename = SAVE_TO + '/en_it_rand_600K_cw1_V_dec15.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model2.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "filename = SAVE_TO + '/en_it_rand_600K_cw1_U_dec15.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model2.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.42820040e-02,  -1.08593737e-03,  -9.77827888e-03, ...,\n",
       "         -7.43078999e-03,  -9.79084056e-04,  -7.75979646e-03],\n",
       "       [  6.15398725e-03,   3.99457384e-03,  -7.21636403e-04, ...,\n",
       "         -4.45156882e-04,   4.76947054e-03,   4.01509507e-03],\n",
       "       [  1.42526999e-03,  -2.62570567e-03,   6.83171733e-04, ...,\n",
       "         -2.14850740e-03,  -6.21526386e-04,   8.00127018e-05],\n",
       "       ..., \n",
       "       [ -4.83615768e-05,  -7.35577720e-04,   2.69850646e-03, ...,\n",
       "         -1.72964076e-03,   2.55509047e-03,   6.92204339e-04],\n",
       "       [  3.46941641e-03,   5.76911087e-04,   7.60798051e-04, ...,\n",
       "          3.49387084e-03,   3.47503019e-03,  -1.87479728e-03],\n",
       "       [ -7.38707022e-04,  -1.68911624e-03,  -2.75655207e-03, ...,\n",
       "         -2.23345775e-03,  -2.73358473e-03,   1.52106478e-03]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm:\n",
    "filename = SAVE_TO + '/en_it_rand_600K_cw1_U_dec15.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    C_embedding = pickle.load(f)\n",
    "    \n",
    "C_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Translation & Larger Context Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "WINDOW_SIZE = 8\n",
    "MAX_EPOCHS = 30 # fail safe\n",
    "\n",
    "batched_data = batch_generator(en_it_data, en_it_vocab, BATCH_SIZE, \n",
    "                               WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_random\n",
    "\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# create model\n",
    "model3 = BiW2V_random(('en', 'it'), bi_dict, en_it_vocab.to_ids,\n",
    "                      index = en_it_vocab.index, \n",
    "                      H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model3.BuildCoreGraph()\n",
    "model3.BuildTrainingGraph()\n",
    "model3.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(48579, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(48579, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(48579,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.000122350947062\n",
      "   [en_the] sim words:  en_diakonoff, it_abraham, it_iuta, en_coups, en_griffith, en_alter, en_upward, en_1770s,\n",
      "   [en_first] sim words:  it_firmato, en_boil, it_fiocina, it_vga, it_[[876681]], en_featural, it_risparmio, en_run,\n",
      "   [it_nuovo] sim words:  it_sabana, it_dell’antimateria, en_stabbing, en_probes, it_hetman, it_rof, it_roseto, en_restarted,\n",
      "   [it_parola] sim words:  it_d'artificio, it_rimanesse, en_infantile, it_branca, it_permesso, en_materialism, en_reunited, en_non-syndromal,\n",
      "... STEP 60000 : Average Loss : 3.43592488782\n",
      "... STEP 120000 : Average Loss : 2.32015497759\n",
      "   [en_the] sim words:  en_counteracts, en_antiquated, en_tropics, it_1829, en_spearheading, it_ambientati, en_nvt, it_amburgo,\n",
      "   [en_first] sim words:  it_arresero, it_pozzo, en_boil, en_unique, it_retrocedendo, it_acquisiti, it_risparmio, en_reading,\n",
      "   [it_nuovo] sim words:  it_hetman, it_sabana, en_probes, en_stabbing, it_rof, it_dell’antimateria, en_prudhoe, it_roseto,\n",
      "   [it_parola] sim words:  it_d'artificio, it_rimanesse, en_infantile, it_branca, en_non-syndromal, it_permesso, it_specie, en_materialism,\n",
      "... STEP 180000 : Average Loss : 1.96030490431\n",
      "... STEP 240000 : Average Loss : 1.76807048526\n",
      "   [en_the] sim words:  it_ambientati, en_this, it_frassino, it_alghe, it_mf, en_governors, en_nvt, it_l'orchestra,\n",
      "   [en_first] sim words:  it_arresero, it_pozzo, it_retrocedendo, en_patents, it_incanti, en_boil, en_tossing, it_ghignante,\n",
      "   [it_nuovo] sim words:  it_hetman, it_sabana, en_prudhoe, en_probes, en_stabbing, it_rof, it_sottogruppo, it_dell’antimateria,\n",
      "   [it_parola] sim words:  it_d'artificio, it_rimanesse, en_infantile, en_non-syndromal, it_branca, it_permesso, it_miracoli, en_reunited,\n",
      "... STEP 300000 : Average Loss : 1.62638815477\n",
      "... STEP 360000 : Average Loss : 1.508576124\n",
      "   [en_the] sim words:  en_counteracts, en_guava, en_spontaneity, it_bakr, en_assistance, en_mid, it_scaduto, en_tropics,\n",
      "   [en_first] sim words:  it_arresero, en_largest, it_pozzo, it_incanti, en_patents, en_tossing, it_ghignante, it_retrocedendo,\n",
      "   [it_nuovo] sim words:  it_hetman, it_sabana, en_prudhoe, en_probes, it_dorfe, it_rof, it_sottogruppo, en_criminal,\n",
      "   [it_parola] sim words:  it_d'artificio, en_non-syndromal, en_infantile, it_rimanesse, it_branca, it_miracoli, it_permesso, en_reunited,\n",
      "... STEP 420000 : Average Loss : 1.40801186306\n",
      "... STEP 480000 : Average Loss : 1.3143499378\n",
      "   [en_the] sim words:  it_frassino, en_spontaneity, en_fernando, it_bagno, en_antiquated, en_1,427, en_counteracts, it_riformista,\n",
      "   [en_first] sim words:  en_largest, it_arresero, it_berbera, it_incanti, it_pozzo, en_2009, en_tossing, it_vignali,\n",
      "   [it_nuovo] sim words:  it_hetman, it_jorge, it_dorfe, it_myślibórz, en_anthem, it_sabana, en_probes, en_criminal,\n",
      "   [it_parola] sim words:  it_d'artificio, en_non-syndromal, en_infantile, it_rimanesse, en_reunited, it_branca, it_permesso, it_miracoli,\n",
      "... STEP 540000 : Average Loss : 1.22699242282\n",
      "... STEP 600000 : Average Loss : 1.14706762033\n",
      "   [en_the] sim words:  en_assistance, en_spontaneity, en_antiquated, en_actium, en_a, it_frassino, it_scaduto, en_selfless,\n",
      "   [en_first] sim words:  en_largest, it_berbera, en_figurative, it_arresero, it_pozzo, en_patents, en_melts, en_2009,\n",
      "   [it_nuovo] sim words:  it_jorge, it_hetman, en_anthem, it_dorfe, it_myślibórz, en_probes, it_sottogruppo, en_prudhoe,\n",
      "   [it_parola] sim words:  it_d'artificio, en_non-syndromal, en_infantile, it_rimanesse, en_reunited, it_permesso, it_miracoli, it_divo,\n",
      "... Training Complete\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "nBATCHES = 600000 # ~ 25 epochs\n",
    "DATA_GENERATOR = batched_data\n",
    "TEST_WORDS = [3, 84, 669, 6646]\n",
    "\n",
    "# training call\n",
    "model3.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES:__ Interesting, the larger context seems to hurt the training performance. Is this because of the extra padding? Or will a smart adjustment of the learning rate redeem this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrds = \"en_the en_a en_this en_'s en_an en_their en_its en_these en_his \\\n",
    "       en_first en_on en_in en_for en_to en_with en_are en_. en_all \\\n",
    "       it_nuovo it_di it_un it_, <s> it_i it_con it_è it_più it_parola \\\n",
    "       en_censorship en_minima it_profesional it_michail en_الأول \\\n",
    "       it_ritengono en_prevention it_mckenna en_third\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in [\"en_the\", \"en_first\", \"it_nuovo\", \"it_parole\"]:\n",
    "    wrds += list(bi_dict[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordset = set(en_it_vocab.to_ids(wrds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'TSNE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-87e85cb87b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_embeddings_in_2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mmillervedam/Documents/MIDS/w266/FinalProject/Notebooks/models.py\u001b[0m in \u001b[0;36mplot_embeddings_in_2D\u001b[0;34m(self, wordset)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must train the embeddings before plotting.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0mlow_dim_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'TSNE' is not defined"
     ]
    }
   ],
   "source": [
    "model2.plot_embeddings_in_2D(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
