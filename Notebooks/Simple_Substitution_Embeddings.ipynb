{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Substitution\n",
    "`w266 Final Project: Crosslingual Word Embeddings`\n",
    "\n",
    "The code in this notebook was used to develop an algorithm to generate crosslingual word embeddings by training on a monolingual corpus and substituting translations at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filepaths\n",
    "BASE = '/Users/mmillervedam/Documents/MIDS/w266' #'/home/mmillervedam/' \n",
    "PROJ = '/Users/mmillervedam/Documents/MIDS/w266/FinalProject'#'/home/mmillervedam/ProjectRepo'\n",
    "FPATH_EN = BASE + '/Data/test/wiki_en_10K.txt' # first 10000 lines from wiki dump\n",
    "FPATH_ES = BASE + '/Data/test/wiki_es_10K.txt' # first 10000 lines from wiki dump\n",
    "#FULL_EN = BASE + '/Data/en/full.txt'\n",
    "#FULL_ES = BASE + '/Data/es/full.txt'\n",
    "EN_ES_DICT = PROJ +'/XlingualEmb/data/dicts/en.es.panlex.all.processed'\n",
    "EN_IT_DICT  = PROJ +'/XlingualEmb/data/dicts/en.it.panlex.all.processed'\n",
    "EN_IT_RAW = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'\n",
    "EN_IT_RAW = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory to save pickled embeddings\n",
    "SAVE_TO = PROJ + '/Notebooks/embeddings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Preprocess Data\n",
    "__`ORIGINAL AUTHORS SAY:`__ \"Normally, the monolingual word embeddings are trained on billions of words. However, getting that much of monolingual data for a low-resource language is also challenging. That is why we only select the top 5 million sentences (around 100 million words) for each language.\" - _Section 5.1, Duong et. al._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import Corpus, Vocabulary, batch_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load corpus\n",
    "en_it_data = Corpus(EN_IT_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20000  430928 3746786 /Users/mmillervedam/Documents/MIDS/w266/FinalProject/XlingualEmb/data/mono/en_it.shuf.10k\r\n"
     ]
    }
   ],
   "source": [
    "# Corpus Stats\n",
    "!wc {EN_IT_RAW}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`i.e.:`__ 20K sentences (10K in each language) with ~430K tokens\n",
    "> So this must not be their full data For now, I'm just going to look at the top 20K words and see what happens. In reality we should probably modify the Vocab class so that it explicily collects the top words for each language separately and then concatenates the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading english-italian dictionary\n",
    "pld = pd.read_csv(EN_IT_DICT, sep='\\t', names = ['en', 'it'], dtype=str)\n",
    "en_set = set(pld.en.unique())\n",
    "it_set = set(pld.it.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: 266450\n",
      "IT: 258641\n"
     ]
    }
   ],
   "source": [
    "# dictionary vocab lengths:\n",
    "print('EN:', len(en_set))\n",
    "print('IT:', len(it_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary for ease of runtime translation\n",
    "# WARNING this takes a sec to run\n",
    "bi_dict = pld.groupby(['en'])['it'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add other direction\n",
    "# WARNING this takes another sec to run\n",
    "bi_dict.update(pld.groupby(['it'])['en'].unique().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it_aggirare', 'it_andai', 'it_andara', 'it_andare',\n",
       "       'it_andare_avanti'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo en to it\n",
    "bi_dict['en_go'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en_adieu', 'en_bye-bye', 'en_bye', 'en_cheerio', 'en_ciao'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo it to en\n",
    "bi_dict['it_ciao'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train multilingual Vocabulary\n",
    "en_it_vocab = Vocabulary(en_it_data.gen_tokens(), size = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48579"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of corpus vocabulary\n",
    "en_it_vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24176"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overlap with dictionary vocabulary\n",
    "len([w for w in en_it_vocab.types if w in bi_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Data Generator\n",
    "__`CHECK PAPER for HYPERPARAMS!`__: I can't seem to find where they talk abou the context window size, embedding size and batch size they use -- it may actually be in the Vulic and Moens paper instead of the Duong one.\n",
    "\n",
    "__`RLH Update`__: Duong et al. section 6, footnote 4: \"Default learning rate of 0.025, negative sampling with 25 samples, subsampling rate of value 1eâˆ’4, embedding dimension d = 200, window size cs = 48 and run for 15 epochs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "WINDOW_SIZE = 1\n",
    "MAX_EPOCHS = 1 # fail safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batched_data = batch_generator(en_it_data, \n",
    "                               en_it_vocab, \n",
    "                               BATCH_SIZE, \n",
    "                               WINDOW_SIZE, \n",
    "                               MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT IDS: [[0, 1], [0, 1], [0, 34], [20, 15624], [34, 1584]]\n",
      "CONTEXT: [['<s>', '</s>'], ['<s>', '</s>'], ['<s>', 'it_un'], ['it_in', 'it_remoto'], ['it_un', 'it_passato']]\n",
      "LABEL IDS: [43790, 24849, 20, 34, 15624]\n",
      "LABELS: ['it_[[877881]]', 'it_[[879362]]', 'it_in', 'it_un', 'it_remoto']\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "for context, label in batched_data:\n",
    "    print(\"CONTEXT IDS:\", context[:5])\n",
    "    print(\"CONTEXT:\", [en_it_vocab.to_words(c) for c in context[:5]])\n",
    "    print(\"LABEL IDS:\", label[:5])\n",
    "    print(\"LABELS:\", en_it_vocab.to_words(label[:5]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun Validation Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 84, 669, 6646]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_it_vocab.to_ids(['en_the','en_first', 'it_nuovo', 'it_parola'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it_della', 'it_gli', 'it_i', 'it_il', 'it_la', 'it_l\\xc3\\xa0',\n",
       "       'it_le', 'it_lo', 'it_ma'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['en_the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it_anteriore', 'it_anteriormente', 'it_antico', 'it_anzitutto',\n",
       "       'it_anzi_tutto', 'it_avvio', 'it_dapprima', 'it_davanti',\n",
       "       'it_di_fronte', 'it_il_primo', 'it_in', 'it_in_cima', 'it_inizio',\n",
       "       'it_innanzitutto', 'it_innanzi_tutto', 'it_in_primis',\n",
       "       'it_in_primo_luogo', 'it_la_prima', 'it_per_la_prima_volta',\n",
       "       'it_per_primo', 'it_precedente', 'it_prima', 'it_prima_di_tutto',\n",
       "       'it_primariamente', 'it_primario', 'it_prime', 'it_primieramente',\n",
       "       'it_primiero', 'it_primo', 'it_principale', 'it_principio'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['en_first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en_fresh', 'en_green', 'en_latter-day', 'en_new', 'en_novel',\n",
       "       'en_raw', 'en_recent', 'en_renewed', 'en_unexampled', 'en_unused',\n",
       "       'en_young'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['it_nuovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en_drake', 'en_language', 'en_mot', 'en_parole', 'en_promise',\n",
       "       'en_-shaped', 'en_speech', 'en_term', 'en_tongue', 'en_verb',\n",
       "       'en_vocable', 'en_word_of_honor', 'en_word'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['it_parola']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model - no word sub yet!\n",
    "__`CODE NOTES:`__ To get this running I had to hard code the context length (set to 2) inside `BuildCoreGraph()` where we generate `self.input_` in line 102. That should really be inferred from the `self.context_` itself but it doesn't seem to like the placeholder dimension (we don't have a span length until runtime). Does tensorflow not have a vectorized average? Something to fix (later). I also had to hard code the number of samples for softmax (I had originally put this as a `tf.placeholder_with_default` thinking we could pass it in to the training function (since its a training parameter) but TF kicked out an error message asking for an integer so for now I'll just give it what it wants. I need to think more about why TF doesn't want this changing from batch to batch. (or if there is another reason it wants an int)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "WINDOW_SIZE = 1\n",
    "MAX_EPOCHS = 15 # fail safe\n",
    "\n",
    "batched_data = batch_generator(en_it_data, en_it_vocab, BATCH_SIZE, \n",
    "                               WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V\n",
    "\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# create model\n",
    "model = BiW2V(index = en_it_vocab.index, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model.BuildCoreGraph()\n",
    "model.BuildTrainingGraph()\n",
    "model.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "__`IMPORTANT!`__ right now the model only works with a window of 1 because the feed dict can't handle context windows of different lengths. We'll either need to figure out how to have a variable length dimension or else add extra padding to the sentences to account for the window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(48579, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(48579, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(48579,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP  0 : Average Loss : 0.00021203883489\n",
      "   >>> [en_the] nbrs:  it_perde, en_manager, it_realizzarsi, en_overseeing, it_flags, en_troad, it_chiusi, it_reintegrata,\n",
      "   >>> [en_first] nbrs:  en_anarcho-pacifism, it_suggerimento, en_boroughs, en_teesside, en_deprived, en_bulletin, en_predation, it_gobbo,\n",
      "   >>> [it_nuovo] nbrs:  it_indifferentemente, it_porche, it_bondi, it_carlo, it_indennizzare, en_principlism, en_consult, it_nicaragua,\n",
      "   >>> [it_parola] nbrs:  it_configurazione, it_l'eterogeneitÃ , en_belleville, en_outsiders, en_respectively, it_mercatini, it_[[877979]], it_calcificazione,\n",
      "... STEP  30000 : Average Loss : 3.95838090201\n",
      "... STEP  60000 : Average Loss : 2.98953760585\n",
      "   >>> [en_the] nbrs:  en_a, en_an, en_., en_in, en_his, en_'s, en_its, en_and,\n",
      "   >>> [en_first] nbrs:  en_anarcho-pacifism, it_suggerimento, en_in, en_teesside, en_boroughs, en_deprived, en_\", en_bulletin,\n",
      "   >>> [it_nuovo] nbrs:  it_indifferentemente, it_porche, it_carlo, it_bondi, it_grinch, it_indennizzare, it_nicaragua, it_[[879106]],\n",
      "   >>> [it_parola] nbrs:  it_configurazione, it_l'eterogeneitÃ , en_belleville, en_outsiders, it_[[877979]], en_respectively, it_mercatini, it_calcificazione,\n",
      "... STEP  90000 : Average Loss : 2.61610243719\n",
      "... STEP  120000 : Average Loss : 2.3578601825\n",
      "   >>> [en_the] nbrs:  en_a, en_an, en_his, en_its, en_'s, en_this, en_their, en_.,\n",
      "   >>> [en_first] nbrs:  en_in, en_anarcho-pacifism, en_on, en_\", it_suggerimento, en_by, en_for, en_during,\n",
      "   >>> [it_nuovo] nbrs:  it_indifferentemente, it_carlo, it_porche, en_\", it_grinch, it_bondi, it_[[879106]], it_',\n",
      "   >>> [it_parola] nbrs:  it_configurazione, it_l'eterogeneitÃ , en_belleville, en_outsiders, it_[[877979]], it_calcificazione, en_respectively, it_mercatini,\n",
      "... STEP  150000 : Average Loss : 2.17127792774\n",
      "... STEP  180000 : Average Loss : 2.00235680762\n",
      "   >>> [en_the] nbrs:  en_a, en_an, en_its, en_his, en_this, en_'s, en_their, en_s,\n",
      "   >>> [en_first] nbrs:  en_anarcho-pacifism, en_on, en_in, en_during, en_\", en_by, it_suggerimento, en_for,\n",
      "   >>> [it_nuovo] nbrs:  it_indifferentemente, en_\", it_', it_carlo, it_porche, it_grinch, it_[[879106]], en_1541,\n",
      "   >>> [it_parola] nbrs:  it_configurazione, it_l'eterogeneitÃ , en_belleville, en_outsiders, it_calcificazione, it_[[877979]], it_mercatini, en_respectively,\n",
      "... STEP  210000 : Average Loss : 1.87448061404\n",
      "... STEP  240000 : Average Loss : 1.76584294785\n",
      "   >>> [en_the] nbrs:  en_a, en_its, en_an, en_his, en_this, en_their, en_'s, en_many,\n",
      "   >>> [en_first] nbrs:  en_anarcho-pacifism, en_on, en_during, en_\", en_by, en_in, it_suggerimento, en_for,\n",
      "   >>> [it_nuovo] nbrs:  it_', it_indifferentemente, en_\", it_carlo, it_porche, it_[[879106]], it_grinch, it_18,\n",
      "   >>> [it_parola] nbrs:  it_configurazione, it_l'eterogeneitÃ , en_belleville, it_calcificazione, it_[[877979]], en_outsiders, it_mercatini, en_terms,\n",
      "... STEP  270000 : Average Loss : 1.66933886879\n",
      "... STEP  300000 : Average Loss : 1.58978501598\n",
      "   >>> [en_the] nbrs:  en_its, en_a, en_an, en_his, en_this, en_their, en_'s, en_her,\n",
      "   >>> [en_first] nbrs:  en_during, en_anarcho-pacifism, en_on, en_\", en_by, it_suggerimento, en_in, en_teesside,\n",
      "   >>> [it_nuovo] nbrs:  it_', it_indifferentemente, en_\", it_carlo, it_con, it_[[879106]], it_porche, it_18,\n",
      "   >>> [it_parola] nbrs:  it_configurazione, it_l'eterogeneitÃ , en_belleville, it_calcificazione, it_[[877979]], en_outsiders, en_terms, it_mercatini,\n",
      "... Training Complete\n",
      "... 300000 batches trained in 418.807106018 seconds\n"
     ]
    }
   ],
   "source": [
    "# time\n",
    "start = time.time()\n",
    "\n",
    "# training parameters\n",
    "TEST_WORDS = [3, 84, 669, 6646] # en_the, en_first, it_nuovo, it_parole\n",
    "nBATCHES = 300000 # ~ 14 epochs\n",
    "DATA_GENERATOR = batched_data\n",
    "\n",
    "# training call\n",
    "model.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = 0.15)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES:__ This is just a context of 1 (ie. window = 3) and there's no bilingual signal. When I ran it w/ the default learning rate there was mad overfitting for `the`'s neighbors but `first` had some much better results (eg. `third` and `only`). It would be interesting to really tune the hyperparamters to see how good we could do (this is essentially monolingual word2vec with two languages)... as a point of comparison for the bilingual versions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.02402642e-03,   4.16290626e-04,   8.46864350e-05, ...,\n",
       "         -1.02722656e-03,  -7.92390842e-04,   8.39135144e-04],\n",
       "       [  3.52726347e-04,  -4.58918861e-04,   2.22952978e-04, ...,\n",
       "         -4.55987407e-04,   8.35586034e-05,   2.18325004e-04],\n",
       "       [  4.07747721e-04,  -4.21624805e-04,  -3.38321872e-04, ...,\n",
       "          9.61679689e-06,   3.33360600e-04,   2.10861690e-04],\n",
       "       ..., \n",
       "       [ -4.02371108e-04,   3.36044555e-04,   2.32033111e-04, ...,\n",
       "          2.27759840e-04,   1.72980988e-04,   3.51097609e-04],\n",
       "       [ -2.04241878e-04,  -1.52394496e-04,   3.81836639e-04, ...,\n",
       "          1.73038512e-04,  -1.08486944e-04,   1.55255693e-04],\n",
       "       [  1.15991745e-04,  -3.61194252e-04,  -3.92610062e-04, ...,\n",
       "          5.17776061e-04,   4.07395535e-04,   2.53971462e-04]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the embeddings\n",
    "model.context_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__`Hmmmm...`__ These don't look normalized to me. Something to return to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Random Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "WINDOW_SIZE = 1\n",
    "MAX_EPOCHS = 30 # fail safe\n",
    "\n",
    "batched_data = batch_generator(en_it_data, en_it_vocab, BATCH_SIZE, \n",
    "                               WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_random\n",
    "\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "# create model\n",
    "model2 = BiW2V_random(('en', 'it'), bi_dict, en_it_vocab.to_ids,\n",
    "                      index = en_it_vocab.index, \n",
    "                      H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model2.BuildCoreGraph()\n",
    "model2.BuildTrainingGraph()\n",
    "model2.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "TEST_WORDS = [3, 84, 669, 6646] # en_the, en_first, it_nuovo, it_parole\n",
    "nBATCHES = 600000 # ~ 14 epochs\n",
    "DATA_GENERATOR = batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(48579, 128) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(48579, 128) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(48579,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP  0 : Average Loss : 0.000111448200544\n",
      "   [en_the] sim words:  en_psychical, en_slogan, it_balbo, it_race, en_1/4, it_regina, it_causando, it_[[879322]],\n",
      "   [en_first] sim words:  it_interviÃº, it_fermarsi, it_tanfo, it_navate, en_delivers, en_sworn, it_avvocatura, it_fortificare,\n",
      "   [it_nuovo] sim words:  en_dismissal, it_bevve, en_septa, en_implements, it_telegiornale, it_marcello, it_acciaio, it_roxx,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_ritengono, it_michail, en_Ø§Ù„Ø£ÙˆÙ„, it_mckenna, en_prevention,\n",
      "... STEP  60000 : Average Loss : 4.30245931981\n",
      "... STEP  120000 : Average Loss : 3.33011244143\n",
      "   [en_the] sim words:  en_a, en_,, en_., en_in, en_and, en_'s, en_of, en_slogan,\n",
      "   [en_first] sim words:  en_in, en_on, it_interviÃº, it_tanfo, it_fermarsi, it_navate, it_avvocatura, en_sworn,\n",
      "   [it_nuovo] sim words:  en_septa, it_bevve, en_dismissal, it_telegiornale, it_i, it_marcello, it_di, it_acciaio,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_ritengono, it_michail, en_Ø§Ù„Ø£ÙˆÙ„, it_mckenna, en_prevention,\n",
      "... STEP  180000 : Average Loss : 2.96933935808\n",
      "... STEP  240000 : Average Loss : 2.73646217626\n",
      "   [en_the] sim words:  en_a, en_., en_'s, en_in, en_and, en_this, en_an, en_that,\n",
      "   [en_first] sim words:  en_on, en_in, en_to, en_for, en_,, en_with, en_are, en_is,\n",
      "   [it_nuovo] sim words:  it_di, it_un, it_i, it_,, <s>, en_septa, it_bevve, it_telegiornale,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_michail, it_ritengono, en_Ø§Ù„Ø£ÙˆÙ„, it_mckenna, en_prevention,\n",
      "... STEP  300000 : Average Loss : 2.56025394741\n",
      "... STEP  360000 : Average Loss : 2.41503330296\n",
      "   [en_the] sim words:  en_a, en_'s, en_., en_this, en_an, en_their, en_these, en_in,\n",
      "   [en_first] sim words:  en_on, en_in, en_for, en_to, en_with, en_are, en_,, en_was,\n",
      "   [it_nuovo] sim words:  it_di, it_un, it_,, <s>, it_i, en_septa, it_con, it_Ã¨,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_michail, it_ritengono, en_Ø§Ù„Ø£ÙˆÙ„, en_prevention, it_mckenna,\n",
      "... STEP  420000 : Average Loss : 2.2927963642\n",
      "... STEP  480000 : Average Loss : 2.18326287779\n",
      "   [en_the] sim words:  en_a, en_'s, en_this, en_an, en_., en_their, en_these, en_his,\n",
      "   [en_first] sim words:  en_on, en_in, en_for, en_to, en_with, en_are, en_was, en_.,\n",
      "   [it_nuovo] sim words:  it_di, it_un, it_,, <s>, it_i, it_con, it_Ã¨, en_septa,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_michail, en_Ø§Ù„Ø£ÙˆÙ„, it_ritengono, en_prevention, it_mckenna,\n",
      "... STEP  540000 : Average Loss : 2.0917483625\n",
      "... STEP  600000 : Average Loss : 2.00766153559\n",
      "   [en_the] sim words:  en_a, en_this, en_'s, en_an, en_their, en_its, en_these, en_his,\n",
      "   [en_first] sim words:  en_on, en_in, en_for, en_to, en_with, en_are, en_., en_all,\n",
      "   [it_nuovo] sim words:  it_di, it_un, it_,, <s>, it_i, it_con, it_Ã¨, it_piÃ¹,\n",
      "   [it_parola] sim words:  en_censorship, en_minima, it_profesional, it_michail, en_Ø§Ù„Ø£ÙˆÙ„, it_ritengono, en_prevention, it_mckenna,\n",
      "... Training Complete\n",
      "... 600000 batches trained in 837.840520144 seconds\n"
     ]
    }
   ],
   "source": [
    "# training call\n",
    "start = time.time()\n",
    "model2.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = 0.05)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTES:`__ Same words look reasonable in the English examples. I'd be interesting in training this longer to see if that helps but its probably worth fixing the context window issue first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving final embeddings in case we want to do more stuff later\n",
    "filename = SAVE_TO + '/en_it_rand_600K_cw1_V_dec15.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model2.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "filename = SAVE_TO + '/en_it_rand_600K_cw1_U_dec15.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model2.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.42820040e-02,  -1.08593737e-03,  -9.77827888e-03, ...,\n",
       "         -7.43078999e-03,  -9.79084056e-04,  -7.75979646e-03],\n",
       "       [  6.15398725e-03,   3.99457384e-03,  -7.21636403e-04, ...,\n",
       "         -4.45156882e-04,   4.76947054e-03,   4.01509507e-03],\n",
       "       [  1.42526999e-03,  -2.62570567e-03,   6.83171733e-04, ...,\n",
       "         -2.14850740e-03,  -6.21526386e-04,   8.00127018e-05],\n",
       "       ..., \n",
       "       [ -4.83615768e-05,  -7.35577720e-04,   2.69850646e-03, ...,\n",
       "         -1.72964076e-03,   2.55509047e-03,   6.92204339e-04],\n",
       "       [  3.46941641e-03,   5.76911087e-04,   7.60798051e-04, ...,\n",
       "          3.49387084e-03,   3.47503019e-03,  -1.87479728e-03],\n",
       "       [ -7.38707022e-04,  -1.68911624e-03,  -2.75655207e-03, ...,\n",
       "         -2.23345775e-03,  -2.73358473e-03,   1.52106478e-03]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm:\n",
    "filename = SAVE_TO + '/en_it_rand_600K_cw1_U_dec15.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    C_embedding = pickle.load(f)\n",
    "    \n",
    "C_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viz and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrds = \"en_the en_a en_this en_'s en_an en_their en_its en_these en_his \\\n",
    "       en_first en_on en_in en_for en_to en_with en_are en_. en_all \\\n",
    "       it_nuovo it_di it_un it_, <s> it_i it_con it_Ã¨ it_piÃ¹ it_parola \\\n",
    "       en_censorship en_minima it_profesional it_michail en_Ø§Ù„Ø£ÙˆÙ„ \\\n",
    "       it_ritengono en_prevention it_mckenna en_third\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w in [\"en_the\", \"en_first\", \"it_nuovo\", \"it_parole\"]:\n",
    "    wrds.append(bi_dict[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-683037c12760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_it_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mmillervedam/Documents/MIDS/w266/FinalProject/Notebooks/parsing.pyc\u001b[0m in \u001b[0;36mto_ids\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNK_ID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "wordset = set(en_it_vocab.to_ids(list(wrds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en_fresh', 'en_green', 'en_latter-day', 'en_new', 'en_novel',\n",
       "       'en_raw', 'en_recent', 'en_renewed', 'en_unexampled', 'en_unused',\n",
       "       'en_young'], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict['it_nuovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.plot_embeddings_in_2D()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
