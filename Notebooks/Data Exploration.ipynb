{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Exploration\n",
    "__`w266 Final Project | MIDS Fall 2017`__\n",
    "\n",
    "This notebook contains the code to investigate the data available from Duong et al's repo plus an intial forray into the [PanLex API](https://dev.panlex.org/api/), and [Polyglot python package](http://polyglot.readthedocs.io/en/latest/Embeddings.html).\n",
    "\n",
    "## Contents\n",
    "* [Original Data](#Original-Data)\n",
    "* [Polyglot Embeddings](#Polyglot-Embeddings)\n",
    "* [Polyglot Wikipedia Dumps](#Polyglot-Wikipedia-Dumps)\n",
    "* [PanLex API](#PanLex-API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "HOME = '/home/mmillervedam/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 138168\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam 30951284 Nov 30 19:25 en.de.panlex.all.processed\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam  8626690 Nov 30 19:25 en.el.panlex.all.processed\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam 26496021 Nov 30 19:25 en.es.panlex.all.processed\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam 19429394 Nov 30 19:25 en.fi.panlex.all.processed\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam 17820708 Nov 30 19:25 en.it.panlex.all.processed\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam 24258057 Nov 30 19:25 en.ja.panlex.all.processed\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam 12340490 Nov 30 19:25 en.nl.panlex.all.processed\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam  1541535 Nov 30 19:25 en.sr.panlex.all.processed\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam        1 Nov 30 19:25 README.md\n",
      "total 3664\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam 3746786 Nov 30 19:25 en_it.shuf.10k\n",
      "-rw-rw-r-- 1 mmillervedam mmillervedam       1 Nov 30 19:25 README.md\n"
     ]
    }
   ],
   "source": [
    "# What files are available?\n",
    "!ls -l ../XlingualEmb/data/dicts\n",
    "!ls -l ../XlingualEmb/data/mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_0\tes_cero\r\n",
      "en_0_or_1_matches\tes_0_ó_1_coincidencias\r\n",
      "en_0_or_more_matches\tes_0_o_más_coincidencias\r\n",
      "en_1000000000000\tes_billón\r\n",
      "en_1000000000\tes_billón\r\n"
     ]
    }
   ],
   "source": [
    "# What do the panlex processsed files look like?\n",
    "!head -n 5 ../XlingualEmb/data/dicts/en.es.panlex.all.processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_[[877881]]\r\n",
      "it_[[879362]]\r\n",
      "it_in it_un it_remoto it_passato it_aveva it_progettato it_, it_per it_conto it_dei it_demoniazzi it_silastici it_di it_striterax it_, it_una it_bomba it_in it_grado it_di it_collegare it_simultaneamente it_tutti it_i it_nuclei it_di it_tutte it_le it_stelle it_, it_creando it_così it_un'immensa it_supernova it_che it_avrebbe it_distrutto it_l'universo it_, it_secondo it_i it_desideri it_dei it_demoniazzi it_silastici it_.\r\n",
      "it_krikkitesi it_i it_krikkitesi it_sono it_una it_razza it_aliena it_che it_per it_miliardi it_di it_anni it_aveva it_vissuto it_senza it_la it_minima it_consapevolezza it_dell'esistenza it_di it_altri it_mondi it_o it_altre it_specie it_.\r\n",
      "en_as en_the en_patron en_of en_delphi en_( en_pythian en_apollo en_) en_, en_apollo en_was en_an en_oracular en_god en_— en_the en_prophetic en_deity en_of en_the en_delphic en_oracle en_.\r\n",
      "it_all'inizio it_del it_2006 it_ha it_pubblicato it_il it_suo it_primo it_singolo it_solista it_, it_nell'angolo it_, it_con it_la it_partecipazione it_dello it_stesso it_zero it_, it_che it_ha it_collaborato it_anche it_alla it_stesura it_del it_testo it_.\r\n",
      "en_medieval en_muslim en_scholars en_regularly en_described en_aristotle en_as en_the en_\" en_first en_teacher en_\" en_.\r\n",
      "it_[[876688]]\r\n"
     ]
    }
   ],
   "source": [
    "# What does the monolingual file look like?\n",
    "!head -n 8 ../XlingualEmb/data/mono/en_it.shuf.10k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polyglot Wikipedia Dumps\n",
    "\n",
    "These files are made available from [Rami Al Rfou's website](https://sites.google.com/site/rmyeid/projects/polyglot#TOC-Download-Wikipedia-Text-Dumps) and are protected by a creative commons liscence in association with the following publication:  \n",
    "\n",
    "__Citation:__ [Polyglot: Distributed Word Representations for Multilingual NLP](http://www.aclweb.org/anthology/W13-3520), \n",
    "Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. \n",
    "In Proceedings Seventeenth Conference on Computational Natural Language Learning (CoNLL 2013).\n",
    "> `@InProceedings{polyglot:2013:ACL-CoNLL,\n",
    "  author    = {Al-Rfou, Rami  and  Perozzi, Bryan  and  Skiena, Steven},\n",
    "  title     = {Polyglot: Distributed Word Representations for Multilingual NLP},\n",
    "  booktitle = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},\n",
    "  month     = {August},\n",
    "  year      = {2013},\n",
    "  address   = {Sofia, Bulgaria},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {183--192}, \n",
    "  url       = {http://www.aclweb.org/anthology/W13-3520}\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Doc Downloading Script\n",
    "Since the compressed wikipedia text files are so large a simple `wget` or `curl` command won't do. The code below came from [this SO post](https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive) -- it uses the requests package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file_from_google_drive(id, destination):\n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "\n",
    "        return None\n",
    "\n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download English Corpus\n",
    "__File:__ `en_wiki_text.tar.lzma`\n",
    "__Google Doc ID:__ `0B5lWReQPSvmGOTNxdHo3b0lMc3c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file_from_google_drive('0B5lWReQPSvmGOTNxdHo3b0lMc3c', HOME +'Data/en_wiki_text.tar.lzma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download Spanish Corpus\n",
    "__File:__ `es_wiki_text.tar.lzma` __Google Doc ID:__ `0B5lWReQPSvmGOXdCZEZPSnZoYXc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 8.72 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "download_file_from_google_drive('0B5lWReQPSvmGOXdCZEZPSnZoYXc', HOME +'Data/es_wiki_text.tar.lzma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the French Corpus\n",
    "__File:__ `fr_wiki_text.tar.lzma` __Google Doc ID:__ `0B5lWReQPSvmGdkIxeVJESWcyVU0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 9.43 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "download_file_from_google_drive('0B5lWReQPSvmGdkIxeVJESWcyVU0', HOME +'Data/fr_wiki_text.tar.lzma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Japanese Corpus\n",
    "__File:__ `ja_wiki_text.tar.lzma` __Google Doc ID:__ `0B5lWReQPSvmGYzlWMC1KcV9kVzQ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_file_from_google_drive('0B5lWReQPSvmGYzlWMC1KcV9kVzQ', HOME +'Data/ja_wiki_text.tar.lzma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decompress the files\n",
    "Next you'll need to decompress these files. Do this from your terminal after navigating to the `Data` folder (see linux commands below). Read more about the [tar command](https://www.howtogeek.com/248780/how-to-compress-and-extract-files-using-the-tar-command-on-linux/) or [lzma compressed files](https://www.lifewire.com/lzma-file-2621951) and [here](https://fileinfo.com/extension/lzma).\n",
    "> `cd /home/mmillervedam/Data`  \n",
    "> `tar --lzma -xvpf en_wiki_text.tar.lzma`    \n",
    "> `tar --lzma -xvpf es_wiki_text.tar.lzma`  \n",
    "> `tar --lzma -xvpf fr_wiki_text.tar.lzma`    \n",
    "> `tar --lzma -xvpf ja_wiki_text.tar.lzma`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   88083626 /home/mmillervedam/Data/en/full.txt\n",
      "   18833490 /home/mmillervedam/Data/es/full.txt\n",
      "   23856824 /home/mmillervedam/Data/fr/full.txt\n",
      "   52875002 /home/mmillervedam/Data/ja/full.txt\n",
      "  183648942 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l {HOME}Data/*/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12]]\r\n",
      "Anarchism is often defined as a political philosophy which holds the state to be undesirable , unnecessary , or harmful .\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 {HOME}Data/en/full.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7]]\r\n",
      "El Principado de Andorra ( en catalán : Principat d'Andorra ) es un pequeño principado soberano del suroeste de Europa con una extensión de 468 km2 , situado en los Pirineos entre España y Francia , con una altitud media de 1.996 metros sobre el nivel del mar .\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 {HOME}Data/es/full.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]]\r\n",
      "Paul Jules Antoine Meillet , né le à Moulins ( Allier ) et mort le à Châteaumeillant ( Cher ), est le principal linguiste français des premières décennies du .\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 {HOME}Data/fr/full.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5]]\r\n",
      "アンパサンド\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 {HOME}Data/ja/full.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polyglot Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PanLex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
