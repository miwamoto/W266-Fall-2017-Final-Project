{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW Implementation on TF\n",
    "`w266 Final Project: Crosslingual Word Embeddings`   \n",
    "\n",
    "The code in this notebook is based on the Skip-Gram model in the [TensorFlow tutorial code](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py). I will first attempt to the basic Word2Vec algorithm to a sample of our data (Wikipedia dumps in English). Then I'll examine different ways of visualizing the embeddings that result. Finally I will explore what it might look like to make [Duong et al's modifications](https://arxiv.org/pdf/1606.09403.pdf) to train crosslingual embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Overview \n",
    "\n",
    "__Basic Idea__: start with 1-hot vector, pass it through a linear activation layer then into a softmax and optimize for the probability of nearby words(Skipgram) or the centerword(CBOW). The 'embeddings' are the parameters of the linear activation (which transform the vector of size $|V|$ into an embedding of size $N$:\n",
    "$$\\text{Weight Matrix:}\\qquad W \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "$$\\text{Bias (?):}\\qquad b \\in \\mathbb{R}^{n}$$\n",
    "\n",
    "__Key Modifications:__ \n",
    "* Duong et all use a CBOW style algorithm but substitute a word's translation at training time so that they learn embeddings for the target language word based on the source language context. (see section 4.1)\n",
    "* As a result, instead of a single weight matrix, they use a concatenation of two (see section 4 intro):\n",
    "$$\\text{Context Matrix:}\\qquad W \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "$$\\text{Embedding Matrix:}\\qquad U \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "* Since normalizing Softmax is costly, they instead optimize for a _log-pseudo likelihood_ by learning to differentiate data from negative examples selected from a noise distribution (following Mikolov 2013, see section 3) (Note that the TF tutorial models how to do this 'noise contrastive estimation')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys  \n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import collections\n",
    "import datetime as dt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###  MI - Added for zip file used in example files\n",
    "\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filepaths\n",
    "BASE = '/home/mmillervedam/Data'\n",
    "FPATH_EN = BASE + '/test/wiki_en_10K.txt' # first 10000 lines from wiki dump\n",
    "FPATH_ES = BASE + '/test/wiki_es_10K.txt' # first 10000 lines from wiki dump\n",
    "FULL_EN = BASE + '/en/full.txt'\n",
    "FULL_ES = BASE + '/es/full.txt'\n",
    "DPATH = '/home/mmillervedam/ProjectRepo/XlingualEmb/data/dicts/en.es.panlex.all.processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "# MI VOCAB_SIZE = 5000\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "### MI - Added example file download for local testing -- we can delete later\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load & Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Extract the file as a list of words.\n",
    "    NOTE: this is modified from original function in TF  \n",
    "    tutorialwhich expected a zipped input file.\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        data = tf.compat.as_str(f.read()).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "### MI Added for zip file to run locally - we can delete these later\n",
    "\n",
    "def read_data1(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name)).split()\n",
    "    f.close()\n",
    "    \n",
    "words = read_data1(filename)\n",
    "print('Data size %d' % len(words))\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer preserves order (see code in Appendix)\n",
    "en_raw = read_data(FPATH_EN)\n",
    "es_raw = read_data(FPATH_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look\n",
    "print(en_raw[:10])\n",
    "print(es_raw[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE!`__ We'll need to prepend 'en' and 'es' before training crosslingual versions.   \n",
    "__`QUESTIONS:`__ Do we deal with special characters?, punctuation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"\n",
    "    Process raw inputs into a dataset.\n",
    "    Creates vocabulary from top n words indexed by rank.\n",
    "    NOTE: this function is directly from TF tutorial\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 1737307], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n",
      "Length of Data 17005207\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "[['UNK', 1737307], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "### MI Added for test data\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words, VOCAB_SIZE)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "print('Length of Data',len(data))  #list\n",
    "print(len(count)) #list\n",
    "print(len(dictionary))#dict\n",
    "print(len(reverse_dictionary))\n",
    "print(count[:5])\n",
    "print('----------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism\n",
      "('anarchism', 303)\n"
     ]
    }
   ],
   "source": [
    "print(reverse_dictionary[5239])\n",
    "print(count[5239])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Builder indexes by count (see code in Appendix)\n",
    "en_data, en_counts, en_dict, en_index = build_dataset(en_raw, VOCAB_SIZE)\n",
    "es_data, es_counts, es_dict, es_index = build_dataset(es_raw, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del en_raw  # Uncomment to reduce memory.\n",
    "print(\"ENGLISH:\")\n",
    "print('Most common words (+UNK):\\n', en_counts[:5])\n",
    "print('Sample data:\\n',' '.join(['%s(%s)'%(en_index[i],i) for i in en_data[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del es_raw  # Uncomment to reduce memory.\n",
    "print(\"SPANISH:\")\n",
    "print('Most common words (+UNK)\\n', es_counts[:5])\n",
    "print('Sample data\\n:',' '.join(['%s(%s)'%(es_index[i],i) for i in es_data[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Batched Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram and CBOW##\n",
    "First we will implement a skip gram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### PARAMETERS ####################\n",
    "batch_size = 8 # Number of inputs to process at once.\n",
    "num_skips = 2 # How many times to reuse an input to generate a context.\n",
    "skip_window = 2 # How many words to consider left and right.\n",
    "data_index = 0  # -see note below-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### PARAMETERS MI ####################\n",
    "batch_size = 128 # Number of inputs to process at once.\n",
    "num_skips = 2 # How many times to reuse an input to generate a context.\n",
    "skip_window = 2 # How many words to consider left and right.\n",
    "data_index = 0  # -see note below-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    \"\"\"\n",
    "    Function to generate a training batch for the skip-gram model.\n",
    "    NOTE: this wass modified from original function in TF  \n",
    "    tutorial by adventuresinML tutorial - mostly just renamed.\n",
    "    \"\"\"\n",
    "    \n",
    "    global data_index\n",
    "    \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_cbow(data, batch_size, bag_window):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to generate a training batch for the CBOW model.\n",
    "    Modified generate_batch() to produce context batches.\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    global data_index\n",
    "    span = 2 * bag_window + 1 \n",
    "    batch = np.ndarray(shape=(batch_size, span - 1), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)  \n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    for i in range(batch_size):\n",
    "\n",
    "        buffer_list = list(buffer)\n",
    "        labels[i, 0] = buffer_list.pop(bag_window)\n",
    "        batch[i] = buffer_list\n",
    "\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ The TF tutorial sets data_index as global inside the generate_batch function. Double check you're getting the expected behavior below b/c we're doubling up on languages. \n",
    "> `UPDATE`: OK - it looks like this is because the 'generate batch' function is used dynamically to window over the data. I'll figure out how to handle the global indexer when I get to the tensorflow portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n",
      "============= Skip-gram ===============\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']\n",
      "    labels: ['anarchism', 'as', 'originated', 'a', 'term', 'as', 'of', 'a']\n",
      "\n",
      "================ CBOW =================\n",
      "with bag_window = 1:\n",
      "    context: [('anarchism', 'as'), ('originated', 'a'), ('as', 'term'), ('a', 'of'), ('term', 'abuse'), ('of', 'first'), ('abuse', 'used'), ('first', 'against')]\n",
      "    labels: ['originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used']\n"
     ]
    }
   ],
   "source": [
    "### MI Testing generate_batch and generate_batch_cbow\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "# Skip-gram\n",
    "print('============= Skip-gram ===============')\n",
    "num_skips = 2\n",
    "skip_window = 1\n",
    "batch, labels = generate_batch(data, batch_size = 8, num_skips = num_skips, skip_window = skip_window)\n",
    "print('with num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\n",
    "\n",
    "# CBOW\n",
    "data_index = 0\n",
    "print()\n",
    "print('================ CBOW =================')\n",
    "\n",
    "bag_window = 1\n",
    "batch1, labels1 = generate_batch_cbow(data, batch_size = 8, bag_window = bag_window)\n",
    "print('with bag_window = %d:' % (bag_window))\n",
    "print('    context:', [(reverse_dictionary[bix], reverse_dictionary[bix2])  for (bix, bix2) in batch1])\n",
    "print('    labels:', [reverse_dictionary[lix] for lix in labels1.reshape(8)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## ENGLISH BATCHES & CONTEXT #################\n",
    "# batch = list of text segmetns represented by their indices\n",
    "# contexts = corresponding skip_gram context set indices\n",
    "en_batch, en_context = generate_batch(en_data, batch_size, \n",
    "                                      num_skips, skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look\n",
    "print('RAW BATCH:', en_batch)\n",
    "print('RAW CONTEXT:', en_context.squeeze())\n",
    "print(\"Decoded:\")\n",
    "for i in range(8):\n",
    "    print(\"   \", en_batch[i], en_index[en_batch[i]],\n",
    "        '->', en_context[i, 0], en_index[en_context[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## SPANISH BATCHES & CONTEXT #################\n",
    "# batch = list of text segmetns represented by their indices\n",
    "# contexts = corresponding skip_gram context set indices\n",
    "es_batch, es_context = generate_batch(es_data, batch_size, \n",
    "                                      num_skips, skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look\n",
    "print('RAW BATCH:', es_batch)\n",
    "print('RAW CONTEXT:', es_context.squeeze())\n",
    "print(\"Decoded:\")\n",
    "for i in range(8):\n",
    "    print(\"   \", es_batch[i], es_index[en_batch[i]],\n",
    "        '->', es_context[i, 0], es_index[es_context[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ To implment Duong et Al's work we'd perform the word substitution at this stage, replacing the words in the batch with the index of their translation... In fact we'd probably do so using a dictionary of indices for the vocab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TensorFlow Model - Using CBOW (testing 3 models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ Set up the model graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# recall that we set the vocabulary size at the top of the NB\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### PARAMETERS ####################\n",
    "batch_size = 32 # Number of inputs to process at once.\n",
    "embedding_size = 300 # Hidden layer representation size\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a context.\n",
    "\n",
    "# Validation variables\n",
    "valid_size = 8     # Random set of words to evaluate similarity on.\n",
    "valid_window = 50  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the TF graph\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### DATA PLACEHOLDERS ####################\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Shape of the place holders have been modified for CBOW implementation\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size, bag_window * 2])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    \n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    train_one_hot = tf.one_hot(train_labels, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### INPUT(EMBEDDING)LAYER #################\n",
    "with graph.as_default():\n",
    "    embeddings = tf.Variable(tf.random_uniform([VOCAB_SIZE, \n",
    "                                                embedding_size],\n",
    "                                               -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    # this var is used for CBOW only\n",
    "    reduced_embed = tf.div(tf.reduce_sum(embed, 1), skip_window*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################## HIDDEN LAYER ######################\n",
    "with graph.as_default():\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([VOCAB_SIZE, embedding_size],\n",
    "                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    biases = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "    \n",
    "    ## Here reduced_embed is used instead of embed for CBOW\n",
    "    hidden_out = tf.matmul(reduced_embed, tf.transpose(weights)) + biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first implement CBOW with the model used in the TF tutorial - Softmax Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Softmax Cross Entropy ##################\n",
    "with graph.as_default():\n",
    "    \n",
    "    train_one_hot = tf.one_hot(train_labels, VOCAB_SIZE)\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
    "                                                labels=train_one_hot))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the full softmax model will be slow, we will test the two sampled loss functions provided in TensorFlow.  The first will be NCE Loss and the second will be Sampled Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  NCE Loss  ########################\n",
    "with graph.as_default():\n",
    "    \n",
    "    nce_loss = tf.reduce_mean(tf.nn.nce_loss(weights = weights, \n",
    "                                         biases = biases, \n",
    "                                         inputs = tf.reduce_sum(embed, 1), \n",
    "                                         labels = train_labels, \n",
    "                                         num_sampled= num_sampled, \n",
    "                                         num_classes= VOCAB_SIZE,\n",
    "                                         partition_strategy=\"div\"))\n",
    "\n",
    "    nce_optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ If we're going to set up experiments/comparisons between different embedding training methods (eg. Duongs word2vec modification vs the post training aligned word vectors referenced in the Babylon Repo)... we'll want to fix the embedding size across the multiple models. Maybe even fix the initialization for the weights?-- no in this case the weights are irrelevant across models b/c they'll be optimizing different things. Presumably part of what we're interested in is comparisons of speed to train in concert w/ efficacy on the translation task and random initialization always begs the question of 'did we just get lucky'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Sampled Softmax ######################\n",
    "with graph.as_default():\n",
    "    sampled_loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights = weights,\n",
    "                                                     biases = biases, \n",
    "                                                     inputs = tf.reduce_sum(embed, 1), \n",
    "                                                     labels = train_labels, \n",
    "                                                     num_sampled= num_sampled, \n",
    "                                                     num_classes= VOCAB_SIZE))\n",
    "    sampled_optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(sampled_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Set up validation set - a randomly chosen set of words to use to track our progress as we train. By construction we'll pick words from the 100 most frequent in the vocabulary then use cosine similarity to find the nearest neighbors in the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### VALIDATION EXAMPLES #################\n",
    "valid_size = 8    # Random set of words to evaluate similarity on.\n",
    "valid_window = 50  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "with graph.as_default():\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### SIMILARITY CALCULATION ################\n",
    "with graph.as_default():\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable initializer\n",
    "with graph.as_default():\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3:__ Run the model & track progress by examining the matches for words in our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_index = 0 # used to track batches\n",
    "\n",
    "# For testing various models with CBOW\n",
    "\n",
    "NCE = 0\n",
    "SMAX_X_ENTROPY = 1\n",
    "SAMPLED_SMAX = 2\n",
    "\n",
    "\n",
    "def run_cbow(graph, num_steps, model = SAMPLED_SMAX ):\n",
    "    \"\"\"Runner code for word2vec TF model w/ full softmax\"\"\"\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            #def generate_batch_cbow(data, batch_size, bag_window):\n",
    "            batch_inputs, batch_labels = generate_batch_cbow(data,\n",
    "                                                         batch_size, \n",
    "                                                         bag_window)\n",
    "            feed_dict = {train_inputs: batch_inputs, \n",
    "                         train_labels: batch_labels}\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op \n",
    "            \n",
    "            # We tested various models\n",
    "            # 1) NCE optimizer\n",
    "            # 2) Softmax Cross Entropy \n",
    "            # 3) Sampled Softmax\n",
    "                       \n",
    "            if model == NCE:\n",
    "                _, loss_val = session.run([nce_optimizer, nce_loss], \n",
    "                                          feed_dict=feed_dict)\n",
    "            elif model == SMAX_X_ENTROPY:\n",
    "                _, loss_val = session.run([optimizer, cross_entropy], \n",
    "                                          feed_dict=feed_dict)\n",
    "            else:\n",
    "                _, loss_val = session.run([sampled_optimizer, sampled_loss], \n",
    "                                          feed_dict=feed_dict)\n",
    "                \n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                    top_k = 8  # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = reverse_dictionary[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                    print(log_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runner Call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing\n",
    "First, the Softmax Cross Entropy Optimizer is tested.  This model is the baseline from the TF tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  9.27666282654\n",
      "Nearest to zero: chemistry, theft, linguistics, missiles, abstraction, millions, kinds, share,\n",
      "Nearest to is: maltese, wasn, garbage, ferdinand, ukraine, marks, reconstruction, blues,\n",
      "Nearest to seven: vol, capacitor, textiles, sources, antony, corps, era, cp,\n",
      "Nearest to nine: epic, fellow, importantly, snyder, arizona, special, spending, falling,\n",
      "Nearest to not: civic, beverages, teacher, mitchell, increase, featured, roma, wwii,\n",
      "Nearest to it: clown, playoffs, proceedings, generalized, contested, extreme, fl, anton,\n",
      "Nearest to have: itself, simulation, enemy, label, catholicism, arabia, devoted, framework,\n",
      "Nearest to for: escaped, player, bone, wax, reduces, european, advantages, going,\n",
      "Average loss at step  2000 :  6.17026983762\n",
      "Average loss at step  4000 :  5.80681890154\n",
      "Average loss at step  6000 :  5.63998748465\n",
      "Average loss at step  8000 :  5.53292568243\n",
      "Average loss at step  10000 :  5.52537331802\n",
      "Nearest to zero: nine, five, eight, six, three, seven, four, one,\n",
      "Nearest to is: was, be, are, maltese, ferdinand, garbage, marks, in,\n",
      "Nearest to seven: zero, nine, six, five, antony, vol, sources, textiles,\n",
      "Nearest to nine: zero, eight, seven, six, two, one, three, five,\n",
      "Nearest to not: mitchell, beverages, increase, civic, teacher, suitable, wwii, marvin,\n",
      "Nearest to it: he, clown, contested, shops, proceedings, sex, fl, playoffs,\n",
      "Nearest to have: simulation, itself, label, something, has, manganese, boy, des,\n",
      "Nearest to for: escaped, of, reduces, european, player, wax, bone, tallest,\n",
      "Softmax Cross Entropy method took 583.542615 seconds to run 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "start_time = dt.datetime.now()\n",
    "run_cbow(graph, num_steps=num_steps, model = SMAX_X_ENTROPY)\n",
    "end_time = dt.datetime.now()\n",
    "print(\"Softmax Cross Entropy method took {} seconds to run 10000 iterations\".format((end_time - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  234.274353027\n",
      "Nearest to zero: silver, poetry, stationary, pierce, regulation, insulin, ak, besides,\n",
      "Nearest to is: hung, contexts, flows, astronomical, acquire, signing, krak, manufacture,\n",
      "Nearest to seven: friendship, sit, neutron, jew, contributors, costume, billion, bits,\n",
      "Nearest to nine: attained, bet, applied, southeast, saul, serbia, elite, morning,\n",
      "Nearest to not: read, casting, faculty, relational, muhammad, shut, dean, exposition,\n",
      "Nearest to it: lanka, anne, arrest, impressed, weber, scots, tunnel, receivers,\n",
      "Nearest to have: kissinger, scores, yet, send, performers, precision, texts, six,\n",
      "Nearest to for: catalan, klan, existed, underworld, myth, arc, arrangements, potatoes,\n",
      "Average loss at step  2000 :  43.2367338331\n",
      "Average loss at step  4000 :  16.4807954127\n",
      "Average loss at step  6000 :  869893967876.0\n",
      "Average loss at step  8000 :  6.3441603282e+17\n",
      "Average loss at step  10000 :  1.00599554835e+19\n",
      "Nearest to zero: three, hermes, him, electrons, retrieved, mad, p, headed,\n",
      "Nearest to is: few, sciences, hitler, as, within, abraham, formal, closed,\n",
      "Nearest to seven: embraced, germany, correspondence, celtic, rightarrow, winter, resonance, universidad,\n",
      "Nearest to nine: considers, e, ties, can, areas, this, philosophy, savage,\n",
      "Nearest to not: card, telecommunications, pressure, say, features, rather, occurs, algebraic,\n",
      "Nearest to it: loose, roles, industries, magic, regional, eisenhower, are, must,\n",
      "Nearest to have: has, shaped, does, look, coded, libertarian, years, program,\n",
      "Nearest to for: on, boys, los, of, male, sea, world, from,\n",
      "NCE method took 15.81516 seconds to run 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "start_time = dt.datetime.now()\n",
    "run_cbow(graph, num_steps=num_steps, model = NCE)\n",
    "end_time = dt.datetime.now()\n",
    "print(\"NCE method took {} seconds to run 10000 iterations\".format((end_time - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Looks like something is wrong here:__\n",
    "Occassionally, the average loss goes up!  Need to investigate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  6.75405597687\n",
      "Nearest to zero: maximum, theatre, woods, curious, favoured, bosnia, restriction, crushed,\n",
      "Nearest to is: producer, processor, precursor, productivity, disc, asian, hull, na,\n",
      "Nearest to seven: dublin, marvin, ken, disputes, perfect, office, comparing, helsinki,\n",
      "Nearest to nine: up, monument, cyberpunk, challenges, trivial, count, acquisition, required,\n",
      "Nearest to not: summer, or, algorithm, orthography, acupuncture, wonder, standard, testimony,\n",
      "Nearest to it: bruce, mars, parsons, contribution, assume, regarding, areas, unstable,\n",
      "Nearest to have: dense, arrested, autism, duck, singers, exposition, horace, athletic,\n",
      "Nearest to for: joining, dominion, regime, malay, farms, hurt, monster, given,\n",
      "Average loss at step  2000 :  4.40612527955\n",
      "Average loss at step  4000 :  3.97505038351\n",
      "Average loss at step  6000 :  3.854579023\n",
      "Average loss at step  8000 :  3.50023281932\n",
      "Average loss at step  10000 :  3.49135156456\n",
      "Nearest to zero: four, seven, nine, five, six, three, two, eight,\n",
      "Nearest to is: was, are, has, were, in, bars, scored, does,\n",
      "Nearest to seven: nine, five, six, four, zero, three, eight, one,\n",
      "Nearest to nine: seven, five, four, eight, zero, six, three, one,\n",
      "Nearest to not: encyclop, photographer, roma, it, troll, literature, incompatible, acupuncture,\n",
      "Nearest to it: he, this, bruce, there, noir, contribution, firearm, ants,\n",
      "Nearest to have: be, are, arrested, dense, autism, was, singers, nash,\n",
      "Nearest to for: in, of, on, from, to, with, joining, diocese,\n",
      "Sampled Softmax method took 16.898114 seconds to run 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "start_time = dt.datetime.now()\n",
    "run_cbow(graph, num_steps=num_steps, model = SAMPLED_SMAX)\n",
    "end_time = dt.datetime.now()\n",
    "print(\"Sampled Softmax method took {} seconds to run 10000 iterations\".format((end_time - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results Discussion__:\n",
    "The sampled softmax TF loss function produces the best results with the high performance sampling functions.  We will proceed with the implementation with this optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10001\n",
    "nce_start_time = dt.datetime.now()\n",
    "run_cbow(graph, num_steps=num_steps)\n",
    "nce_end_time = dt.datetime.now()\n",
    "print(\"NCE method took {} seconds to run 10000 iterations\".format((nce_end_time-nce_start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: output from ^^ saved to:\n",
    "path = 'wtv_output/en_smalldata_10Kiter_fullsfmx.txt'\n",
    "!tail -n 1 {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at loss\n",
    "!grep 'Average' {path} | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at NN for 'the'\n",
    "!grep 'Nearest to them:' {path} |tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ The data ^^ are undoubtedly too small... 'Alabama' shouldn't appear in the top 100 words. However I'll wait to look at larger data with the sampling method which is much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Model using CBOW w/ Sampled Softmax (faster) #\n",
    "\n",
    "We'll write it as a class this time for ease of calling later.  TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def with_self_graph(function):\n",
    "    \"\"\"Decorator-foo borrowed from w266 a4.\"\"\"\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        with self.graph.as_default():\n",
    "            return function(self, *args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "#from __future__ import unicode_literals\n",
    "\n",
    "class Word2Vec(object):\n",
    "    \"\"\"Single Layer Neural Net to Learn Word Embeddings.\"\"\"\n",
    "    # This code was adapted from:\n",
    "    # SOURCE: https://github.com/tensorflow/tensorflow\n",
    "    #         /blob/r1.2/tensorflow/examples/tutorials\n",
    "    #         /word2vec/word2vec_basic.py\n",
    "    \n",
    "    def __init__(self, graph=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize TensorFlow Neural Net Model.\n",
    "        Args:\n",
    "          V: vocabulary size\n",
    "          H: embedding size\n",
    "          \n",
    "        Kwargs:\n",
    "          softmax_ns = 64  (number of negative samples)\n",
    "          alpha = 1.0  (learning rate)\n",
    "          examples = np.array of 5 top 100 words for validation\n",
    "        \"\"\"\n",
    "        # Set TensorFlow graph. All TF code will work on this graph.\n",
    "        self.graph = graph or tf.Graph()\n",
    "        self.SetParams(*args, **kwargs)\n",
    "        \n",
    "    @with_self_graph # TODO : remove this unless we plan to init as tf.const\n",
    "    def SetParams(self, V, H, softmax_ns=64, learning_rate=1.0):\n",
    "        # Model structure.\n",
    "        self.V = V\n",
    "        self.H = H\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        self.softmax_ns = softmax_ns\n",
    "        self.alpha = 1.0\n",
    "        \n",
    "        # Words for validation\n",
    "        self.examples = np.random.choice(100, 10, replace=False)\n",
    "        \n",
    "        # Results\n",
    "        self.epochs_trained = 0\n",
    "        self.final_embeddings = None\n",
    "            \n",
    "    @with_self_graph\n",
    "    def BuildCoreGraph(self):\n",
    "        \n",
    "        batch_size = 128 # TODO : I've hard coded this for now b/c I want to get\n",
    "                         # the rest of the code running, but eventually this should\n",
    "                         # be inferred dynamically from the input shape as in a4.\n",
    "        \n",
    "        # Data Placeholders\n",
    "        self.inputs_ = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        self.context_ = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        \n",
    "        # Embedding Layer\n",
    "        with tf.variable_scope(\"Embedding_Layer\"):\n",
    "            self.embeddings_ = tf.Variable(tf.random_uniform([self.V, self.H], \n",
    "                                                             -1.0, 1.0), name = 'Embeddings')\n",
    "            self.embed_ = tf.nn.embedding_lookup(self.embeddings_, self.inputs_)\n",
    "            # Normalized Embeddings facillitate cosine similarity calculation\n",
    "            # .... but don't train on these! they're just for evaluation!\n",
    "            self.norm_ = tf.sqrt(tf.reduce_sum(tf.square(self.embeddings_), 1, keep_dims=True))\n",
    "            self.normalized_embeddings_ = self.embeddings_ / self.norm_\n",
    "            \n",
    "        # Hidden Layer\n",
    "        with tf.variable_scope(\"Hidden_Layer\"):\n",
    "            self.W_ = tf.Variable(tf.truncated_normal([self.V, self.H],\n",
    "                                  stddev=1.0 / math.sqrt(self.H)), name = 'W')\n",
    "            self.b_ = tf.Variable(tf.zeros([self.V,], dtype=tf.float32), name = 'b')\n",
    "            self.logits_ = tf.matmul(self.embed_, tf.transpose(self.W_)) + self.b_\n",
    "           \n",
    "    @with_self_graph\n",
    "    def BuildTrainingGraph(self):\n",
    "        with tf.variable_scope(\"Training\"):\n",
    "            nce_args = dict(weights=self.W_, \n",
    "                            biases=self.b_, \n",
    "                            labels=self.context_, \n",
    "                            inputs=self.embed_, \n",
    "                            num_sampled=self.softmax_ns, \n",
    "                            num_classes=self.V)\n",
    "            self.nce_loss_ = tf.reduce_mean(tf.nn.nce_loss(**nce_args))\n",
    "            self.optimizer_ = tf.train.GradientDescentOptimizer(self.alpha)\n",
    "            self.train_step_ = self.optimizer_.minimize(self.nce_loss_)\n",
    "        \n",
    "    @with_self_graph\n",
    "    def BuildValidationGraph(self):\n",
    "        self.test_ = tf.constant(self.examples, dtype=tf.int32)\n",
    "        self.test_embed_ = tf.nn.embedding_lookup(self.normalized_embeddings_, \n",
    "                                                  self.test_)\n",
    "        self.similarity = tf.matmul(self.test_embed_, \n",
    "                                    self.normalized_embeddings_, \n",
    "                                    transpose_b=True)\n",
    "        \n",
    "    def learn_embeddings(self, num_steps, batch_fxn, data, index, verbose = True):\n",
    "        \"\"\"\n",
    "        Runs a specified number of training steps.\n",
    "        NOTE: right now the batch fxn is hard coded with inputs: \n",
    "                  (data,batch_size=128,num_skips=2,skip_window=2)\n",
    "              It should output two arrays representing the input & \n",
    "              context indices for a single batch. \n",
    "              TODO: replace this with something less clunky!\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as session:\n",
    "            \n",
    "            # initialize all variables\n",
    "            init = tf.global_variables_initializer()\n",
    "            init.run()\n",
    "            print('... Model Initialized')\n",
    "            if verbose:\n",
    "                for var in tf.trainable_variables():\n",
    "                    print(\"\\t\", var)\n",
    "        \n",
    "            # iterate through specificied number of training steps\n",
    "            average_loss = 0\n",
    "            for step in range(num_steps):\n",
    "                # Get the next batch of inputs & their skipgram context\n",
    "                batch_inputs, batch_context = batch_fxn(data, 128, 2, 2)\n",
    "\n",
    "                # Run the train op\n",
    "                feed_dict = {self.inputs_: batch_inputs, self.context_: batch_context}\n",
    "                _, loss_val = session.run([self.train_step_, self.nce_loss_], \n",
    "                                          feed_dict=feed_dict)\n",
    "                \n",
    "                # Logging Progress\n",
    "                average_loss += loss_val\n",
    "                loss_logging_interval = num_steps // 10\n",
    "                sim_logging_interval = num_steps // 5\n",
    "                if not verbose:\n",
    "                    continue\n",
    "                if step % loss_logging_interval == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss /= loss_logging_interval\n",
    "                    # The average loss is an estimate of the loss over the last 1000 batches.\n",
    "                    print('Average loss at step ', step, ': ', average_loss)\n",
    "                    average_loss = 0  \n",
    "                if step % sim_logging_interval == 0:\n",
    "                    sim = self.similarity.eval()\n",
    "                    for i in xrange(len(self.examples)):\n",
    "                        word = index[self.examples[i]]\n",
    "                        top_k = 8  # number of nearest neighbors\n",
    "                        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                        log_str = '   Nearest to %s:' % word\n",
    "                        for k in xrange(top_k):\n",
    "                            nbr = index[nearest[k]]\n",
    "                            log_str = '%s %s,' % (log_str, nbr)\n",
    "                        print(log_str)\n",
    "            # results\n",
    "            self.epochs_trained = num_steps\n",
    "            self.final_embeddings = self.normalized_embeddings_.eval()\n",
    "        return self.final_embeddings\n",
    "    \n",
    "    def plot_embeddings_in_2D(self, num, index):\n",
    "        \"\"\" \n",
    "        Plot 2D representation of embeddings.\n",
    "        Args: \n",
    "            num = int (number of examples to plot)\n",
    "            index = reverse dictionary of word indices\n",
    "            filename = path to save plot\n",
    "        \"\"\"\n",
    "        if self.final_embeddings is None:\n",
    "            print(\"You must train the embeddings before plotting.\")\n",
    "        else:\n",
    "            tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "            low_dim_embs = tsne.fit_transform(self.final_embeddings[:num, :])\n",
    "            labels = [index[i] for i in xrange(num)]\n",
    "            plt.figure(figsize=(18, 18))  # in inches\n",
    "            for i, label in enumerate(labels):\n",
    "                x, y = low_dim_embs[i, :]\n",
    "                plt.scatter(x, y)\n",
    "                plt.annotate(str(label), xy=(x, y), xytext=(5, 2), \n",
    "                             textcoords='offset points', ha='right', va='bottom')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 0:__ Data prep (_we did this above, just running a few checks here_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using the shortened version of the English Wikipedia File\n",
    "print('Corpus Size: %s Words' % (len(en_data)))\n",
    "print('Vocabulary Size: %s Words' % (len(en_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional Parameters for batch function\n",
    "BATCH_SIZE = 128 # Number of inputs to process at once.\n",
    "EMBEDDING_SIZE = 128 # Hidden layer representation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: the following are hard coded into the class above, just here for reference \n",
    "# TODO make a better batch iterator & a data handler so we don't have to do this!\n",
    "skip_window = 2 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a context.\n",
    "data_index = 0 # Used to track batches for now, TODO fix batch iterator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ Create Model & Initialize TF Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(V=VOCAB_SIZE, H=EMBEDDING_SIZE)\n",
    "model.BuildCoreGraph()\n",
    "model.BuildTrainingGraph()\n",
    "model.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __`Question for Mona & Roseanna:`__ When does it makes sense to keep these graph building methods separate and when should they all be part of the same class method? In this case we're never going to run 'inference' except in the context of the test/validation exercise... because we don't really care about the ultimate prediction of context words we really care about the embeddings. I've left these as 3 methods for now (following the lead of A4) but I wonder if we could combine two or all of them when we create our Xlingual version of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Train the model.   \n",
    "__`Note:`__ The training function is logging the sampled softmax loss... (NCE)... not sure if thats terribly instructive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSTEPS = 50000\n",
    "start_time = dt.datetime.now()\n",
    "embeddings = model.learn_embeddings(NSTEPS, generate_batch, en_data, en_index)\n",
    "end_time = dt.datetime.now()\n",
    "total = (end_time - start_time).total_seconds()\n",
    "print(\"NCE method took {} seconds to run {} iterations\".format(total, NSTEPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3:__ Plot the resulting embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_embeddings_in_2D(300, en_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__`NOTE:`__ This plotting code clearly needs some work... but cool idea. I think it makes sense to rewrite the method so that it accepts a specific set of input words or indices not just a number of top words to plot. \n",
    "\n",
    "__`Also NOTE:`__ matplotlib is going to throw a fit when it encounters non ascii characters. [This SO post](https://stackoverflow.com/questions/21129020/how-to-fix-unicodedecodeerror-ascii-codec-cant-decode-byte) explains that this is a Python 2 problem and suggests `from __future__ import unicode_literals` might help... but it doesn't seem to. Another SO post suggested that `sys.setdefaultencoding('utf-8')` should fix it, but that causes [this print problem](https://stackoverflow.com/questions/25494182/print-not-showing-in-ipython-notebook-python) (print output of Jupyter cells is getting redirected to the terminal). The suggested solution (below) seems to work. I may switch to using plotly in which case no matter. Otherwise we should dig into this a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set decoding for matplotlib to handle accents\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('please work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec on Full Spanish Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 0:__ Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "BATCH_SIZE = 128 # Number of inputs to process at once.\n",
    "EMBEDDING_SIZE = 128 # Hidden layer representation size\n",
    "data_index = 0 # Used to track batches for now, TODO fix batch iterator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in raw file\n",
    "es_raw = read_data(FULL_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse data into dictionary\n",
    "es_data, es_counts, es_es_dict, es_index = build_dataset(es_raw, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look\n",
    "del es_raw  # reduce memory.\n",
    "print('Corpus Size: %s Words' % (len(es_data)))\n",
    "print('Vocabulary Size: %s Words' % (len(es_counts)))\n",
    "print('Most common words (+UNK)\\n', es_counts[:5])\n",
    "print('Sample data\\n:',' '.join(['%s(%s)'%(es_index[i],i) for i in es_data[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(V=VOCAB_SIZE, H=EMBEDDING_SIZE)\n",
    "model.BuildCoreGraph()\n",
    "model.BuildTrainingGraph()\n",
    "model.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Train Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSTEPS = 100001\n",
    "start_time = dt.datetime.now()\n",
    "embeddings = model.learn_embeddings(NSTEPS, generate_batch, es_data, es_index)\n",
    "end_time = dt.datetime.now()\n",
    "total = (end_time - start_time).total_seconds()\n",
    "print(\"NCE method took {} seconds to run {} iterations\".format(total, NSTEPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3:__ Plot a few of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_embeddings_in_2D(500, es_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4:__ Save embeddings & index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm dim\n",
    "model.final_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary\n",
    "len(es_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make them a tuple to pickle\n",
    "embeddings_tuple = (es_index, model.final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to file\n",
    "import pickle\n",
    "filename = './wtv_output/es_w2v_100K_embed.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(embeddings_tuple, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confirm reload\n",
    "filename = './wtv_output/es_w2v_100K_embed.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    test_tuple = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuple[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
