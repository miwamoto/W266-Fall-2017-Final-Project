{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE Substitution\n",
    "`w266 Final Project: Crosslingual Word Embeddings`\n",
    "\n",
    "Instead of traning on randomly substituted words, here we'll choose the highest ranked translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set base paths depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE = '/home/mmillervedam/Data'\n",
    "PROJ = '/home/mmillervedam/ProjectRepo'\n",
    "#PROJ = '/Users/mona/OneDrive/repos/final_proj/W266-Fall-2017-Final-Project'\n",
    "\n",
    "# raw data\n",
    "FULL_EN_ES = \"/home/miwamoto/en_es_shuf.txt\"\n",
    "FULL_EN_IT = \"/home/miwamoto/en_it_shuf.txt\"\n",
    "\n",
    "# vocabularies\n",
    "VOCAB_EN_ES = BASE + '/vocab/en_es_index.pkl'\n",
    "VOCAB_EN_IT = BASE + '/vocab/en_it_index.pkl'\n",
    "\n",
    "# panlex dicts\n",
    "PANLEX_EN_ES = BASE + '/panlex/en_es_dict.pkl'\n",
    "PANLEX_EN_IT = BASE + '/panlex/en_it_dict.pkl'\n",
    "\n",
    "# directory to save pickled embeddings\n",
    "SAVE_TO = BASE + '/embeddings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English - Spanish\n",
    "### Load Data, Dict & Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import Corpus, BilingualVocabulary, batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "en_es_data = Corpus(FULL_EN_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load panlex dictionary\n",
    "with open(PANLEX_EN_ES,'rb') as f:\n",
    "    en_es_translations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "en_es_vocab = BilingualVocabulary([], languages = ('en','es'))\n",
    "with open(VOCAB_EN_ES,'rb') as f:\n",
    "    en_es_vocab.load_from_index(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded 702982 panlex translations\n",
      "... loaded 200003 word ('en', 'es')vocabulary\n"
     ]
    }
   ],
   "source": [
    "# confirmations\n",
    "print('... loaded %s panlex translations'%(len(en_es_translations)))\n",
    "print('... loaded %s word %svocabulary'%(en_es_vocab.size,en_es_vocab.language))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 270, 100160, 100249]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_es_vocab.to_ids(['en_the','en_last', 'es_mundo', 'es_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_the ['es_el', 'es_el_la_los_las', 'es_entonces', 'es_la', 'es_las']\n",
      "en_last ['es_aguantar', 'es_anterior', 'es_atr\\xc3\\xa1s', 'es_conclusi\\xc3\\xb3n', 'es_concluyentemente']\n",
      "es_mundo ['en_age', 'en_cosmos', 'en_creation', 'en_earth_globe', 'en_earth']\n",
      "es_real ['en_actual', 'en_certain', 'en_clear', 'en_concise', 'en_de_facto']\n"
     ]
    }
   ],
   "source": [
    "for w in ['en_the','en_last', 'es_mundo', 'es_real']:\n",
    "    print(w,list(en_es_translations[w][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Batch Generator Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "WINDOW_SIZE = 4\n",
    "MAX_EPOCHS = 30 # fail safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: recreate sentence & batch generators so they're insynch\n",
    "token_gen = en_es_data.gen_tokens()\n",
    "batch_gen = batch_generator(en_es_data, en_es_vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: have a look\n",
    "print(\"ORIGINAL DATA:\")\n",
    "print([next(token_gen) for _ in range(BATCH_SIZE)])\n",
    "\n",
    "for context, label in batch_gen:\n",
    "    print(\"CONTEXT IDS:\\n\", context[:5])\n",
    "    print(\"CONTEXT:\\n\", [en_es_vocab.to_words(c) for c in context[:5]])\n",
    "    print(\"LABEL IDS:\\n\", label[:5])\n",
    "    print(\"LABELS:\\n\", en_es_vocab.to_words(label[:5]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_random\n",
    "\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# create model\n",
    "model = BiW2V_random(bilingual_dict = en_es_translations,\n",
    "                     vocab = en_es_vocab, \n",
    "                     H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model.BuildCoreGraph()\n",
    "model.BuildTrainingGraph()\n",
    "model.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8573299  222459019 1902018560 /home/miwamoto/en_es_shuf.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc {FULL_EN_ES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4634562"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "222459019 / 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nBATCHES = 600000 # less than 1 epoch\n",
    "BATCH_SIZE = 48\n",
    "WINDOW_SIZE = 4\n",
    "MAX_EPOCHS = 5 # fail safe\n",
    "DATA_GENERATOR = batch_generator(en_es_data, en_es_vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)\n",
    "TEST_WORDS = en_es_vocab.to_ids(['en_the','en_last', 'es_mundo', 'es_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(200003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(200003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(200003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.000104637964567\n",
      "   [en_the] closest:  es_eliminar, en_47north, en_austerity, en_then-wife, es_valido, en_binghamton, en_1986-87, es_canfranc,\n",
      "   [en_last] closest:  en_nihilism, en_piplup, es_esconderse, es_accidentadas, en_readout, en_gmbh, en_redevelop, es_barú,\n",
      "   [es_mundo] closest:  es_poets, en_criterium, en_graduate, en_kim, es_reset, es_juquila, en_-29, en_9x,\n",
      "   [es_real] closest:  es_negociante, en_shades, es_fundaban, en_sensational, en_misaki, es_preclasificado, en_•, es_coles,\n",
      "... STEP 60000 : Average Loss : 2.20431351974\n",
      "... STEP 120000 : Average Loss : 1.85183114125\n",
      "   [en_the] closest:  en_a, en_., en_'s, en_an, en_his, en_to, en_that, en_for,\n",
      "   [en_last] closest:  en_nihilism, es_esconderse, es_accidentadas, es_gobernación, en_redevelop, es_ramalho, en_piplup, en_stepfather,\n",
      "   [es_mundo] closest:  es_poets, en_criterium, en_kim, en_graduate, es_reset, en_-29, es_juquila, en_9x,\n",
      "   [es_real] closest:  es_negociante, es_fundaban, en_shades, en_sensational, en_misaki, es_preclasificado, en_tander, es_coles,\n",
      "... STEP 180000 : Average Loss : 1.788826969\n",
      "... STEP 240000 : Average Loss : 1.74040148907\n",
      "   [en_the] closest:  en_a, en_an, en_., en_its, en_'s, en_this, en_their, en_his,\n",
      "   [en_last] closest:  es_gobernación, en_until, es_esconderse, en_nihilism, es_accidentadas, es_euclides, en_redevelop, es_ramalho,\n",
      "   [es_mundo] closest:  es_poets, en_criterium, en_graduate, en_kim, es_reset, en_-29, es_juquila, en_9x,\n",
      "   [es_real] closest:  es_negociante, es_fundaban, en_shades, es_-, en_sensational, es_preclasificado, en_misaki, en_tander,\n",
      "... STEP 300000 : Average Loss : 1.6998725493\n",
      "... STEP 360000 : Average Loss : 1.6552147465\n",
      "   [en_the] closest:  en_a, en_its, en_an, en_this, en_., en_their, en_'s, en_it,\n",
      "   [en_last] closest:  en_until, en_during, es_gobernación, es_euclides, en_after, es_esconderse, en_world, en_nihilism,\n",
      "   [es_mundo] closest:  es_poets, en_criterium, en_graduate, en_kim, es_reset, es_la, en_-29, es_juquila,\n",
      "   [es_real] closest:  es_-, es_negociante, es_del, es_fundaban, en_shades, en_sensational, es_preclasificado, en_beaven,\n",
      "... STEP 420000 : Average Loss : 1.64478840086\n",
      "... STEP 480000 : Average Loss : 1.62253890352\n",
      "   [en_the] closest:  en_a, en_its, en_this, en_an, en_their, en_., en_'s, en_his,\n",
      "   [en_last] closest:  en_during, en_until, en_next, en_after, en_first, en_before, es_gobernación, es_euclides,\n",
      "   [es_mundo] closest:  es_poets, en_criterium, es_la, en_graduate, en_kim, es_reset, en_-29, es_juquila,\n",
      "   [es_real] closest:  es_-, es_del, es_negociante, es_fundaban, es_de, en_shades, es_preclasificado, en_bathing,\n",
      "... STEP 540000 : Average Loss : 1.59895060406\n",
      "... STEP 600000 : Average Loss : 1.59025856495\n",
      "   [en_the] closest:  en_a, en_its, en_this, en_an, en_their, en_., en_'s, en_his,\n",
      "   [en_last] closest:  en_next, en_during, en_after, en_first, en_until, en_before, en_later, en_world,\n",
      "   [es_mundo] closest:  es_poets, en_criterium, es_la, en_graduate, en_kim, es_reset, en_-29, es_juquila,\n",
      "   [es_real] closest:  es_-, es_del, es_se, es_negociante, es_fundaban, es_de, en_bathing, es_preclasificado,\n",
      "... Training Complete\n",
      "... 600000 batches trained in 1611.18958998 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "model.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = 0.5)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View & Save Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.84318463e-04,   1.55181682e-04,   7.64811266e-05, ...,\n",
       "         -5.75772647e-05,  -6.62243519e-06,   1.13493570e-05],\n",
       "       [ -7.16788854e-05,   2.40120396e-04,  -1.49121872e-04, ...,\n",
       "         -3.24801571e-04,  -3.24414665e-04,  -1.84669960e-04],\n",
       "       [  1.31410881e-04,  -1.30790548e-04,   1.72700151e-04, ...,\n",
       "          6.08587834e-05,   6.42145678e-05,   1.10936184e-04],\n",
       "       ..., \n",
       "       [  1.36078175e-04,   1.21385499e-04,   2.41268077e-04, ...,\n",
       "          2.29004600e-05,  -5.72097269e-06,   1.64323093e-04],\n",
       "       [ -9.66184962e-05,  -1.97520858e-04,   2.46643118e-04, ...,\n",
       "         -2.46647396e-04,  -3.95404604e-05,  -2.58605985e-04],\n",
       "       [ -5.71376295e-05,  -1.30071737e-06,  -2.53239996e-04, ...,\n",
       "          1.78198912e-04,  -8.43432063e-05,  -2.27683529e-04]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the embeddings\n",
    "model.context_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.08320143e-03,   1.13701622e-03,   5.60377259e-04, ...,\n",
       "         -4.21868666e-04,  -4.85225864e-05,   8.31567449e-05],\n",
       "       [ -5.25191252e-04,   1.75936229e-03,  -1.09261612e-03, ...,\n",
       "         -2.37982115e-03,  -2.37698643e-03,  -1.35307701e-03],\n",
       "       [  9.62847553e-04,  -9.58302408e-04,   1.26537413e-03, ...,\n",
       "          4.45912330e-04,   4.70500207e-04,   8.12829472e-04],\n",
       "       ..., \n",
       "       [  9.97044845e-04,   8.89391638e-04,   1.76777132e-03, ...,\n",
       "          1.67791688e-04,  -4.19175703e-05,   1.20399543e-03],\n",
       "       [ -7.07923784e-04,  -1.44723547e-03,   1.80715427e-03, ...,\n",
       "         -1.80718559e-03,  -2.89712974e-04,  -1.89480628e-03],\n",
       "       [ -4.18647425e-04,  -9.53035669e-06,  -1.85548968e-03, ...,\n",
       "          1.30566361e-03,  -6.17982703e-04,  -1.66823738e-03]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the embeddings\n",
    "model.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving final embeddings in case we want to do more stuff later\n",
    "filename = SAVE_TO + '/en_es_rand_600K_cw4_V_dec18.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "filename = SAVE_TO + '/en_es_rand_600K_cw4_U_dec18.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.84318463e-04,   1.55181682e-04,   7.64811266e-05, ...,\n",
       "         -5.75772647e-05,  -6.62243519e-06,   1.13493570e-05],\n",
       "       [ -7.16788854e-05,   2.40120396e-04,  -1.49121872e-04, ...,\n",
       "         -3.24801571e-04,  -3.24414665e-04,  -1.84669960e-04],\n",
       "       [  1.31410881e-04,  -1.30790548e-04,   1.72700151e-04, ...,\n",
       "          6.08587834e-05,   6.42145678e-05,   1.10936184e-04],\n",
       "       ..., \n",
       "       [  1.36078175e-04,   1.21385499e-04,   2.41268077e-04, ...,\n",
       "          2.29004600e-05,  -5.72097269e-06,   1.64323093e-04],\n",
       "       [ -9.66184962e-05,  -1.97520858e-04,   2.46643118e-04, ...,\n",
       "         -2.46647396e-04,  -3.95404604e-05,  -2.58605985e-04],\n",
       "       [ -5.71376295e-05,  -1.30071737e-06,  -2.53239996e-04, ...,\n",
       "          1.78198912e-04,  -8.43432063e-05,  -2.27683529e-04]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm reload:\n",
    "filename = SAVE_TO + '/en_es_rand_600K_cw4_V_dec18.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    C_embedding = pickle.load(f)\n",
    "\n",
    "C_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualization ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrds = \"en_the en_a en_this en_'s en_an en_their en_its en_these en_his \\\n",
    "       en_first en_on en_in en_for en_to en_with en_are en_. en_all \\\n",
    "       it_nuovo it_di it_un it_, <s> it_i it_con it_è it_più it_parola \\\n",
    "       en_censorship en_minima it_profesional it_michail en_الأول \\\n",
    "       it_ritengono en_prevention it_mckenna en_third\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in [\"en_the\", \"en_first\", \"it_nuovo\", \"it_parole\"]:\n",
    "    wrds += list(bi_dict[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordset = set(en_it_vocab.to_ids(wrds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'TSNE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-87e85cb87b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_embeddings_in_2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mmillervedam/Documents/MIDS/w266/FinalProject/Notebooks/models.py\u001b[0m in \u001b[0;36mplot_embeddings_in_2D\u001b[0;34m(self, wordset)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must train the embeddings before plotting.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0mlow_dim_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'TSNE' is not defined"
     ]
    }
   ],
   "source": [
    "model2.plot_embeddings_in_2D(wordset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MLE Starts HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_mle\n",
    "\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# create model\n",
    "model1 = BiW2V_mle(bilingual_dict = en_es_translations,\n",
    "                     vocab = en_es_vocab, \n",
    "                     H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model1.BuildCoreGraph()\n",
    "model1.BuildTrainingGraph()\n",
    "model1.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nBATCHES = 600000 # less than 1 epoch\n",
    "BATCH_SIZE = 48\n",
    "WINDOW_SIZE = 4\n",
    "MAX_EPOCHS = 5 # fail safe\n",
    "DATA_GENERATOR = batch_generator(en_es_data, en_es_vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)\n",
    "TEST_WORDS = en_es_vocab.to_ids(['en_the','en_last', 'es_mundo', 'es_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(200003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(200003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(200003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.000104258473714\n",
      "   [en_the] closest:  es_faunas, en_tetrarchy, es_redován, es_mangano, en_mapsto, en_guastalla, es_grabarían, en_238,\n",
      "   [en_last] closest:  en_lightspeed, en_weisman, es_acertadas, en_6-8, en_10-7, es_eredivisie, es_sarcoidosis, es_taichi,\n",
      "   [es_mundo] closest:  en_scholarship, es_grp, en_friendlies, en_appalachia, es_celtíbera, en_dichotomy, en_unto, en_fian,\n",
      "   [es_real] closest:  en_gyeongsang, es_galen, en_indentured, en_personage, en_santé, en_manual, es_ctm, en_caguas,\n",
      "... STEP 60000 : Average Loss : 2.20471700367\n",
      "... STEP 120000 : Average Loss : 1.85234213303\n",
      "   [en_the] closest:  en_a, en_., en_'s, en_an, en_his, en_its, en_their, en_to,\n",
      "   [en_last] closest:  en_weisman, en_lightspeed, es_acertadas, en_lorna, en_afi, es_iptv, en_at, es_coryphaenoides,\n",
      "   [es_mundo] closest:  en_scholarship, en_friendlies, es_grp, en_appalachia, es_celtíbera, en_unto, en_dichotomy, es_arvensis,\n",
      "   [es_real] closest:  es_la, es_en, en_gyeongsang, en_indentured, es_-, en_personage, es_galen, en_santé,\n",
      "... STEP 180000 : Average Loss : 1.78723174386\n",
      "... STEP 240000 : Average Loss : 1.74137462171\n",
      "   [en_the] closest:  en_a, en_its, en_an, en_., en_'s, en_this, en_their, en_his,\n",
      "   [en_last] closest:  en_at, en_lightspeed, en_weisman, en_first, es_acertadas, es_coryphaenoides, en_for, en_afi,\n",
      "   [es_mundo] closest:  en_scholarship, en_friendlies, es_grp, en_appalachia, es_para, en_had, es_celtíbera, en_unto,\n",
      "   [es_real] closest:  es_la, es_-, es_su, es_en, es_una, en_gyeongsang, en_indentured, es_se,\n",
      "... STEP 300000 : Average Loss : 1.69827337749\n",
      "... STEP 360000 : Average Loss : 1.65491860156\n",
      "   [en_the] closest:  en_a, en_its, en_this, en_an, en_their, en_., en_'s, en_his,\n",
      "   [en_last] closest:  en_first, en_at, en_during, en_for, es_coryphaenoides, en_before, en_ensembles, en_throughout,\n",
      "   [es_mundo] closest:  es_para, en_scholarship, es_un, en_friendlies, en_had, en_appalachia, es_grp, es_celtíbera,\n",
      "   [es_real] closest:  es_la, es_su, es_una, es_en, es_-, es_se, es_para, en_gyeongsang,\n",
      "... STEP 420000 : Average Loss : 1.64496264889\n",
      "... STEP 480000 : Average Loss : 1.62340057515\n",
      "   [en_the] closest:  en_a, en_its, en_this, en_an, en_their, en_., en_'s, en_his,\n",
      "   [en_last] closest:  en_first, en_during, en_before, en_at, en_throughout, en_spent, en_season, en_over,\n",
      "   [es_mundo] closest:  es_para, es_un, en_scholarship, en_friendlies, en_had, en_appalachia, es_grp, en_world,\n",
      "   [es_real] closest:  es_la, es_su, es_una, es_en, es_se, es_-, es_para, en_gyeongsang,\n",
      "... STEP 540000 : Average Loss : 1.59937025807\n",
      "... STEP 600000 : Average Loss : 1.58774573967\n",
      "   [en_the] closest:  en_its, en_this, en_a, en_an, en_their, en_'s, en_his, en_.,\n",
      "   [en_last] closest:  en_first, en_before, en_during, en_throughout, en_spent, en_final, en_season, en_at,\n",
      "   [es_mundo] closest:  es_para, es_un, en_scholarship, en_world, en_friendlies, en_appalachia, es_grp, es_celtíbera,\n",
      "   [es_real] closest:  es_la, es_una, es_su, es_en, es_-, es_se, en_gyeongsang, es_para,\n",
      "... Training Complete\n",
      "... 600000 batches trained in 1836.58886099 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "model1.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = 0.5)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View & Save Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.05455947e-04,   2.86336963e-05,  -1.84920864e-04, ...,\n",
       "         -1.58480965e-04,  -1.00291945e-04,  -1.74433619e-04],\n",
       "       [  2.75573053e-04,  -1.77735696e-04,   3.85022897e-04, ...,\n",
       "          1.47817482e-04,   3.12980497e-04,  -2.16852786e-04],\n",
       "       [ -1.72919987e-04,  -1.40841832e-04,   1.59651914e-04, ...,\n",
       "         -2.03922173e-04,  -1.85943486e-06,   2.17361230e-04],\n",
       "       ..., \n",
       "       [  6.11688884e-05,   2.09798236e-04,  -3.35832774e-05, ...,\n",
       "         -4.93436892e-05,   5.14835165e-06,  -1.18142401e-04],\n",
       "       [ -4.56703710e-05,   2.44815776e-04,  -2.59716689e-05, ...,\n",
       "         -1.92585194e-05,  -3.75785567e-05,  -1.30966378e-04],\n",
       "       [  4.98521388e-07,   9.05969064e-05,  -3.35373807e-05, ...,\n",
       "         -2.10497194e-04,   2.58300832e-04,   1.47561132e-05]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the embeddings\n",
    "model1.context_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.72689295e-04,   2.09802776e-04,  -1.35493896e-03, ...,\n",
       "         -1.16121036e-03,  -7.34851987e-04,  -1.27809762e-03],\n",
       "       [  2.01915926e-03,  -1.30229234e-03,   2.82111228e-03, ...,\n",
       "          1.08307775e-03,   2.29324843e-03,  -1.58890826e-03],\n",
       "       [ -1.26700697e-03,  -1.03196618e-03,   1.16979005e-03, ...,\n",
       "         -1.49416400e-03,  -1.36243179e-05,   1.59263366e-03],\n",
       "       ..., \n",
       "       [  4.48192324e-04,   1.53721869e-03,  -2.46069016e-04, ...,\n",
       "         -3.61547543e-04,   3.77226352e-05,  -8.65644601e-04],\n",
       "       [ -3.34632699e-04,   1.79379666e-03,  -1.90297767e-04, ...,\n",
       "         -1.41109645e-04,  -2.75342929e-04,  -9.59607540e-04],\n",
       "       [  3.65273013e-06,   6.63815183e-04,  -2.45732692e-04, ...,\n",
       "         -1.54234003e-03,   1.89260358e-03,   1.08119944e-04]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the embeddings\n",
    "model1.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving final embeddings in case we want to do more stuff later\n",
    "filename = SAVE_TO + '/en_es_mle_600K_cw4_V_dec18.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model1.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "filename = SAVE_TO + '/en_es_mle_600K_cw4_U_dec18.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model1.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.05455947e-04,   2.86336963e-05,  -1.84920864e-04, ...,\n",
       "         -1.58480965e-04,  -1.00291945e-04,  -1.74433619e-04],\n",
       "       [  2.75573053e-04,  -1.77735696e-04,   3.85022897e-04, ...,\n",
       "          1.47817482e-04,   3.12980497e-04,  -2.16852786e-04],\n",
       "       [ -1.72919987e-04,  -1.40841832e-04,   1.59651914e-04, ...,\n",
       "         -2.03922173e-04,  -1.85943486e-06,   2.17361230e-04],\n",
       "       ..., \n",
       "       [  6.11688884e-05,   2.09798236e-04,  -3.35832774e-05, ...,\n",
       "         -4.93436892e-05,   5.14835165e-06,  -1.18142401e-04],\n",
       "       [ -4.56703710e-05,   2.44815776e-04,  -2.59716689e-05, ...,\n",
       "         -1.92585194e-05,  -3.75785567e-05,  -1.30966378e-04],\n",
       "       [  4.98521388e-07,   9.05969064e-05,  -3.35373807e-05, ...,\n",
       "         -2.10497194e-04,   2.58300832e-04,   1.47561132e-05]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm reload:\n",
    "filename = SAVE_TO + '/en_es_mle_600K_cw4_V_dec18.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    C_embedding = pickle.load(f)\n",
    "\n",
    "C_embedding"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
