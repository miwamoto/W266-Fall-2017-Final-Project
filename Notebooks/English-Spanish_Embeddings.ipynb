{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English - Spanish Embeddings (3 versions)\n",
    "`w266 Final Project: Crosslingual Word Embeddings`\n",
    "\n",
    "Instead of traning on randomly substituted words, here we'll choose the translation that is closest to the context embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Base Paths__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE = '/home/mmillervedam/Data'\n",
    "PROJ = '/home/mmillervedam/ProjectRepo'\n",
    "GTT_BASE = PROJ + '/BaselineModels/data/ground_truth_translations/'\n",
    "#PROJ = '/Users/mona/OneDrive/repos/final_proj/W266-Fall-2017-Final-Project'\n",
    "\n",
    "# directory to save pickled embeddings\n",
    "SAVE_TO = BASE + '/embeddings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Globals__ - _the parameters below fully determine all 3 models in this NB_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "LANG = ('en','es')\n",
    "FULL_TEXT = \"/home/miwamoto/en_es_shuf.txt\"\n",
    "VOCAB_INDEX = BASE + '/vocab/en_es_small.pkl'\n",
    "PANLEX = BASE + '/panlex/en_es_dict.pkl'\n",
    "GTT_PATH = GTT_BASE + \"%s-%s-clean.csv\" % (LANG[1], LANG[0])\n",
    "\n",
    "# Model\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# Training\n",
    "nBATCHES = 100000 # <<< 1 epoch with our 1 million sentence corpus\n",
    "BATCH_SIZE = 48\n",
    "WINDOW_SIZE = 4\n",
    "MAX_EPOCHS = 5 # fail safe\n",
    "ALPHA = 0.5 # authors use a much smaller learning rate but train longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import Corpus, BilingualVocabulary, batch_generator, get_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load corpus\n",
    "raw_data = Corpus(FULL_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load panlex dictionary\n",
    "with open(PANLEX,'rb') as f:\n",
    "    translations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocab = BilingualVocabulary([], languages = LANG)\n",
    "with open(VOCAB_INDEX,'rb') as f:\n",
    "    vocab.load_from_index(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded 702982 panlex translations\n",
      "... loaded 20003 word ('en', 'es') vocabulary\n"
     ]
    }
   ],
   "source": [
    "# confirmations\n",
    "print('... loaded %s panlex translations'%(len(translations)))\n",
    "print('... loaded %s word %s vocabulary'%(vocab.size,vocab.language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... test word ids: [3, 235, 10097, 10409]\n"
     ]
    }
   ],
   "source": [
    "# Validation Words (for training printout)\n",
    "TEST_WORDS = vocab.to_ids(['en_the','en_last', 'es_si', 'es_primero'])\n",
    "print('... test word ids:', TEST_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded 225168 ground truth translations.\n"
     ]
    }
   ],
   "source": [
    "# Ground Truth Translations\n",
    "GTT_DF = pd.read_csv(GTT_PATH, names = [LANG[1], LANG[0]], sep=' ', header=None)\n",
    "print('... loaded %s ground truth translations.'%(len(GTT_DF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded 9276 evaluation words.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Words (for reporting recall)\n",
    "eval_words = [w for w in get_common_words(vocab) if w.startswith(LANG[1])]\n",
    "EVAL_IDS = vocab.to_ids(eval_words)\n",
    "print('... loaded %s evaluation words.' % (len(EVAL_IDS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Random Translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_random\n",
    "\n",
    "# create model\n",
    "model_1 = BiW2V_random(bilingual_dict = translations,\n",
    "                       vocab = vocab, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model_1.BuildCoreGraph()\n",
    "model_1.BuildTrainingGraph()\n",
    "model_1.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fresh data generator\n",
    "DATA_GENERATOR = batch_generator(raw_data, vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(20003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.00130660667419\n",
      "   [en_the] closest:  en_pull, es_censo, es_matorrales, es_grave, es_apolo, es_liberalismo, en_resignation, en_scheme,\n",
      "   [en_last] closest:  en_dye, en_solving, es_quebec, en_doctor, en_preceding, en_grazing, en_halt, es_protagonistas,\n",
      "   [es_si] closest:  en_elements, en_fewer, en_bowling, es_cisneros, en_attain, en_singular, en_carrying, es_empresas,\n",
      "   [es_primero] closest:  en_macedonian, en_dating, en_afl, en_highest, en_freud, en_dense, en_pits, es_atlántico,\n",
      "... STEP 10000 : Average Loss : 4.3614641548\n",
      "... STEP 20000 : Average Loss : 4.03525338483\n",
      "   [en_the] closest:  en_a, es_grave, en_pull, es_liberalismo, es_censo, en_notorious, es_probabilidad, es_repentina,\n",
      "   [en_last] closest:  en_solving, en_dye, en_preceding, en_doctor, es_quebec, en_grazing, es_protagonistas, en_lutheran,\n",
      "   [es_si] closest:  en_fewer, en_elements, en_bowling, es_cisneros, en_singular, en_carrying, en_attain, es_empresas,\n",
      "   [es_primero] closest:  en_macedonian, en_dating, en_afl, en_highest, en_freud, en_dense, en_pits, es_atlántico,\n",
      "... STEP 30000 : Average Loss : 3.95121204739\n",
      "... STEP 40000 : Average Loss : 3.89561320728\n",
      "   [en_the] closest:  en_a, es_la, es_liberalismo, es_grave, en_pull, es_censo, es_probabilidad, en_another,\n",
      "   [en_last] closest:  en_solving, en_doctor, en_preceding, en_dye, es_protagonistas, en_grazing, en_lutheran, es_quebec,\n",
      "   [es_si] closest:  es_por, es_., en_fewer, en_elements, es_del, es_cisneros, en_carrying, en_singular,\n",
      "   [es_primero] closest:  en_dating, en_macedonian, en_afl, en_highest, en_freud, en_dense, es_plantel, en_pits,\n",
      "... STEP 50000 : Average Loss : 3.8376687728\n",
      "... STEP 60000 : Average Loss : 3.83208325282\n",
      "   [en_the] closest:  en_a, es_la, es_su, en_his, en_its, en_an, en_another, en_pull,\n",
      "   [en_last] closest:  en_doctor, en_preceding, en_solving, es_protagonistas, en_dye, en_grazing, en_lutheran, es_quebec,\n",
      "   [es_si] closest:  es_por, es_del, es_., en_fewer, es_cisneros, en_elements, en_carrying, en_singular,\n",
      "   [es_primero] closest:  en_dating, en_afl, en_macedonian, en_highest, en_dense, en_freud, es_plantel, en_blockade,\n",
      "... STEP 70000 : Average Loss : 3.7822154462\n",
      "... STEP 80000 : Average Loss : 3.79664892309\n",
      "   [en_the] closest:  en_a, es_la, en_an, en_its, es_su, en_their, en_his, es_el,\n",
      "   [en_last] closest:  en_doctor, en_preceding, en_solving, es_protagonistas, en_grazing, en_dye, en_lutheran, en_vegetation,\n",
      "   [es_si] closest:  es_por, es_del, en_singular, en_carrying, en_elements, es_cisneros, en_fewer, es_.,\n",
      "   [es_primero] closest:  en_afl, en_dating, en_macedonian, en_highest, en_dense, es_plantel, en_freud, en_blockade,\n",
      "... STEP 90000 : Average Loss : 3.74241748057\n",
      "... STEP 100000 : Average Loss : 3.69778117068\n",
      "   [en_the] closest:  en_a, es_la, en_its, en_an, en_their, es_su, es_una, en_his,\n",
      "   [en_last] closest:  en_preceding, en_doctor, en_grazing, es_protagonistas, en_solving, en_dye, en_lutheran, en_vegetation,\n",
      "   [es_si] closest:  es_por, es_del, en_carrying, en_singular, en_elements, es_cisneros, en_fewer, es_pasados,\n",
      "   [es_primero] closest:  en_afl, en_dating, en_macedonian, en_highest, en_dense, es_plantel, en_freud, en_blockade,\n",
      "... Training Complete\n",
      "... 100000 batches trained in 280.030622005 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "model_1.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = ALPHA)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context \n",
    "filename = SAVE_TO + '/en_es_rand_100K_V_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_1.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# word\n",
    "filename = SAVE_TO + '/en_es_rand_100K_U_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_1.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load saved embeddings\n",
    "with open(SAVE_TO + '/en_es_rand_100K_V_dec19.pkl','rb') as f:\n",
    "    C_embedding1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... C shape: (20003, 200)\n",
      "... eval IDs should be > 10003: [12873, 17315, 15943, 12143, 12575]\n",
      "... number to eval: 9276\n",
      "... ground truth source language: es\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "print('... C shape:', C_embedding1.shape)\n",
    "print('... eval IDs should be > 10003:', EVAL_IDS[:5])\n",
    "print('... number to eval:', len(EVAL_IDS))\n",
    "print('... ground truth source language:', GTT_DF.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bilingual Induction Task__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import evaluateBLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Evaluating 9276 'es' Ground Truth Translations\n",
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V validation.\n",
      "... finding neighbors...\n",
      "... Done. Total successful translation rate: 0 (23 / 9276)\n"
     ]
    }
   ],
   "source": [
    "src_nbrs, tgt_nbrs = evaluateBLI(C_embedding1, vocab, GTT_DF, \n",
    "                                 EVAL_IDS, top_k = 10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es_primero :\n",
      ">>> ['es_primero', 'es_plantel', 'es_atl\\xc3\\xa1ntico', 'es_japoneses', 'es_autoridades', 'es_fall', 'es_ampliaci\\xc3\\xb3n', 'es_directora', 'es_mateo', 'es_andina']\n",
      ">>> ['en_afl', 'en_dating', 'en_macedonian', 'en_highest', 'en_dense', 'en_freud', 'en_blockade', 'en_pits', 'en_article', 'en_hydrogen']\n"
     ]
    }
   ],
   "source": [
    "# visual check\n",
    "for wrd_id in TEST_WORDS:\n",
    "    try:\n",
    "        idx = EVAL_IDS.index(wrd_id)\n",
    "    except:\n",
    "        continue\n",
    "    synon = vocab.to_words(src_nbrs[idx])\n",
    "    trans = vocab.to_words(tgt_nbrs[idx])\n",
    "    print(vocab.to_words([wrd_id])[0],\":\")\n",
    "    print(\">>>\", synon)\n",
    "    print(\">>>\", trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Most Common Target Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_mle\n",
    "\n",
    "# create model\n",
    "model_2 = BiW2V_mle(bilingual_dict = translations,\n",
    "                       vocab = vocab, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model_2.BuildCoreGraph()\n",
    "model_2.BuildTrainingGraph()\n",
    "model_2.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fresh data generator\n",
    "DATA_GENERATOR = batch_generator(raw_data, vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(20003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.00128500576019\n",
      "   [en_the] closest:  en_cultivation, en_origin, en_trafficking, en_duo, en_disaster, es_aspecto, es_zoo, es_discutir,\n",
      "   [en_last] closest:  en_austrian, en_denote, en_fight, es_berkeley, en_abortion, en_excessive, es_futuros, en_save,\n",
      "   [es_si] closest:  es_diputado, en_psychiatry, es_regimiento, en_drained, en_arizona, es_antioquia, es_abandonada, es_mosela,\n",
      "   [es_primero] closest:  es_best, en_marketplace, en_roots, en_detailing, es_cineasta, es_parlamento, en_terrestrial, en_continental,\n",
      "... STEP 10000 : Average Loss : 4.37539166083\n",
      "... STEP 20000 : Average Loss : 4.00909108711\n",
      "   [en_the] closest:  en_a, en_origin, en_cultivation, en_his, en_and, en_sexuality, es_un, en_penalties,\n",
      "   [en_last] closest:  en_austrian, en_denote, en_fight, es_berkeley, en_abortion, es_futuros, en_excessive, en_jo,\n",
      "   [es_si] closest:  es_diputado, en_psychiatry, es_por, es_regimiento, en_drained, es_niza, es_una, es_escandinavia,\n",
      "   [es_primero] closest:  es_best, en_marketplace, en_roots, en_detailing, es_parlamento, en_salisbury, es_cineasta, en_continental,\n",
      "... STEP 30000 : Average Loss : 3.91010784086\n",
      "... STEP 40000 : Average Loss : 3.85902561221\n",
      "   [en_the] closest:  en_a, en_his, en_cultivation, en_origin, en_their, es_la, en_and, es_un,\n",
      "   [en_last] closest:  en_austrian, en_denote, es_berkeley, en_fight, en_abortion, es_futuros, en_excessive, en_jo,\n",
      "   [es_si] closest:  es_por, es_diputado, es_una, en_psychiatry, es_niza, es_regimiento, en_drained, en_argument,\n",
      "   [es_primero] closest:  es_best, en_marketplace, en_roots, en_detailing, es_cineasta, es_parlamento, en_continental, en_salisbury,\n",
      "... STEP 50000 : Average Loss : 3.79621106901\n",
      "... STEP 60000 : Average Loss : 3.77302563875\n",
      "   [en_the] closest:  en_a, en_his, en_their, en_an, en_origin, en_cultivation, es_la, es_un,\n",
      "   [en_last] closest:  en_austrian, es_futuros, en_denote, en_abortion, en_fight, es_berkeley, en_excessive, en_jo,\n",
      "   [es_si] closest:  es_por, es_diputado, es_una, en_that, en_psychiatry, es_niza, es_puede, es_como,\n",
      "   [es_primero] closest:  es_best, en_marketplace, en_roots, en_detailing, es_parlamento, es_cineasta, en_continental, en_salisbury,\n",
      "... STEP 70000 : Average Loss : 3.72301409636\n",
      "... STEP 80000 : Average Loss : 3.71647097499\n",
      "   [en_the] closest:  en_a, en_his, en_their, en_an, en_its, en_cultivation, en_origin, es_un,\n",
      "   [en_last] closest:  en_austrian, es_futuros, en_abortion, en_fight, en_denote, es_berkeley, en_excessive, en_jo,\n",
      "   [es_si] closest:  es_por, es_una, en_that, es_puede, es_diputado, es_como, es_niza, en_psychiatry,\n",
      "   [es_primero] closest:  es_best, en_marketplace, en_detailing, en_roots, es_parlamento, en_continental, es_cineasta, en_salisbury,\n",
      "... STEP 90000 : Average Loss : 3.68035427465\n",
      "... STEP 100000 : Average Loss : 3.6224290942\n",
      "   [en_the] closest:  en_a, en_their, en_his, en_an, en_its, en_cultivation, en_origin, es_un,\n",
      "   [en_last] closest:  en_austrian, es_futuros, en_abortion, en_fight, en_denote, es_berkeley, en_excessive, en_pm,\n",
      "   [es_si] closest:  es_por, es_una, es_puede, en_that, es_como, es_diputado, es_o, es_niza,\n",
      "   [es_primero] closest:  es_best, en_marketplace, en_detailing, en_roots, es_según, es_parlamento, en_continental, es_cineasta,\n",
      "... Training Complete\n",
      "... 100000 batches trained in 324.108913898 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "model_2.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = ALPHA)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context \n",
    "filename = SAVE_TO + '/en_es_mle_100K_V_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_2.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# word\n",
    "filename = SAVE_TO + '/en_es_mle_100K_U_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_2.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load saved embeddings\n",
    "with open(SAVE_TO + '/en_es_mle_100K_V_dec19.pkl','rb') as f:\n",
    "    C_embedding2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... C shape: (20003, 200)\n",
      "... eval IDs should be > 10003: [12873, 17315, 15943, 12143, 12575]\n",
      "... number to eval: 9276\n",
      "... ground truth source language: es\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "print('... C shape:', C_embedding2.shape)\n",
    "print('... eval IDs should be > 10003:', EVAL_IDS[:5])\n",
    "print('... number to eval:', len(EVAL_IDS))\n",
    "print('... ground truth source language:', GTT_DF.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bilingual Induction Task__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import evaluateBLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Evaluating 9276 'es' Ground Truth Translations\n",
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V validation.\n",
      "... finding neighbors...\n"
     ]
    }
   ],
   "source": [
    "src_nbrs, tgt_nbrs = evaluateBLI(C_embedding2, vocab, GTT_DF, \n",
    "                                 EVAL_IDS, top_k = 10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visual check\n",
    "for wrd_id in TEST_WORDS:\n",
    "    try:\n",
    "        idx = EVAL_IDS.index(wrd_id)\n",
    "    except:\n",
    "        continue\n",
    "    synon = vocab.to_words(src_nbrs[idx])\n",
    "    trans = vocab.to_words(tgt_nbrs[idx])\n",
    "    print(vocab.to_words([wrd_id])[0],\":\")\n",
    "    print(\">>>\", synon)\n",
    "    print(\">>>\", trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: Closest Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_nn\n",
    "\n",
    "# create model\n",
    "model_3 = BiW2V_nn(bilingual_dict = translations,\n",
    "                   vocab = vocab, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model_3.BuildCoreGraph()\n",
    "model_3.BuildTrainingGraph()\n",
    "model_3.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fresh data generator\n",
    "DATA_GENERATOR = batch_generator(raw_data, vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(20003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.0236346282959\n",
      "   [en_the] closest:  es_break, es_satisfacer, en_intervention, es_reverendo, en_aboriginal, en_gorge, es_controvertida, en_cancel,\n",
      "   [en_last] closest:  en_furnace, en_angelo, es_emplazamiento, en_respond, es_más, es_ninguno, en_progressive, en_optimal,\n",
      "   [es_si] closest:  es_canta, es_jurisdicción, es_ego, en_until, es_atlántica, es_colaboración, en_guidelines, en_noticed,\n",
      "   [es_primero] closest:  en_space, en_scheduled, es_león, en_deficiency, es_primavera, en_catalogue, en_declaration, en_ponds,\n",
      "... STEP 500 : Average Loss : 6.23121501386\n",
      "... STEP 1000 : Average Loss : 5.38436549139\n",
      "   [en_the] closest:  es_break, en_intervention, en_cancel, es_reverendo, en_gorge, en_neighboring, es_satisfacer, es_origina,\n",
      "   [en_last] closest:  en_furnace, en_angelo, es_emplazamiento, en_respond, es_más, es_ninguno, en_progressive, en_optimal,\n",
      "   [es_si] closest:  es_canta, es_jurisdicción, es_ego, en_until, es_atlántica, es_colaboración, en_guidelines, en_noticed,\n",
      "   [es_primero] closest:  en_space, en_scheduled, es_león, en_deficiency, es_primavera, en_catalogue, en_declaration, en_ponds,\n",
      "... STEP 1500 : Average Loss : 5.25732460451\n",
      "... STEP 2000 : Average Loss : 5.09882386208\n",
      "   [en_the] closest:  es_break, en_intervention, es_la, en_gorge, en_cancel, es_reverendo, en_neighboring, en_still,\n",
      "   [en_last] closest:  en_furnace, en_angelo, es_emplazamiento, en_respond, es_más, es_ninguno, en_progressive, en_optimal,\n",
      "   [es_si] closest:  es_canta, es_jurisdicción, es_ego, en_until, es_atlántica, es_colaboración, en_guidelines, en_noticed,\n",
      "   [es_primero] closest:  en_space, en_scheduled, es_león, en_deficiency, es_primavera, en_catalogue, en_declaration, en_ponds,\n",
      "... STEP 2500 : Average Loss : 5.00043768167\n",
      "... STEP 3000 : Average Loss : 4.9841535778\n",
      "   [en_the] closest:  es_break, en_intervention, en_a, es_la, en_gorge, en_cancel, en_neighboring, es_reverendo,\n",
      "   [en_last] closest:  en_furnace, en_angelo, es_emplazamiento, en_respond, es_más, es_ninguno, en_progressive, en_optimal,\n",
      "   [es_si] closest:  es_canta, es_jurisdicción, es_ego, en_until, es_colaboración, es_atlántica, en_guidelines, en_noticed,\n",
      "   [es_primero] closest:  en_space, en_scheduled, es_león, en_deficiency, es_primavera, en_catalogue, en_declaration, en_ponds,\n",
      "... STEP 3500 : Average Loss : 4.93343259811\n",
      "... STEP 4000 : Average Loss : 4.99372910714\n",
      "   [en_the] closest:  en_a, es_break, en_intervention, en_gorge, en_cancel, en_still, en_neighboring, es_reverendo,\n",
      "   [en_last] closest:  en_furnace, en_angelo, es_emplazamiento, es_más, en_respond, es_ninguno, en_progressive, en_optimal,\n",
      "   [es_si] closest:  es_canta, es_jurisdicción, es_ego, en_until, es_colaboración, es_atlántica, en_guidelines, en_noticed,\n",
      "   [es_primero] closest:  en_space, en_scheduled, es_león, en_deficiency, es_primavera, en_catalogue, en_declaration, en_ponds,\n",
      "... STEP 4500 : Average Loss : 4.91129490376\n",
      "... STEP 5000 : Average Loss : 4.81853557158\n",
      "   [en_the] closest:  en_a, es_break, en_intervention, en_gorge, en_cancel, en_neighboring, en_still, es_reverendo,\n",
      "   [en_last] closest:  en_furnace, en_angelo, es_emplazamiento, es_más, en_respond, es_ninguno, en_progressive, en_optimal,\n",
      "   [es_si] closest:  es_canta, es_jurisdicción, es_ego, en_until, es_colaboración, es_atlántica, en_guidelines, en_noticed,\n",
      "   [es_primero] closest:  en_space, en_scheduled, es_león, es_primavera, en_deficiency, en_catalogue, en_declaration, en_ponds,\n",
      "... Training Complete\n",
      "... 5000 batches trained in 2623.83593607 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "nBATCHES = 5000 # Takes too long w/ nn so we'll only do 5K\n",
    "start = time.time()\n",
    "model_3.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = ALPHA)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context \n",
    "filename = SAVE_TO + '/en_es_nn_5K_V_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_2.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# word\n",
    "filename = SAVE_TO + '/en_es_nn_5K_U_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_2.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load saved embeddings\n",
    "with open(SAVE_TO + '/en_es_nn_5K_V_dec19.pkl','rb') as f:\n",
    "    C_embedding3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "print('... C shape:', C_embedding3.shape)\n",
    "print('... eval IDs should be > 10003:', EVAL_IDS[:5])\n",
    "print('... number to eval:', len(EVAL_IDS))\n",
    "print('... ground truth source language:', GTT_DF.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bilingual Induction Task__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import evaluateBLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_nbrs, tgt_nbrs = evaluateBLI(C_embedding3, vocab, gtt_df, \n",
    "                                 EVAL_IDS, top_k = 10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visual check\n",
    "for wrd_id in TEST_WORDS:\n",
    "    try:\n",
    "        idx = EVAL_IDS.index(wrd_id)\n",
    "    except:\n",
    "        continue\n",
    "    synon = vocab.to_words(src_nbrs[idx])\n",
    "    trans = vocab.to_words(tgt_nbrs[idx])\n",
    "    print(vocab.to_words([wrd_id])[0],\":\")\n",
    "    print(\">>>\", synon)\n",
    "    print(\">>>\", trans)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
