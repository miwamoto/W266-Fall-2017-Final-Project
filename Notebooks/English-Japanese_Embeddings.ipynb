{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English - Japanese Embeddings (3 versions)\n",
    "`w266 Final Project: Crosslingual Word Embeddings`\n",
    "\n",
    "Instead of traning on randomly substituted words, here we'll choose the translation that is closest to the context embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Base Paths__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maya's paths\n",
    "#BASE = '/home/mmillervedam/Data'\n",
    "#PROJ = '/home/mmillervedam/ProjectRepo'\n",
    "\n",
    "# Mona;s paths\n",
    "BASE = '/home/miwamoto/Data'\n",
    "PROJ = '/home/miwamoto/W266-Fall-2017-Final-Project'\n",
    "\n",
    "GTT_BASE = PROJ + '/BaselineModels/data/ground_truth_translations/'\n",
    "\n",
    "# directory to save pickled embeddings\n",
    "SAVE_TO = BASE + '/embeddings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Globals__ - _the parameters below fully determine all 3 models in this NB_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "LANG = ('en','ja')\n",
    "FULL_TEXT = \"/home/miwamoto/en_ja_shuf.txt\"\n",
    "VOCAB_INDEX = BASE + '/vocab/en_ja_small.pkl'\n",
    "PANLEX = BASE + '/panlex/en_ja_dict.pkl'\n",
    "GTT_PATH = GTT_BASE + \"%s-%s-clean.csv\" % (LANG[0], LANG[1])\n",
    "\n",
    "# Model\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# Training\n",
    "nBATCHES = 50000 # <<< 1 epoch with our 1 million sentence corpus\n",
    "BATCH_SIZE = 48\n",
    "WINDOW_SIZE = 4\n",
    "MAX_EPOCHS = 5 # fail safe\n",
    "ALPHA = 0.5 # authors use a much smaller learning rate but train longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing import Corpus, BilingualVocabulary, batch_generator, get_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "raw_data = Corpus(FULL_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load panlex dictionary\n",
    "with open(PANLEX,'rb') as f:\n",
    "    translations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocab = BilingualVocabulary([], languages = LANG)\n",
    "with open(VOCAB_INDEX,'rb') as f:\n",
    "    vocab.load_from_index(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded 634705 panlex translations\n",
      "... loaded 20003 word ('en', 'ja') vocabulary\n"
     ]
    }
   ],
   "source": [
    "# confirmations\n",
    "print('... loaded %s panlex translations'%(len(translations)))\n",
    "print('... loaded %s word %s vocabulary'%(vocab.size,vocab.language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... test word ids: [3, 228, 10004, 10012]\n",
      "en_exemption\n",
      "en_bohemian\n",
      "en_walnut\n",
      "en_ljubljana\n",
      "en_timor\n",
      "en_venom\n",
      "en_scriptures\n",
      "en_tariff\n",
      "en_penetration\n",
      "en_pedal\n",
      "en_transmissions\n",
      "en_fluent\n",
      "en_sexes\n",
      "ja_年\n",
      "ja_月\n",
      "ja_日\n",
      "ja_的\n",
      "ja_3\n",
      "ja_第\n",
      "ja_人\n",
      "ja_者\n",
      "ja_後\n",
      "ja_日本\n",
      "ja_行う\n",
      "ja_中\n",
      "ja_一\n",
      "ja_現在\n",
      "ja_時\n",
      "ja_化\n",
      "ja_大学\n"
     ]
    }
   ],
   "source": [
    "# Validation Words (for training printout)\n",
    "TEST_WORDS = vocab.to_ids(['en_the','en_last', 'ja_月', 'ja_日本'])\n",
    "print('... test word ids:', TEST_WORDS)\n",
    "for i in range(9990,10020):\n",
    "    print(vocab.index[i])\n",
    "#print(vocab.wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded 35354 ground truth translations.\n",
      "/home/miwamoto/W266-Fall-2017-Final-Project/BaselineModels/data/ground_truth_translations/en-ja-clean.csv\n",
      "en ja\n"
     ]
    }
   ],
   "source": [
    "# Ground Truth Translations\n",
    "GTT_DF = pd.read_csv(GTT_PATH, names = [LANG[1], LANG[0]], sep=' ', header=None)\n",
    "print('... loaded %s ground truth translations.'%(len(GTT_DF)))\n",
    "print(GTT_PATH)\n",
    "print(LANG[0], LANG[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded 4425 evaluation words.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Words (for reporting recall)\n",
    "eval_words = [w for w in get_common_words(vocab) if w.startswith(LANG[1])]\n",
    "EVAL_IDS = vocab.to_ids(eval_words)\n",
    "print('... loaded %s evaluation words.' % (len(EVAL_IDS)))\n",
    "#print(repr(eval_words[:5]).decode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Random Translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_random\n",
    "\n",
    "# create model\n",
    "model_1 = BiW2V_random(bilingual_dict = translations,\n",
    "                       vocab = vocab, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model_1.BuildCoreGraph()\n",
    "model_1.BuildTrainingGraph()\n",
    "model_1.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh data generator\n",
    "DATA_GENERATOR = batch_generator(raw_data, vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(20003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.0034091506958\n",
      "   [en_the] closest:  en_odyssey, en_ram, en_saturday, ja_早朝, en_beaver, ja_岳, en_feast, ja_朝日新聞社,\n",
      "   [en_last] closest:  ja_派遣, ja_行ける, ja_様, ja_渡, ja_湯, en_duncan, ja_市区, en_implemented,\n",
      "   [ja_月] closest:  ja_ホームラン, ja_標識, en_qualities, en_wartime, en_brilliant, en_real-time, ja_後述, ja_同調,\n",
      "   [ja_日本] closest:  ja_セクタ, en_wheat, en_designed, en_disaster, ja_掴む, en_hello, ja_ふさわしい, en_gym,\n",
      "... STEP 5000 : Average Loss : 4.80178324621\n",
      "... STEP 10000 : Average Loss : 4.42765667364\n",
      "   [en_the] closest:  en_a, en_and, en_to, en_feast, ja_朝日新聞社, ja_早朝, ja_つば, en_window,\n",
      "   [en_last] closest:  ja_派遣, ja_行ける, ja_様, ja_湯, ja_渡, en_implemented, en_unknown, en_duncan,\n",
      "   [ja_月] closest:  ja_ホームラン, en_wartime, ja_標識, en_brilliant, en_real-time, en_unable, ja_番手, ja_同調,\n",
      "   [ja_日本] closest:  ja_セクタ, en_wheat, en_designed, ja_掴む, en_disaster, en_gym, ja_ふさわしい, en_hello,\n",
      "... STEP 15000 : Average Loss : 4.37147912606\n",
      "... STEP 20000 : Average Loss : 4.309117092\n",
      "   [en_the] closest:  en_a, en_and, en_an, en_feast, ja_早朝, en_ram, ja_朝日新聞社, ja_つば,\n",
      "   [en_last] closest:  ja_派遣, ja_行ける, ja_湯, ja_様, ja_渡, en_implemented, en_unknown, ja_子会社,\n",
      "   [ja_月] closest:  ja_ホームラン, en_wartime, en_unable, ja_番手, en_brilliant, en_real-time, ja_標識, ja_品,\n",
      "   [ja_日本] closest:  ja_セクタ, en_wheat, en_designed, ja_掴む, en_gym, ja_ふさわしい, en_disaster, ja_決断,\n",
      "... STEP 25000 : Average Loss : 4.22568185294\n",
      "... STEP 30000 : Average Loss : 4.24140441358\n",
      "   [en_the] closest:  en_a, en_an, en_and, en_feast, ja_早朝, ja_朝日新聞社, en_their, en_ram,\n",
      "   [en_last] closest:  ja_派遣, ja_行ける, ja_湯, ja_様, ja_渡, en_unknown, en_implemented, ja_子会社,\n",
      "   [ja_月] closest:  ja_年, ja_ホームラン, ja_番手, en_unable, en_wartime, en_real-time, en_brilliant, ja_報告,\n",
      "   [ja_日本] closest:  ja_セクタ, en_wheat, ja_掴む, en_designed, en_gym, ja_ふさわしい, en_disaster, en_hancock,\n",
      "... STEP 35000 : Average Loss : 4.20761260872\n",
      "... STEP 40000 : Average Loss : 4.17715613236\n",
      "   [en_the] closest:  en_a, en_an, en_their, ja_早朝, en_feast, en_ram, en_his, ja_朝日新聞社,\n",
      "   [en_last] closest:  ja_派遣, ja_行ける, ja_湯, ja_渡, ja_様, en_unknown, en_implemented, ja_子会社,\n",
      "   [ja_月] closest:  ja_年, ja_ホームラン, ja_番手, en_wartime, en_unable, ja_報告, en_real-time, en_brilliant,\n",
      "   [ja_日本] closest:  ja_セクタ, en_wheat, ja_者, ja_掴む, en_designed, en_gym, ja_ふさわしい, ja_一,\n",
      "... STEP 45000 : Average Loss : 4.04557069729\n",
      "... STEP 50000 : Average Loss : 4.10841253561\n",
      "   [en_the] closest:  en_a, en_an, en_their, en_his, ja_早朝, en_this, en_odyssey, en_ram,\n",
      "   [en_last] closest:  ja_派遣, ja_行ける, ja_湯, ja_渡, ja_様, en_unknown, en_implemented, ja_子会社,\n",
      "   [ja_月] closest:  ja_年, ja_ホームラン, en_wartime, ja_番手, en_unable, ja_報告, en_real-time, en_kids,\n",
      "   [ja_日本] closest:  ja_者, ja_セクタ, ja_行う, en_wheat, ja_掴む, ja_一, en_gym, ja_ふさわしい,\n",
      "... Training Complete\n",
      "... 50000 batches trained in 139.029198885 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "model_1.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = ALPHA)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context \n",
    "filename = SAVE_TO + '/en_ja_rand_500K_V_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_1.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# word\n",
    "filename = SAVE_TO + '/en_ja_rand_500K_U_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_1.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved embeddings\n",
    "with open(SAVE_TO + '/en_ja_rand_500K_V_dec19.pkl','rb') as f:\n",
    "    C_embedding1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... C shape: (20003, 200)\n",
      "... eval IDs should be > 10003: [11818, 10216, 12063, 19195, 16293]\n",
      "... number to eval: 4425\n",
      "... ground truth source language: ja\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "print('... C shape:', C_embedding1.shape)\n",
    "print('... eval IDs should be > 10003:', EVAL_IDS[:5])\n",
    "print('... number to eval:', len(EVAL_IDS))\n",
    "print('... ground truth source language:', GTT_DF.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bilingual Induction Task__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import evaluateBLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Evaluating 4425 'ja' Ground Truth Translations\n",
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V validation.\n",
      "... finding neighbors...\n",
      "... Done. Total successful translation rate: 0 (0 / 4425)\n"
     ]
    }
   ],
   "source": [
    "src_nbrs, tgt_nbrs = evaluateBLI(C_embedding1, vocab, GTT_DF, \n",
    "                                 EVAL_IDS, top_k = 10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ja_日本 :\n",
      ">>> ['ja_\\xe6\\x97\\xa5\\xe6\\x9c\\xac', 'ja_\\xe8\\x80\\x85', 'ja_\\xe3\\x82\\xbb\\xe3\\x82\\xaf\\xe3\\x82\\xbf', 'ja_\\xe8\\xa1\\x8c\\xe3\\x81\\x86', 'ja_\\xe6\\x8e\\xb4\\xe3\\x82\\x80', 'ja_\\xe4\\xb8\\x80', 'ja_\\xe3\\x81\\xb5\\xe3\\x81\\x95\\xe3\\x82\\x8f\\xe3\\x81\\x97\\xe3\\x81\\x84', 'ja_\\xe7\\x9a\\x84', 'ja_\\xe8\\xbb\\x8d', 'ja_\\xe7\\x9f\\xa5\\xe4\\xba\\x8b']\n",
      ">>> ['en_wheat', 'en_gym', 'en_designed', 'en_hancock', 'en_integrity', 'en_disaster', 'en_plaza', 'en_bud', 'en_beneath', 'en_bo']\n"
     ]
    }
   ],
   "source": [
    "# visual check\n",
    "for wrd_id in TEST_WORDS:\n",
    "    try:\n",
    "        idx = EVAL_IDS.index(wrd_id)\n",
    "    except:\n",
    "        continue\n",
    "    synon = vocab.to_words(src_nbrs[idx])\n",
    "    trans = vocab.to_words(tgt_nbrs[idx])\n",
    "    print(vocab.to_words([wrd_id])[0],\":\")\n",
    "    print(\">>>\", synon)\n",
    "    print(\">>>\", trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Most Common Target Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_mle\n",
    "\n",
    "# create model\n",
    "model_2 = BiW2V_mle(bilingual_dict = translations,\n",
    "                       vocab = vocab, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model_2.BuildCoreGraph()\n",
    "model_2.BuildTrainingGraph()\n",
    "model_2.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh data generator\n",
    "DATA_GENERATOR = batch_generator(raw_data, vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(20003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.00321937179565\n",
      "   [en_the] closest:  ja_re, en_16, ja_立教, ja_居, en_buddhist, ja_野生, en_granddaughter, ja_余地,\n",
      "   [en_last] closest:  en_labrador, en_hal, ja_良, en_imam, en_cerebral, en_aging, en_bite, en_detailed,\n",
      "   [ja_月] closest:  ja_遭遇, en_russell, ja_社会党, en_getting, en_maurice, ja_向上, ja_npo, ja_清朝,\n",
      "   [ja_日本] closest:  en_melting, ja_院, en_pupil, en_line, ja_尹, ja_出典, en_parody, en_striker,\n",
      "... STEP 5000 : Average Loss : 5.10440200653\n",
      "... STEP 10000 : Average Loss : 4.62019368992\n",
      "   [en_the] closest:  en_a, en_16, ja_余地, ja_野生, en_particularly, ja_兼ねる, en_life, ja_居,\n",
      "   [en_last] closest:  en_labrador, en_hal, ja_良, en_imam, en_bite, en_aging, en_cerebral, en_forget,\n",
      "   [ja_月] closest:  ja_npo, ja_遭遇, en_getting, en_russell, en_meant, ja_相当, ja_清朝, ja_社会党,\n",
      "   [ja_日本] closest:  ja_院, en_pupil, en_melting, en_line, ja_尹, en_parody, ja_出典, ja_水泳,\n",
      "... STEP 15000 : Average Loss : 4.5368384414\n",
      "... STEP 20000 : Average Loss : 4.45160590127\n",
      "   [en_the] closest:  en_a, en_16, ja_余地, ja_居, ja_野生, ja_兼ねる, en_granddaughter, en_particularly,\n",
      "   [en_last] closest:  en_labrador, en_hal, ja_良, en_bite, en_imam, en_aging, en_forget, en_devastating,\n",
      "   [ja_月] closest:  ja_npo, ja_遭遇, en_getting, en_russell, en_meant, en_rouge, en_officials, ja_清朝,\n",
      "   [ja_日本] closest:  ja_院, en_pupil, ja_尹, en_line, en_melting, en_parody, ja_水泳, en_facebook,\n",
      "... STEP 25000 : Average Loss : 4.37287760735\n",
      "... STEP 30000 : Average Loss : 4.36085861535\n",
      "   [en_the] closest:  en_a, en_an, en_16, ja_居, ja_兼ねる, ja_野生, en_particularly, en_granddaughter,\n",
      "   [en_last] closest:  en_hal, en_labrador, en_bite, en_devastating, en_forget, en_imam, en_aging, ja_良,\n",
      "   [ja_月] closest:  ja_npo, ja_遭遇, ja_年, en_russell, en_getting, en_meant, ja_第, en_officials,\n",
      "   [ja_日本] closest:  ja_院, ja_尹, en_pupil, en_parody, ja_年, en_line, en_melting, ja_水泳,\n",
      "... STEP 35000 : Average Loss : 4.32837978203\n",
      "... STEP 40000 : Average Loss : 4.29448601015\n",
      "   [en_the] closest:  en_a, en_an, en_its, en_16, ja_兼ねる, en_granddaughter, ja_居, ja_野生,\n",
      "   [en_last] closest:  en_hal, en_labrador, en_bite, en_devastating, en_forget, en_imam, en_aging, en_births,\n",
      "   [ja_月] closest:  ja_年, ja_npo, ja_第, ja_遭遇, en_russell, en_meant, en_rouge, en_getting,\n",
      "   [ja_日本] closest:  ja_年, ja_的, ja_院, ja_尹, en_pupil, ja_放送, en_parody, ja_水泳,\n",
      "... STEP 45000 : Average Loss : 4.15093339341\n",
      "... STEP 50000 : Average Loss : 4.21378524487\n",
      "   [en_the] closest:  en_a, en_an, en_its, en_their, ja_兼ねる, ja_人民, en_16, en_part,\n",
      "   [en_last] closest:  en_hal, en_labrador, en_bite, en_devastating, en_forget, en_births, en_aging, en_imam,\n",
      "   [ja_月] closest:  ja_年, ja_第, ja_npo, ja_遭遇, en_russell, en_rouge, en_statistical, en_meant,\n",
      "   [ja_日本] closest:  ja_年, ja_的, ja_院, ja_尹, en_pupil, ja_放送, en_parody, ja_3,\n",
      "... Training Complete\n",
      "... 50000 batches trained in 152.415274858 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "model_2.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = ALPHA)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context \n",
    "filename = SAVE_TO + '/en_ja_mle_50K_V_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_2.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# word\n",
    "filename = SAVE_TO + '/en_ja_mle_50K_U_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_2.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved embeddings\n",
    "with open(SAVE_TO + '/en_ja_mle_50K_V_dec19.pkl','rb') as f:\n",
    "    C_embedding2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... C shape: (20003, 200)\n",
      "... eval IDs should be > 10003: [11818, 10216, 12063, 19195, 16293]\n",
      "... number to eval: 4425\n",
      "... ground truth source language: ja\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "print('... C shape:', C_embedding2.shape)\n",
    "print('... eval IDs should be > 10003:', EVAL_IDS[:5])\n",
    "print('... number to eval:', len(EVAL_IDS))\n",
    "print('... ground truth source language:', GTT_DF.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bilingual Induction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import evaluateBLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Evaluating 4425 'ja' Ground Truth Translations\n",
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V validation.\n",
      "... finding neighbors...\n",
      "... Done. Total successful translation rate: 0 (0 / 4425)\n"
     ]
    }
   ],
   "source": [
    "src_nbrs, tgt_nbrs = evaluateBLI(C_embedding2, vocab, GTT_DF, \n",
    "                                 EVAL_IDS, top_k = 10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ja_日本 :\n",
      ">>> ['ja_\\xe6\\x97\\xa5\\xe6\\x9c\\xac', 'ja_\\xe5\\xb9\\xb4', 'ja_\\xe7\\x9a\\x84', 'ja_\\xe9\\x99\\xa2', 'ja_\\xe5\\xb0\\xb9', 'ja_\\xe6\\x94\\xbe\\xe9\\x80\\x81', 'ja_3', 'ja_\\xe6\\xb0\\xb4\\xe6\\xb3\\xb3', 'ja_\\xe6\\xb3\\x95\\xe6\\x94\\xbf\\xe5\\xa4\\xa7\\xe5\\xad\\xa6', 'ja_\\xe7\\xa0\\x94\\xe7\\xa9\\xb6']\n",
      ">>> ['en_pupil', 'en_parody', 'en_line', 'en_facebook', 'en_melting', 'en_striker', 'en_decrease', 'en_shots', 'en_everyday', 'en_brook']\n"
     ]
    }
   ],
   "source": [
    "# visual check\n",
    "for wrd_id in TEST_WORDS:\n",
    "    try:\n",
    "        idx = EVAL_IDS.index(wrd_id)\n",
    "    except:\n",
    "        continue\n",
    "    synon = vocab.to_words(src_nbrs[idx])\n",
    "    trans = vocab.to_words(tgt_nbrs[idx])\n",
    "    print(vocab.to_words([wrd_id])[0],\":\")\n",
    "    print(\">>>\", synon)\n",
    "    print(\">>>\", trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: Closest Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_nn\n",
    "\n",
    "# create model\n",
    "model_3 = BiW2V_nn(bilingual_dict = translations,\n",
    "                   vocab = vocab, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model_3.BuildCoreGraph()\n",
    "model_3.BuildTrainingGraph()\n",
    "model_3.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh data generator\n",
    "DATA_GENERATOR = batch_generator(raw_data, vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(20003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.0283244400024\n",
      "   [en_the] closest:  ja_飽和, ja_徹底的, ja_改良, ja_押し出す, en_comprising, ja_扱う, en_owner, ja_聖堂,\n",
      "   [en_last] closest:  ja_外務省, ja_地帯, en_garbage, ja_民事, en_dc, en_cal, en_evidence, ja_クイズ,\n",
      "   [ja_月] closest:  en_enjoy, en_box, ja_ピース, en_canal, ja_テキスト, en_olive, ja_上手い, en_instruction,\n",
      "   [ja_日本] closest:  ja_恐れ, en_invalid, en_nobility, ja_クライマックス, en_evaluate, ja_派手, en_march, ja_全体,\n",
      "... STEP 500 : Average Loss : 6.28931427908\n",
      "... STEP 1000 : Average Loss : 5.72066562939\n",
      "   [en_the] closest:  ja_改良, ja_徹底的, ja_気圧, ja_飽和, ja_聖堂, ja_扱う, ja_革命, en_comprising,\n",
      "   [en_last] closest:  ja_外務省, ja_地帯, en_garbage, ja_民事, en_dc, en_cal, en_evidence, ja_クイズ,\n",
      "   [ja_月] closest:  en_box, en_enjoy, en_canal, ja_ピース, ja_テキスト, ja_上手い, en_olive, en_son-in-law,\n",
      "   [ja_日本] closest:  ja_恐れ, en_invalid, en_nobility, ja_クライマックス, en_evaluate, ja_派手, en_march, ja_有数,\n",
      "... STEP 1500 : Average Loss : 5.54261206055\n",
      "... STEP 2000 : Average Loss : 5.39919434357\n",
      "   [en_the] closest:  ja_改良, ja_気圧, ja_徹底的, ja_飽和, ja_聖堂, ja_扱う, ja_革命, <unk>,\n",
      "   [en_last] closest:  ja_外務省, ja_地帯, en_garbage, ja_民事, en_dc, en_cal, en_evidence, ja_クイズ,\n",
      "   [ja_月] closest:  en_enjoy, en_box, en_canal, ja_ピース, ja_テキスト, ja_上手い, en_olive, en_instruction,\n",
      "   [ja_日本] closest:  ja_恐れ, en_invalid, en_nobility, ja_クライマックス, en_evaluate, ja_派手, en_march, ja_全体,\n",
      "... STEP 2500 : Average Loss : 5.39709190226\n",
      "... STEP 3000 : Average Loss : 5.26819474888\n",
      "   [en_the] closest:  ja_改良, ja_気圧, ja_飽和, ja_徹底的, ja_聖堂, ja_革命, ja_扱う, en_comprising,\n",
      "   [en_last] closest:  ja_外務省, ja_地帯, en_garbage, ja_民事, en_dc, en_cal, en_evidence, ja_クイズ,\n",
      "   [ja_月] closest:  en_enjoy, en_box, en_canal, ja_ピース, ja_テキスト, ja_上手い, en_olive, en_instruction,\n",
      "   [ja_日本] closest:  ja_恐れ, en_invalid, en_nobility, ja_クライマックス, en_evaluate, ja_派手, en_march, ja_全体,\n",
      "... STEP 3500 : Average Loss : 5.25430576944\n",
      "... STEP 4000 : Average Loss : 5.23578563499\n",
      "   [en_the] closest:  ja_改良, ja_飽和, ja_気圧, ja_徹底的, ja_聖堂, ja_革命, en_manifest, ja_扱う,\n",
      "   [en_last] closest:  ja_外務省, ja_地帯, en_garbage, ja_民事, en_dc, en_cal, en_evidence, ja_クイズ,\n",
      "   [ja_月] closest:  en_enjoy, en_box, en_canal, ja_テキスト, ja_ピース, ja_上手い, en_olive, en_instruction,\n",
      "   [ja_日本] closest:  ja_恐れ, en_invalid, ja_クライマックス, en_nobility, en_evaluate, ja_派手, ja_全体, en_march,\n",
      "... STEP 4500 : Average Loss : 5.19276010323\n",
      "... STEP 5000 : Average Loss : 5.17068641138\n",
      "   [en_the] closest:  ja_改良, ja_飽和, ja_徹底的, ja_気圧, ja_聖堂, ja_革命, en_manifest, ja_押し出す,\n",
      "   [en_last] closest:  ja_外務省, ja_地帯, en_garbage, ja_民事, en_dc, en_cal, en_evidence, ja_クイズ,\n",
      "   [ja_月] closest:  en_enjoy, en_box, en_canal, ja_上手い, ja_テキスト, ja_ピース, en_olive, en_instruction,\n",
      "   [ja_日本] closest:  ja_恐れ, en_invalid, ja_クライマックス, en_nobility, en_evaluate, ja_派手, ja_全体, en_march,\n",
      "... Training Complete\n",
      "... 5000 batches trained in 2714.25747204 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "nBATCHES = 5000 # Takes too long w/ nn so we'll only do 5K\n",
    "start = time.time()\n",
    "model_3.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = ALPHA)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context \n",
    "filename = SAVE_TO + '/en_ja_nn_5K_V_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_3.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# word\n",
    "filename = SAVE_TO + '/en_ja_nn_5K_U_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_3.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved embeddings\n",
    "with open(SAVE_TO + '/en_ja_nn_5K_V_dec19.pkl','rb') as f:\n",
    "    C_embedding3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... C shape: (20003, 200)\n",
      "... eval IDs should be > 10003: [11818, 10216, 12063, 19195, 16293]\n",
      "... number to eval: 4425\n",
      "... ground truth source language: ja\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "print('... C shape:', C_embedding3.shape)\n",
    "print('... eval IDs should be > 10003:', EVAL_IDS[:5])\n",
    "print('... number to eval:', len(EVAL_IDS))\n",
    "print('... ground truth source language:', GTT_DF.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bilingual Induction Task__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import evaluateBLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Evaluating 4425 'ja' Ground Truth Translations\n",
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V validation.\n",
      "... finding neighbors...\n",
      "... Done. Total successful translation rate: 0 (0 / 4425)\n"
     ]
    }
   ],
   "source": [
    "src_nbrs, tgt_nbrs = evaluateBLI(C_embedding3, vocab, GTT_DF, \n",
    "                                 EVAL_IDS, top_k = 10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ja_日本 :\n",
      ">>> ['ja_\\xe6\\x97\\xa5\\xe6\\x9c\\xac', 'ja_\\xe6\\x81\\x90\\xe3\\x82\\x8c', 'ja_\\xe3\\x82\\xaf\\xe3\\x83\\xa9\\xe3\\x82\\xa4\\xe3\\x83\\x9e\\xe3\\x83\\x83\\xe3\\x82\\xaf\\xe3\\x82\\xb9', 'ja_\\xe6\\xb4\\xbe\\xe6\\x89\\x8b', 'ja_\\xe5\\x85\\xa8\\xe4\\xbd\\x93', 'ja_\\xe6\\x9c\\x89\\xe6\\x95\\xb0', 'ja_\\xe3\\x81\\xbe\\xe3\\x81\\xa3\\xe3\\x81\\x9f\\xe3\\x81\\x8f', 'ja_\\xe5\\xba\\x97\\xe8\\x88\\x97', 'ja_\\xe6\\x9d\\xb1\\xe6\\x98\\xa0', 'ja_\\xe5\\x85\\xb1\\xe9\\xb3\\xb4']\n",
      ">>> ['en_invalid', 'en_nobility', 'en_evaluate', 'en_march', 'en_iceland', 'en_yearly', 'en_minimum', 'en_sea', 'en_cooler', 'en_later']\n"
     ]
    }
   ],
   "source": [
    "# visual check\n",
    "for wrd_id in TEST_WORDS:\n",
    "    try:\n",
    "        idx = EVAL_IDS.index(wrd_id)\n",
    "    except:\n",
    "        continue\n",
    "    synon = vocab.to_words(src_nbrs[idx])\n",
    "    trans = vocab.to_words(tgt_nbrs[idx])\n",
    "    print(vocab.to_words([wrd_id])[0],\":\")\n",
    "    print(\">>>\", synon)\n",
    "    print(\">>>\", trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
