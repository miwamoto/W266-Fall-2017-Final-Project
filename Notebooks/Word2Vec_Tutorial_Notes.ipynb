{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Tutorial Notes & Modifications <a id=top> </a>\n",
    "`MMV | 12/4 | w266 Final Project: Crosslingual Word Embeddings`   \n",
    "\n",
    "\n",
    "The code in this notebook follows [this tutorial](http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/)  which is based on the [TensorFlow tutorial code](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py). I will first attempt to the basic Word2Vec algorithm to a sample of our data (Wikipedia dumps in English). Then I'll examine different ways of visualizing the embeddings that result. Finally I will explore what it might look like to make [Duong et al's modifications](https://arxiv.org/pdf/1606.09403.pdf) to train crosslingual embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Overview \n",
    "\n",
    "__Basic Idea__: start with 1-hot vector, pass it through a linear activation layer then into a softmax and optimize for the probability of nearby words(Skipgram) or the centerword(CBOW). The 'embeddings' are the parameters of the linear activation (which transform the vector of size $|V|$ into an embedding of size $N$:\n",
    "$$\\text{Weight Matrix:}\\qquad W \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "$$\\text{Bias (?):}\\qquad b \\in \\mathbb{R}^{n}$$\n",
    "\n",
    "__Key Modifications:__ \n",
    "* Duong et all use a CBOW style algorithm but substitute a word's translation at training time so that they learn embeddings for the target language word based on the source language context. (see section 4.1)\n",
    "* As a result, instead of a single weight matrix, they use a concatenation of two (see section 4 intro):\n",
    "$$\\text{Context Matrix:}\\qquad W \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "$$\\text{Embedding Matrix:}\\qquad U \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "* Since normalizing Softmax is costly, they instead optimize for a _log-pseudo likelihood_ by learning to differentiate data from negative examples selected from a noise distribution (following Mikolov 2013, see section 3) (Note that the TF tutorial models how to do this 'noise contrastive estimation')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# custom imports - see APPENDIX \n",
    "import helperfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filepaths\n",
    "BASE = '/home/mmillervedam/Data'\n",
    "FPATH_EN = BASE + '/test/wiki_en_10K.txt' # first 10000 lines from wiki dump\n",
    "FPATH_ES = BASE + '/test/wiki_es_10K.txt' # first 10000 lines from wiki dump\n",
    "DPATH = '/home/mmillervedam/ProjectRepo/XlingualEmb/data/dicts/en.es.panlex.all.processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load & Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer preserves order (see code in Appendix)\n",
    "en_raw = helperfunc.read_data(FPATH_EN)\n",
    "es_raw = helperfunc.read_data(FPATH_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[[12]]', 'Anarchism', 'is', 'often', 'defined', 'as', 'a', 'political', 'philosophy', 'which']\n",
      "['[[7]]', 'El', 'Principado', 'de', 'Andorra', '(', 'en', 'catal\\xc3\\xa1n', ':', 'Principat']\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "print(en_raw[:10])\n",
    "print(es_raw[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE!`__ We'll need to prepend 'en' and 'es' before training crosslingual versions.   \n",
    "__`QUESTIONS:`__ Do we deal with special characters?, punctuation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Builder indexes by count (see code in Appendix)\n",
    "en_data, en_counts, en_dict, en_index = helperfunc.build_dataset(en_raw, VOCAB_SIZE)\n",
    "es_data, es_counts, es_dict, es_index = helperfunc.build_dataset(es_raw, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH:\n",
      "Most common words (+UNK):\n",
      " [['UNK', 35112], ('the', 14841), (',', 14135), ('.', 9672), ('of', 8627)]\n",
      "Sample data:\n",
      " UNK(0) Anarchism(1959) is(9) often(92) defined(571) as(11) a(8) political(226) philosophy(301) which(26)\n"
     ]
    }
   ],
   "source": [
    "#del en_raw  # Uncomment to reduce memory.\n",
    "print(\"ENGLISH:\")\n",
    "print('Most common words (+UNK):\\n', en_counts[:5])\n",
    "print('Sample data:\\n',' '.join(['%s(%s)'%(en_index[i],i) for i in en_data[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPANISH:\n",
      "Most common words (+UNK)\n",
      " [['UNK', 40501], ('de', 16422), (',', 14864), ('la', 9002), ('.', 8578)]\n",
      "Sample data\n",
      ": UNK(0) El(27) Principado(1076) de(1) Andorra(160) ((14) en(6) catalÃ¡n(1381) :(32) UNK(0)\n"
     ]
    }
   ],
   "source": [
    "# del es_raw  # Uncomment to reduce memory.\n",
    "print(\"SPANISH:\")\n",
    "print('Most common words (+UNK)\\n', es_counts[:5])\n",
    "print('Sample data\\n:',' '.join(['%s(%s)'%(es_index[i],i) for i in es_data[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Batched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### PARAMETERS ####################\n",
    "batch_size = 8 # Number of inputs to process at once.\n",
    "num_skips = 2 # How many times to reuse an input to generate a context.\n",
    "skip_window = 2 # How many words to consider left and right.\n",
    "data_index = 0  # -see note below-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ The TF tutorial sets data_index as global inside the generate_batch function. Double check you're getting the expected behavior below b/c we're doubling up on languages. \n",
    "> `UPDATE`: OK - it looks like this is because the 'generate batch' function is used dynamically to window over the data. I'll figure out how to handle the global indexer when I get to the tensorflow portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## ENGLISH BATCHES & CONTEXT #################\n",
    "# batch = list of text segmetns represented by their indices\n",
    "# contexts = corresponding skip_gram context set indices\n",
    "en_batch, en_context = helperfunc.generate_batch(en_data, \n",
    "                                                 batch_size, \n",
    "                                                 num_skips, \n",
    "                                                 skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW BATCH: [  9   9  92  92 571 571  11  11]\n",
      "RAW CONTEXT: [   0 1959    9  571    8    9   92    8]\n",
      "Decoded:\n",
      "    9 is -> 0 UNK\n",
      "    9 is -> 1959 Anarchism\n",
      "    92 often -> 9 is\n",
      "    92 often -> 571 defined\n",
      "    571 defined -> 8 a\n",
      "    571 defined -> 9 is\n",
      "    11 as -> 92 often\n",
      "    11 as -> 8 a\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "print('RAW BATCH:', en_batch)\n",
    "print('RAW CONTEXT:', en_context.squeeze())\n",
    "print(\"Decoded:\")\n",
    "for i in range(8):\n",
    "    print(\"   \", en_batch[i], en_index[en_batch[i]],\n",
    "        '->', en_context[i, 0], en_index[en_context[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## SPANISH BATCHES & CONTEXT #################\n",
    "# batch = list of text segmetns represented by their indices\n",
    "# contexts = corresponding skip_gram context set indices\n",
    "es_batch, es_context = helperfunc.generate_batch(es_data, \n",
    "                                                 batch_size, \n",
    "                                                 num_skips, \n",
    "                                                 skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW BATCH: [   6    6 1381 1381   32   32    0    0]\n",
      "RAW CONTEXT: [  32  160    6    0    6 1381 1381   31]\n",
      "Decoded:\n",
      "    6 los -> 32 :\n",
      "    6 los -> 160 Andorra\n",
      "    1381 durante -> 6 en\n",
      "    1381 durante -> 0 UNK\n",
      "    32 material -> 6 en\n",
      "    32 material -> 1381 catalÃ¡n\n",
      "    0 a -> 1381 catalÃ¡n\n",
      "    0 a -> 31 )\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "print('RAW BATCH:', es_batch)\n",
    "print('RAW CONTEXT:', es_context.squeeze())\n",
    "print(\"Decoded:\")\n",
    "for i in range(8):\n",
    "    print(\"   \", es_batch[i], es_index[en_batch[i]],\n",
    "        '->', es_context[i, 0], es_index[es_context[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ To implment Duong et Al's work we'd perform the word substitution at this stage, replacing the words in the batch with the index of their translation... In fact we'd probably do so using a dictionary of indices for the vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix & Supplemental Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helperfunc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helperfunc.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Helper Functions for implementing Word2Vec in Python.\n",
    "\n",
    "Most of the functions in this file come from the Official \n",
    "Tensorflow Docs and are made available via the word2vec\n",
    "tutorial at: https://github.com/tensorflow/tensorflow/blob\n",
    "/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n",
    "As such, this code is protected by their liscence, see ^^.\n",
    "       \n",
    "As noted, some of these helper fuctions were written or modified\n",
    "by the authors of adventuresinmachinelearning.com as part of their\n",
    "word-2-vec tutorial which closely follows the Tensorflow code.\n",
    "\n",
    "I have also modified some to suit our use case.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Extract the file as a list of words.\n",
    "    NOTE: this is modified from original function in TF  \n",
    "    tutorialwhich expected a zipped input file.\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        data = tf.compat.as_str(f.read()).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"\n",
    "    Process raw inputs into a dataset.\n",
    "    Creates vocabulary from top n words indexed by rank.\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data_index = 0\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    \"\"\"\n",
    "    Function to generate a training batch for the skip-gram model.\n",
    "    NOTE: this wass modified from original function in TF  \n",
    "    tutorial by adventuresinML tutorial - mostly just renamed.\n",
    "    \"\"\"\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
