{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Tutorial Notes & Modifications <a id=top> </a>\n",
    "`MMV | 12/4 | w266 Final Project: Crosslingual Word Embeddings`   \n",
    "\n",
    "\n",
    "The code in this notebook follows [this tutorial](http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/)  which is based on the [TensorFlow tutorial code](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py). I will first attempt to the basic Word2Vec algorithm to a sample of our data (Wikipedia dumps in English). Then I'll examine different ways of visualizing the embeddings that result. Finally I will explore what it might look like to make [Duong et al's modifications](https://arxiv.org/pdf/1606.09403.pdf) to train crosslingual embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Overview \n",
    "\n",
    "__Basic Idea__: start with 1-hot vector, pass it through a linear activation layer then into a softmax and optimize for the probability of nearby words(Skipgram) or the centerword(CBOW). The 'embeddings' are the parameters of the linear activation (which transform the vector of size $|V|$ into an embedding of size $N$:\n",
    "$$\\text{Weight Matrix:}\\qquad W \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "$$\\text{Bias (?):}\\qquad b \\in \\mathbb{R}^{n}$$\n",
    "\n",
    "__Key Modifications:__ \n",
    "* Duong et all use a CBOW style algorithm but substitute a word's translation at training time so that they learn embeddings for the target language word based on the source language context. (see section 4.1)\n",
    "* As a result, instead of a single weight matrix, they use a concatenation of two (see section 4 intro):\n",
    "$$\\text{Context Matrix:}\\qquad W \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "$$\\text{Embedding Matrix:}\\qquad U \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "* Since normalizing Softmax is costly, they instead optimize for a _log-pseudo likelihood_ by learning to differentiate data from negative examples selected from a noise distribution (following Mikolov 2013, see section 3) (Note that the TF tutorial models how to do this 'noise contrastive estimation')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import collections\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# custom imports - see APPENDIX \n",
    "import helperfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filepaths\n",
    "BASE = '/home/mmillervedam/Data'\n",
    "FPATH_EN = BASE + '/test/wiki_en_10K.txt' # first 10000 lines from wiki dump\n",
    "FPATH_ES = BASE + '/test/wiki_es_10K.txt' # first 10000 lines from wiki dump\n",
    "DPATH = '/home/mmillervedam/ProjectRepo/XlingualEmb/data/dicts/en.es.panlex.all.processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load & Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenizer preserves order (see code in Appendix)\n",
    "en_raw = helperfunc.read_data(FPATH_EN)\n",
    "es_raw = helperfunc.read_data(FPATH_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[[12]]', 'Anarchism', 'is', 'often', 'defined', 'as', 'a', 'political', 'philosophy', 'which']\n",
      "['[[7]]', 'El', 'Principado', 'de', 'Andorra', '(', 'en', 'catal\\xc3\\xa1n', ':', 'Principat']\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "print(en_raw[:10])\n",
    "print(es_raw[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE!`__ We'll need to prepend 'en' and 'es' before training crosslingual versions.   \n",
    "__`QUESTIONS:`__ Do we deal with special characters?, punctuation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset Builder indexes by count (see code in Appendix)\n",
    "en_data, en_counts, en_dict, en_index = helperfunc.build_dataset(en_raw, VOCAB_SIZE)\n",
    "es_data, es_counts, es_dict, es_index = helperfunc.build_dataset(es_raw, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH:\n",
      "Most common words (+UNK):\n",
      " [['UNK', 35112], ('the', 14841), (',', 14135), ('.', 9672), ('of', 8627)]\n",
      "Sample data:\n",
      " UNK(0) Anarchism(1959) is(9) often(92) defined(571) as(11) a(8) political(226) philosophy(301) which(26)\n"
     ]
    }
   ],
   "source": [
    "#del en_raw  # Uncomment to reduce memory.\n",
    "print(\"ENGLISH:\")\n",
    "print('Most common words (+UNK):\\n', en_counts[:5])\n",
    "print('Sample data:\\n',' '.join(['%s(%s)'%(en_index[i],i) for i in en_data[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPANISH:\n",
      "Most common words (+UNK)\n",
      " [['UNK', 40501], ('de', 16422), (',', 14864), ('la', 9002), ('.', 8578)]\n",
      "Sample data\n",
      ": UNK(0) El(27) Principado(1076) de(1) Andorra(160) ((14) en(6) catalÃ¡n(1381) :(32) UNK(0)\n"
     ]
    }
   ],
   "source": [
    "# del es_raw  # Uncomment to reduce memory.\n",
    "print(\"SPANISH:\")\n",
    "print('Most common words (+UNK)\\n', es_counts[:5])\n",
    "print('Sample data\\n:',' '.join(['%s(%s)'%(es_index[i],i) for i in es_data[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Batched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### PARAMETERS ####################\n",
    "batch_size = 8 # Number of inputs to process at once.\n",
    "num_skips = 2 # How many times to reuse an input to generate a context.\n",
    "skip_window = 2 # How many words to consider left and right.\n",
    "data_index = 0  # -see note below-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ The TF tutorial sets data_index as global inside the generate_batch function. Double check you're getting the expected behavior below b/c we're doubling up on languages. \n",
    "> `UPDATE`: OK - it looks like this is because the 'generate batch' function is used dynamically to window over the data. I'll figure out how to handle the global indexer when I get to the tensorflow portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## ENGLISH BATCHES & CONTEXT #################\n",
    "# batch = list of text segmetns represented by their indices\n",
    "# contexts = corresponding skip_gram context set indices\n",
    "en_batch, en_context = helperfunc.generate_batch(en_data, \n",
    "                                                 batch_size, \n",
    "                                                 num_skips, \n",
    "                                                 skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW BATCH: [  9   9  92  92 571 571  11  11]\n",
      "RAW CONTEXT: [   0 1959    9  571    8    9   92    8]\n",
      "Decoded:\n",
      "    9 is -> 0 UNK\n",
      "    9 is -> 1959 Anarchism\n",
      "    92 often -> 9 is\n",
      "    92 often -> 571 defined\n",
      "    571 defined -> 8 a\n",
      "    571 defined -> 9 is\n",
      "    11 as -> 92 often\n",
      "    11 as -> 8 a\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "print('RAW BATCH:', en_batch)\n",
    "print('RAW CONTEXT:', en_context.squeeze())\n",
    "print(\"Decoded:\")\n",
    "for i in range(8):\n",
    "    print(\"   \", en_batch[i], en_index[en_batch[i]],\n",
    "        '->', en_context[i, 0], en_index[en_context[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## SPANISH BATCHES & CONTEXT #################\n",
    "# batch = list of text segmetns represented by their indices\n",
    "# contexts = corresponding skip_gram context set indices\n",
    "es_batch, es_context = helperfunc.generate_batch(es_data, \n",
    "                                                 batch_size, \n",
    "                                                 num_skips, \n",
    "                                                 skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW BATCH: [   6    6 1381 1381   32   32    0    0]\n",
      "RAW CONTEXT: [ 32 160  14   6   0   6  32  31]\n",
      "Decoded:\n",
      "    6 los -> 32 :\n",
      "    6 los -> 160 Andorra\n",
      "    1381 durante -> 14 (\n",
      "    1381 durante -> 6 en\n",
      "    32 material -> 0 UNK\n",
      "    32 material -> 6 en\n",
      "    0 a -> 32 :\n",
      "    0 a -> 31 )\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "print('RAW BATCH:', es_batch)\n",
    "print('RAW CONTEXT:', es_context.squeeze())\n",
    "print(\"Decoded:\")\n",
    "for i in range(8):\n",
    "    print(\"   \", es_batch[i], es_index[en_batch[i]],\n",
    "        '->', es_context[i, 0], es_index[es_context[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ To implment Duong et Al's work we'd perform the word substitution at this stage, replacing the words in the batch with the index of their translation... In fact we'd probably do so using a dictionary of indices for the vocab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TensorFlow Model w/ full softmax (slow!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ Set up the model graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "# recall that we set the vocabulary size at the top of the NB\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# additional model parameters\n",
    "batch_size = 128 # Number of inputs to process at once.\n",
    "embedding_size = 128 # Hidden layer representation size\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the TF graph\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### DATA PLACEHOLDERS ####################\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    train_one_hot = tf.one_hot(train_context, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### INPUT(EMBEDDING)LAYER #################\n",
    "with graph.as_default():\n",
    "    embeddings = tf.Variable(tf.random_uniform([VOCAB_SIZE, \n",
    "                                                embedding_size],\n",
    "                                               -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## HIDDEN LAYER ######################\n",
    "with graph.as_default():\n",
    "    weights = tf.Variable(tf.truncated_normal([VOCAB_SIZE, embedding_size],\n",
    "                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    biases = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "    hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ If we're going to se up experiments/comparisons between different embedding training methods (eg. Duongs word2vec modification vs the post training aligned word vectors referenced in the Babylon Repo)... we'll want to fix the embedding size across the multiple models. Maybe even fix the initialization for the weights?-- no in this case the weights are irrelevant across models b/c they'll be optimizing different things. Presumably part of what we're interested in is comparisons of speed to train in concert w/ efficacy on the translation task and random initialization always begs the question of 'did we just get lucky'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################## TRAIN OP ########################\n",
    "with graph.as_default():\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
    "                                                labels=train_one_hot))\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Set up validation set - arandomly chosen set of words to use to track our progress as we train. By construction we'll pick words from the 100 most frequent in the vocabulary then use cosine similarity to find the nearest neighbors in the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### VALIDATION EXAMPLES #################\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "with graph.as_default():\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### SIMILARITY CALCULATION ################\n",
    "with graph.as_default():\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable initializer\n",
    "with graph.as_default():\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3:__ Run the model & track progress by examining the matches for words in our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperfunc import generate_batch\n",
    "data_index = 0 # used to track batches\n",
    "\n",
    "def run(graph, num_steps):\n",
    "    \"\"\"Runner code for word2vec TF model w/ full softmax\"\"\"\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_context = generate_batch(en_data,\n",
    "                                                         batch_size, \n",
    "                                                         num_skips, \n",
    "                                                         skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs, \n",
    "                         train_context: batch_context}\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op \n",
    "            _, loss_val = session.run([optimizer, cross_entropy], \n",
    "                                      feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 100\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 500 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = en_index[valid_examples[i]]\n",
    "                    top_k = 8  # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = en_index[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                    print(log_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runner Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 10001\n",
    "softmax_start_time = dt.datetime.now()\n",
    "run(graph, num_steps=num_steps)\n",
    "softmax_end_time = dt.datetime.now()\n",
    "print(\"Softmax method took {} seconds to run 10000 iterations\".format((softmax_end_time-softmax_start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax method took 1461.520475 seconds to run 10000 iterations"
     ]
    }
   ],
   "source": [
    "# NOTE: output from ^^ saved to:\n",
    "path = 'wtv_output/en_smalldata_10Kiter_fullsfmx.txt'\n",
    "!tail -n 1 {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  9100 :  5.23669617653\r\n",
      "Average loss at step  9200 :  5.23754267216\r\n",
      "Average loss at step  9300 :  5.30736485481\r\n",
      "Average loss at step  9400 :  5.27600327492\r\n",
      "Average loss at step  9500 :  5.2527682209\r\n",
      "Average loss at step  9600 :  5.25820608616\r\n",
      "Average loss at step  9700 :  5.34852351189\r\n",
      "Average loss at step  9800 :  5.43696550369\r\n",
      "Average loss at step  9900 :  5.37404325485\r\n",
      "Average loss at step  10000 :  5.33349477768\r\n"
     ]
    }
   ],
   "source": [
    "# take a look at loss\n",
    "!grep 'Average' {path} | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Nixon, remove,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, remove, Nixon,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Nixon, shadow,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, shadow, remove,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, shadow, remove,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Alexander, shadow,\r\n",
      "Nearest to them: chose, lines, origin, Delos, lighter, young, Alexander, shadow,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Alexander, shadow,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Alexander, shadow,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Alexander, shadow,\r\n"
     ]
    }
   ],
   "source": [
    "# take a look at NN for 'the'\n",
    "!grep 'Nearest to them:' {path} |tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ The data ^^ are undoubtedly too small... 'Alabama' shouldn't appear in the top 100 words. However I'll wait to look at larger data with the sampling method which is much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Model w/ NCE (faster)\n",
    "\n",
    "We'll write it as a class this time for ease of calling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def with_self_graph(function):\n",
    "    \"\"\"Decorator-foo borrowed from w266 a4.\"\"\"\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        with self.graph.as_default():\n",
    "            return function(self, *args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "class Word2Vec(object):\n",
    "    \"\"\"Single Layer Neural Net to Learn Word Embeddings.\"\"\"\n",
    "    # This code was adapted from:\n",
    "    # SOURCE: https://github.com/tensorflow/tensorflow\n",
    "    #         /blob/r1.2/tensorflow/examples/tutorials\n",
    "    #         /word2vec/word2vec_basic.py\n",
    "    \n",
    "    def __init__(self, graph=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize TensorFlow Neural Net Model.\n",
    "        Args:\n",
    "          V: vocabulary size\n",
    "          H: embedding size\n",
    "          \n",
    "        Kwargs:\n",
    "          softmax_ns = 64  (number of negative samples)\n",
    "          alpha = 1.0  (learning rate)\n",
    "          examples = np.array of 5 top 100 words for validation\n",
    "        \"\"\"\n",
    "        # Set TensorFlow graph. All TF code will work on this graph.\n",
    "        self.graph = graph or tf.Graph()\n",
    "        self.SetParams(*args, **kwargs)\n",
    "        \n",
    "    @with_self_graph # TODO : remove this unless we plan to init as tf.const\n",
    "    def SetParams(self, V, H, softmax_ns=64, learning_rate=1.0):\n",
    "        # Model structure.\n",
    "        self.V = V\n",
    "        self.H = H\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        self.softmax_ns = softmax_ns\n",
    "        self.alpha = 1.0\n",
    "        \n",
    "        # Words for validation\n",
    "        self.examples = np.random.choice(100, 5, replace=False)\n",
    "        \n",
    "        # Results\n",
    "        self.epochs_trained = 0\n",
    "        self.final_embeddings = None\n",
    "            \n",
    "    @with_self_graph\n",
    "    def BuildCoreGraph(self):\n",
    "        \n",
    "        batch_size = 128 # TODO : I've hard coded this for now b/c I want to get\n",
    "                         # the rest of the code running, but eventually this should\n",
    "                         # be inferred dynamically from the input shape as in a4.\n",
    "        \n",
    "        # Data Placeholders\n",
    "        self.inputs_ = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        self.context_ = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        \n",
    "        # Embedding Layer\n",
    "        with tf.variable_scope(\"Embedding_Layer\"):\n",
    "            self.embeddings_ = tf.Variable(tf.random_uniform([self.V, self.H], -1.0, 1.0))\n",
    "            self.embed_ = tf.nn.embedding_lookup(self.embeddings_, self.inputs_)\n",
    "            # Normalized Embeddings facillitate cosine similarity calculation\n",
    "            # .... but don't train on these! they're just for evaluation!\n",
    "            self.norm_ = tf.sqrt(tf.reduce_sum(tf.square(self.embeddings_), 1, keep_dims=True))\n",
    "            self.normalized_embeddings_ = self.embeddings_ / self.norm_\n",
    "            \n",
    "        # Hidden Layer\n",
    "        with tf.variable_scope(\"Hidden_Layer\"):\n",
    "            self.W_ = tf.Variable(tf.truncated_normal([self.V, self.H],\n",
    "                                  stddev=1.0 / math.sqrt(self.H)))\n",
    "            self.b_ = tf.Variable(tf.zeros([self.V,], dtype=tf.float32))\n",
    "            self.logits_ = tf.matmul(self.embed_, tf.transpose(self.W_)) + self.b_\n",
    "           \n",
    "    @with_self_graph\n",
    "    def BuildTrainingGraph(self):\n",
    "        with tf.variable_scope(\"Training\"):\n",
    "            nce_args = dict(weights=self.W_, \n",
    "                            biases=self.b_, \n",
    "                            labels=self.context_, \n",
    "                            inputs=self.embed_, \n",
    "                            num_sampled=self.softmax_ns, \n",
    "                            num_classes=self.V)\n",
    "            self.nce_loss_ = tf.reduce_mean(tf.nn.nce_loss(**nce_args))\n",
    "            self.optimizer_ = tf.train.GradientDescentOptimizer(self.alpha)\n",
    "            self.train_step_ = self.optimizer_.minimize(self.nce_loss_)\n",
    "            \n",
    "        \n",
    "    @with_self_graph\n",
    "    def BuildValidationGraph(self):\n",
    "        self.test_ = tf.constant(self.examples, dtype=tf.int32)\n",
    "        self.test_embed_ = tf.nn.embedding_lookup(self.normalized_embeddings_, \n",
    "                                                  self.test_)\n",
    "        self.similarity = tf.matmul(self.test_embed_, \n",
    "                                    self.normalized_embeddings_, \n",
    "                                    transpose_b=True)\n",
    "        \n",
    "    def learn_embeddings(self, num_steps, batch_fxn, data, index, verbose = True):\n",
    "        \"\"\"\n",
    "        Runs a specified number of training steps.\n",
    "        NOTE: right now the batch fxn is hard coded with inputs: \n",
    "                  (data,batch_size=128,num_skips=2,skip_window=2)\n",
    "              It should output two arrays representing the input & \n",
    "              context indices for a single batch. \n",
    "              TODO: replace this with something less clunky!\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as session:\n",
    "            \n",
    "            # initialize all variables\n",
    "            init = tf.global_variables_initializer()\n",
    "            init.run()\n",
    "            print('... Model Initialized')\n",
    "        \n",
    "            # iterate through specificied number of training steps\n",
    "            average_loss = 0\n",
    "            for step in range(num_steps):\n",
    "                # Get the next batch of inputs & their skipgram context\n",
    "                batch_inputs, batch_context = batch_fxn(data, 128, 2, 2)\n",
    "\n",
    "                # Run the train op\n",
    "                feed_dict = {self.inputs_: batch_inputs, self.context_: batch_context}\n",
    "                _, loss_val = session.run([self.train_step_, self.nce_loss_], \n",
    "                                          feed_dict=feed_dict)\n",
    "                \n",
    "                # Logging Progress\n",
    "                average_loss += loss_val\n",
    "                loss_logging_interval = num_steps // 10\n",
    "                sim_logging_interval = num_steps // 5\n",
    "                if not verbose:\n",
    "                    continue\n",
    "                if step % loss_logging_interval == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss /= loss_logging_interval\n",
    "                    # The average loss is an estimate of the loss over the last 1000 batches.\n",
    "                    print('Average loss at step ', step, ': ', average_loss)\n",
    "                    average_loss = 0  \n",
    "                if step % sim_logging_interval == 0:\n",
    "                    sim = self.similarity.eval()\n",
    "                    for i in xrange(len(self.examples)):\n",
    "                        word = index[self.examples[i]]\n",
    "                        top_k = 8  # number of nearest neighbors\n",
    "                        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                        log_str = '   Nearest to %s:' % word\n",
    "                        for k in xrange(top_k):\n",
    "                            nbr = index[nearest[k]]\n",
    "                            log_str = '%s %s,' % (log_str, nbr)\n",
    "                        print(log_str)\n",
    "            # results\n",
    "            self.epochs_trained = num_steps\n",
    "            self.final_embeddings = self.normalized_embeddings_.eval()\n",
    "        return self.final_embeddings\n",
    "    \n",
    "    def plot_embeddings_in_2D(self, num, index, filename):\n",
    "        \"\"\" \n",
    "        Plot 2D representation of embeddings.\n",
    "        Args: \n",
    "            num = int (number of examples to plot)\n",
    "            index = reverse dictionary of word indices\n",
    "            filename = path to save plot\n",
    "        \"\"\"\n",
    "        if self.final_embeddings is None:\n",
    "            print(\"You must train the embeddings before plotting.\")\n",
    "        else:\n",
    "            tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "            low_dim_embs = tsne.fit_transform(self.final_embeddings[:num, :])\n",
    "            labels = [index[i] for i in xrange(num)]\n",
    "            plt.figure(figsize=(18, 18))  # in inches\n",
    "            for i, label in enumerate(labels):\n",
    "                x, y = low_dim_embs[i, :]\n",
    "                plt.scatter(x, y)\n",
    "                plt.annotate(label, xy=(x, y), xytext=(5, 2), \n",
    "                             textcoords='offset points', ha='right', va='bottom')\n",
    "            plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 0:__ Data prep (_we did this above, just running a few checks here_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Size: 259807 Words\n",
      "Vocabulary Size: 5000 Words\n"
     ]
    }
   ],
   "source": [
    "# We'll be using the shortened version of the English Wikipedia File\n",
    "print('Corpus Size: %s Words' % (len(en_data)))\n",
    "print('Vocabulary Size: %s Words' % (len(en_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Parameters for batch function\n",
    "BATCH_SIZE = 128 # Number of inputs to process at once.\n",
    "EMBEDDING_SIZE = 128 # Hidden layer representation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: the following are hard coded into the class above, just here for reference \n",
    "# TODO make a better batch iterator & a data handler so we don't have to do this!\n",
    "skip_window = 2 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "from helperfunc import generate_batch # TODO : I want a batch iterator instead of this fxn!\n",
    "data_index = 0 # used to track batches, ugh, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ Create Model & Initialize TF Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(V=VOCAB_SIZE, H=EMBEDDING_SIZE)\n",
    "model.BuildCoreGraph()\n",
    "model.BuildTrainingGraph()\n",
    "model.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __`Question for Mona & Roseanna:`__ When does it makes sense to keep these graph building methods separate and when should they all be part of the same class method? In this case we're never going to run 'inference' except in the context of the test/validation exercise... because we don't really care about the ultimate prediction of context words we really care about the embeddings. I've left these as 3 methods for now (following the lead of A4) but I wonder if we could combine two or all of them when we create our Xlingual version of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "Average loss at step  0 :  211.023757935\n",
      "   Nearest to -: edition, vehicles, formal, AD, major, higher, prove, 67,\n",
      "   Nearest to was: exchange, special, runs, Through, Aramaic, houses, attempt, Education,\n",
      "   Nearest to with: acidic, pharmaceutical, energies, communist, null, Arthur, creation, nonfiction,\n",
      "   Nearest to known: tragedy, operated, senses, announced, around, offered, flying, mechanization,\n",
      "   Nearest to is: scene, between, beyond, largely, few, Orchestra, false, Godwin,\n",
      "Average loss at step  5000 :  12.2470937237\n",
      "Average loss at step  10000 :  4.72273453636\n",
      "   Nearest to -: UNK, edition, AD, vehicles, apple, formal, major, small,\n",
      "   Nearest to was: is, special, exchange, attempt, Education, runs, Doric, quality,\n",
      "   Nearest to with: and, communist, nonfiction, Module, acidic, apple, Urgell, local,\n",
      "   Nearest to known: tragedy, operated, senses, announced, space, around, landing, scholarship,\n",
      "   Nearest to is: was, itself, governor, UNK, emotion, false, problems, apple,\n",
      "Average loss at step  15000 :  4.65328040752\n",
      "Average loss at step  20000 :  4.60469445424\n",
      "   Nearest to -: edition, AD, vehicles, UNK, major, apple, formal, small,\n",
      "   Nearest to was: is, special, Australian, attempt, exchange, Doric, successful, Anthropologists,\n",
      "   Nearest to with: nonfiction, communist, and, pharmaceutical, local, acidic, energies, creation,\n",
      "   Nearest to known: senses, operated, tragedy, announced, space, landing, offered, around,\n",
      "   Nearest to is: was, itself, governor, scene, are, emotion, problems, Andre,\n",
      "Average loss at step  25000 :  4.5726115449\n",
      "Average loss at step  30000 :  4.54218674746\n",
      "   Nearest to -: AD, edition, vehicles, apple, major, Jewish, formal, medicine,\n",
      "   Nearest to was: is, Australian, attempt, special, Doric, successful, mythology, Anthropologists,\n",
      "   Nearest to with: nonfiction, communist, local, and, pharmaceutical, energies, acidic, currently,\n",
      "   Nearest to known: senses, operated, space, landing, tragedy, announced, scholarship, offered,\n",
      "   Nearest to is: was, are, itself, governor, scene, emotion, attitude, calculations,\n",
      "Average loss at step  35000 :  4.52459334459\n",
      "Average loss at step  40000 :  4.49074717803\n",
      "   Nearest to -: UNK, AD, edition, vehicles, degrees, medicine, Jewish, apple,\n",
      "   Nearest to was: is, Australian, notion, attempt, near, traveler, special, Doric,\n",
      "   Nearest to with: pharmaceutical, communist, local, nonfiction, acidic, energies, levels, currently,\n",
      "   Nearest to known: senses, operated, landing, space, offered, scholarship, tragedy, announced,\n",
      "   Nearest to is: was, are, itself, governor, emotion, attitude, Third, scene,\n",
      "Average loss at step  45000 :  4.47909498353\n",
      "NCE method took 50000 seconds to run 91.339289 iterations\n"
     ]
    }
   ],
   "source": [
    "NSTEPS = 50000\n",
    "start_time = dt.datetime.now()\n",
    "embeddings = model.learn_embeddings(NSTEPS, generate_batch, en_data, en_index)\n",
    "end_time = dt.datetime.now()\n",
    "total = (end_time - start_time).total_seconds()\n",
    "print(\"NCE method took {} seconds to run {} iterations\".format(total, NSTEPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Note:`__ This code ^ is logging the sampled softmax loss... (NCE)... not sure if thats terribly instructive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix & Supplemental Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`helperfunc.py`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helperfunc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helperfunc.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Helper Functions for implementing Word2Vec in Python.\n",
    "\n",
    "Most of the functions in this file come from the Official \n",
    "Tensorflow Docs and are made available via the word2vec\n",
    "tutorial at: https://github.com/tensorflow/tensorflow/blob\n",
    "/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n",
    "As such, this code is protected by their liscence, see ^^.\n",
    "       \n",
    "As noted, some of these helper fuctions were written or modified\n",
    "by the authors of adventuresinmachinelearning.com as part of their\n",
    "word-2-vec tutorial which closely follows the Tensorflow code.\n",
    "\n",
    "I have also modified some to suit our use case.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Extract the file as a list of words.\n",
    "    NOTE: this is modified from original function in TF  \n",
    "    tutorialwhich expected a zipped input file.\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        data = tf.compat.as_str(f.read()).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"\n",
    "    Process raw inputs into a dataset.\n",
    "    Creates vocabulary from top n words indexed by rank.\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data_index = 0\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    \"\"\"\n",
    "    Function to generate a training batch for the skip-gram model.\n",
    "    NOTE: this wass modified from original function in TF  \n",
    "    tutorial by adventuresinML tutorial - mostly just renamed.\n",
    "    \"\"\"\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
