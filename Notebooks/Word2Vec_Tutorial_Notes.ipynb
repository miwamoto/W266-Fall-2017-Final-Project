{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Tutorial Notes & Modifications <a id=top> </a>\n",
    "`MMV | 12/4 | w266 Final Project: Crosslingual Word Embeddings`   \n",
    "\n",
    "\n",
    "The code in this notebook follows [this tutorial](http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/)  which is based on the [TensorFlow tutorial code](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py). I will first attempt to the basic Word2Vec algorithm to a sample of our data (Wikipedia dumps in English). Then I'll examine different ways of visualizing the embeddings that result. Finally I will explore what it might look like to make [Duong et al's modifications](https://arxiv.org/pdf/1606.09403.pdf) to train crosslingual embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Overview \n",
    "\n",
    "__Basic Idea__: start with 1-hot vector, pass it through a linear activation layer then into a softmax and optimize for the probability of nearby words(Skipgram) or the centerword(CBOW). The 'embeddings' are the parameters of the linear activation (which transform the vector of size $|V|$ into an embedding of size $N$:\n",
    "$$\\text{Weight Matrix:}\\qquad W \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "$$\\text{Bias (?):}\\qquad b \\in \\mathbb{R}^{n}$$\n",
    "\n",
    "__Key Modifications:__ \n",
    "* Duong et all use a CBOW style algorithm but substitute a word's translation at training time so that they learn embeddings for the target language word based on the source language context. (see section 4.1)\n",
    "* As a result, instead of a single weight matrix, they use a concatenation of two (see section 4 intro):\n",
    "$$\\text{Context Matrix:}\\qquad W \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "$$\\text{Embedding Matrix:}\\qquad U \\in \\mathbb{R}^{|V|\\times N}$$\n",
    "* Since normalizing Softmax is costly, they instead optimize for a _log-pseudo likelihood_ by learning to differentiate data from negative examples selected from a noise distribution (following Mikolov 2013, see section 3) (Note that the TF tutorial models how to do this 'noise contrastive estimation')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# custom imports - see APPENDIX \n",
    "import helperfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filepaths\n",
    "BASE = '/home/mmillervedam/Data'\n",
    "FPATH_EN = BASE + '/test/wiki_en_10K.txt' # first 10000 lines from wiki dump\n",
    "FPATH_ES = BASE + '/test/wiki_es_10K.txt' # first 10000 lines from wiki dump\n",
    "DPATH = '/home/mmillervedam/ProjectRepo/XlingualEmb/data/dicts/en.es.panlex.all.processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load & Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenizer preserves order (see code in Appendix)\n",
    "en_raw = helperfunc.read_data(FPATH_EN)\n",
    "es_raw = helperfunc.read_data(FPATH_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[[12]]', 'Anarchism', 'is', 'often', 'defined', 'as', 'a', 'political', 'philosophy', 'which']\n",
      "['[[7]]', 'El', 'Principado', 'de', 'Andorra', '(', 'en', 'catal\\xc3\\xa1n', ':', 'Principat']\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "print(en_raw[:10])\n",
    "print(es_raw[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE!`__ We'll need to prepend 'en' and 'es' before training crosslingual versions.   \n",
    "__`QUESTIONS:`__ Do we deal with special characters?, punctuation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset Builder indexes by count (see code in Appendix)\n",
    "en_data, en_counts, en_dict, en_index = helperfunc.build_dataset(en_raw, VOCAB_SIZE)\n",
    "es_data, es_counts, es_dict, es_index = helperfunc.build_dataset(es_raw, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH:\n",
      "Most common words (+UNK):\n",
      " [['UNK', 35112], ('the', 14841), (',', 14135), ('.', 9672), ('of', 8627)]\n",
      "Sample data:\n",
      " UNK(0) Anarchism(1959) is(9) often(92) defined(571) as(11) a(8) political(226) philosophy(301) which(26)\n"
     ]
    }
   ],
   "source": [
    "#del en_raw  # Uncomment to reduce memory.\n",
    "print(\"ENGLISH:\")\n",
    "print('Most common words (+UNK):\\n', en_counts[:5])\n",
    "print('Sample data:\\n',' '.join(['%s(%s)'%(en_index[i],i) for i in en_data[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPANISH:\n",
      "Most common words (+UNK)\n",
      " [['UNK', 40501], ('de', 16422), (',', 14864), ('la', 9002), ('.', 8578)]\n",
      "Sample data\n",
      ": UNK(0) El(27) Principado(1076) de(1) Andorra(160) ((14) en(6) catalÃ¡n(1381) :(32) UNK(0)\n"
     ]
    }
   ],
   "source": [
    "# del es_raw  # Uncomment to reduce memory.\n",
    "print(\"SPANISH:\")\n",
    "print('Most common words (+UNK)\\n', es_counts[:5])\n",
    "print('Sample data\\n:',' '.join(['%s(%s)'%(es_index[i],i) for i in es_data[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Batched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### PARAMETERS ####################\n",
    "batch_size = 8 # Number of inputs to process at once.\n",
    "num_skips = 2 # How many times to reuse an input to generate a context.\n",
    "skip_window = 2 # How many words to consider left and right.\n",
    "data_index = 0  # -see note below-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ The TF tutorial sets data_index as global inside the generate_batch function. Double check you're getting the expected behavior below b/c we're doubling up on languages. \n",
    "> `UPDATE`: OK - it looks like this is because the 'generate batch' function is used dynamically to window over the data. I'll figure out how to handle the global indexer when I get to the tensorflow portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## ENGLISH BATCHES & CONTEXT #################\n",
    "# batch = list of text segmetns represented by their indices\n",
    "# contexts = corresponding skip_gram context set indices\n",
    "en_batch, en_context = helperfunc.generate_batch(en_data, \n",
    "                                                 batch_size, \n",
    "                                                 num_skips, \n",
    "                                                 skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW BATCH: [  9   9  92  92 571 571  11  11]\n",
      "RAW CONTEXT: [   0 1959    9  571    8    9   92    8]\n",
      "Decoded:\n",
      "    9 is -> 0 UNK\n",
      "    9 is -> 1959 Anarchism\n",
      "    92 often -> 9 is\n",
      "    92 often -> 571 defined\n",
      "    571 defined -> 8 a\n",
      "    571 defined -> 9 is\n",
      "    11 as -> 92 often\n",
      "    11 as -> 8 a\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "print('RAW BATCH:', en_batch)\n",
    "print('RAW CONTEXT:', en_context.squeeze())\n",
    "print(\"Decoded:\")\n",
    "for i in range(8):\n",
    "    print(\"   \", en_batch[i], en_index[en_batch[i]],\n",
    "        '->', en_context[i, 0], en_index[en_context[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## SPANISH BATCHES & CONTEXT #################\n",
    "# batch = list of text segmetns represented by their indices\n",
    "# contexts = corresponding skip_gram context set indices\n",
    "es_batch, es_context = helperfunc.generate_batch(es_data, \n",
    "                                                 batch_size, \n",
    "                                                 num_skips, \n",
    "                                                 skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW BATCH: [   6    6 1381 1381   32   32    0    0]\n",
      "RAW CONTEXT: [ 32 160  14   6   0   6  32  31]\n",
      "Decoded:\n",
      "    6 los -> 32 :\n",
      "    6 los -> 160 Andorra\n",
      "    1381 durante -> 14 (\n",
      "    1381 durante -> 6 en\n",
      "    32 material -> 0 UNK\n",
      "    32 material -> 6 en\n",
      "    0 a -> 32 :\n",
      "    0 a -> 31 )\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "print('RAW BATCH:', es_batch)\n",
    "print('RAW CONTEXT:', es_context.squeeze())\n",
    "print(\"Decoded:\")\n",
    "for i in range(8):\n",
    "    print(\"   \", es_batch[i], es_index[en_batch[i]],\n",
    "        '->', es_context[i, 0], es_index[es_context[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ To implment Duong et Al's work we'd perform the word substitution at this stage, replacing the words in the batch with the index of their translation... In fact we'd probably do so using a dictionary of indices for the vocab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TensorFlow Model w/ full softmax (slow!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ Set up the model graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "# recall that we set the vocabulary size at the top of the NB\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# additional model parameters\n",
    "batch_size = 128 # Number of inputs to process at once.\n",
    "embedding_size = 128 # Hidden layer representation size\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the TF graph\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### DATA PLACEHOLDERS ####################\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    train_one_hot = tf.one_hot(train_context, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### INPUT(EMBEDDING)LAYER #################\n",
    "with graph.as_default():\n",
    "    embeddings = tf.Variable(tf.random_uniform([VOCAB_SIZE, \n",
    "                                                embedding_size],\n",
    "                                               -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## HIDDEN LAYER ######################\n",
    "with graph.as_default():\n",
    "    weights = tf.Variable(tf.truncated_normal([VOCAB_SIZE, embedding_size],\n",
    "                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    biases = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "    hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ If we're going to se up experiments/comparisons between different embedding training methods (eg. Duongs word2vec modification vs the post training aligned word vectors referenced in the Babylon Repo)... we'll want to fix the embedding size across the multiple models. Maybe even fix the initialization for the weights?-- no in this case the weights are irrelevant across models b/c they'll be optimizing different things. Presumably part of what we're interested in is comparisons of speed to train in concert w/ efficacy on the translation task and random initialization always begs the question of 'did we just get lucky'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################## TRAIN OP ########################\n",
    "with graph.as_default():\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
    "                                                labels=train_one_hot))\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Set up validation set - arandomly chosen set of words to use to track our progress as we train. By construction we'll pick words from the 100 most frequent in the vocabulary then use cosine similarity to find the nearest neighbors in the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### VALIDATION EXAMPLES #################\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "with graph.as_default():\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### SIMILARITY CALCULATION ################\n",
    "with graph.as_default():\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable initializer\n",
    "with graph.as_default():\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3:__ Run the model & track progress by examining the matches for words in our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperfunc import generate_batch\n",
    "data_index = 0 # used to track batches\n",
    "\n",
    "def run(graph, num_steps):\n",
    "    \"\"\"Runner code for word2vec TF model w/ full softmax\"\"\"\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_context = generate_batch(en_data,\n",
    "                                                         batch_size, \n",
    "                                                         num_skips, \n",
    "                                                         skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs, \n",
    "                         train_context: batch_context}\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op \n",
    "            _, loss_val = session.run([optimizer, cross_entropy], \n",
    "                                      feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 100\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 500 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = en_index[valid_examples[i]]\n",
    "                    top_k = 8  # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = en_index[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                    print(log_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runner Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 10001\n",
    "softmax_start_time = dt.datetime.now()\n",
    "run(graph, num_steps=num_steps)\n",
    "softmax_end_time = dt.datetime.now()\n",
    "print(\"Softmax method took {} seconds to run 10000 iterations\".format((softmax_end_time-softmax_start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax method took 1461.520475 seconds to run 10000 iterations"
     ]
    }
   ],
   "source": [
    "# NOTE: output from ^^ saved to:\n",
    "path = 'wtv_output/en_smalldata_10Kiter_fullsfmx.txt'\n",
    "!tail -n 1 {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  9100 :  5.23669617653\r\n",
      "Average loss at step  9200 :  5.23754267216\r\n",
      "Average loss at step  9300 :  5.30736485481\r\n",
      "Average loss at step  9400 :  5.27600327492\r\n",
      "Average loss at step  9500 :  5.2527682209\r\n",
      "Average loss at step  9600 :  5.25820608616\r\n",
      "Average loss at step  9700 :  5.34852351189\r\n",
      "Average loss at step  9800 :  5.43696550369\r\n",
      "Average loss at step  9900 :  5.37404325485\r\n",
      "Average loss at step  10000 :  5.33349477768\r\n"
     ]
    }
   ],
   "source": [
    "# take a look at loss\n",
    "!grep 'Average' {path} | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Nixon, remove,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, remove, Nixon,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Nixon, shadow,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, shadow, remove,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, shadow, remove,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Alexander, shadow,\r\n",
      "Nearest to them: chose, lines, origin, Delos, lighter, young, Alexander, shadow,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Alexander, shadow,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Alexander, shadow,\r\n",
      "Nearest to them: chose, origin, lines, Delos, lighter, young, Alexander, shadow,\r\n"
     ]
    }
   ],
   "source": [
    "# take a look at NN for 'the'\n",
    "!grep 'Nearest to them:' {path} |tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ The data ^^ are undoubtedly too small... 'Alabama' shouldn't appear in the top 100 words. However I'll wait to look at larger data with the sampling method which is much mor efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Model w/ NCE (faster)\n",
    "\n",
    "We'll write it as a class this time for ease of calling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def with_self_graph(function):\n",
    "    \"\"\"Decorator-foo borrowed from w266 a4.\"\"\"\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        with self.graph.as_default():\n",
    "            return function(self, *args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    # This code was adapted from:\n",
    "    # SOURCE: https://github.com/tensorflow/tensorflow\n",
    "    #         /blob/r1.2/tensorflow/examples/tutorials\n",
    "    #         /word2vec/word2vec_basic.py\n",
    "    \n",
    "    def __init__(self, graph=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          V: vocabulary size\n",
    "          H: embedding size\n",
    "          \n",
    "        Kwargs (reset defaulst w/ feed_dict):\n",
    "          softmax_ns = 64\n",
    "          learning_rate = 1.0\n",
    "        \"\"\"\n",
    "        # Set TensorFlow graph. All TF code will work on this graph.\n",
    "        self.graph = graph or tf.Graph()\n",
    "        self.SetParams(*args, **kwargs)\n",
    "        \n",
    "    @with_self_graph\n",
    "    def SetParams(self, V, H, softmax_ns=64, learning_rate=1.0):\n",
    "        # Model structure.\n",
    "        self.V = V\n",
    "        self.H = H\n",
    "\n",
    "        # Training hyperparameters\n",
    "        with tf.name_scope(\"Training_Parameters\"):\n",
    "            # Number of samples for sampled softmax.\n",
    "            self.softmax_ns = softmax_ns\n",
    "            # Learning Rate\n",
    "            self.learning_rate = 1.0\n",
    "            #self.learning_rate_ = tf.placeholder(tf.float32, [], name=\"learning_rate\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix & Supplemental Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`helperfunc.py`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helperfunc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helperfunc.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Helper Functions for implementing Word2Vec in Python.\n",
    "\n",
    "Most of the functions in this file come from the Official \n",
    "Tensorflow Docs and are made available via the word2vec\n",
    "tutorial at: https://github.com/tensorflow/tensorflow/blob\n",
    "/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n",
    "As such, this code is protected by their liscence, see ^^.\n",
    "       \n",
    "As noted, some of these helper fuctions were written or modified\n",
    "by the authors of adventuresinmachinelearning.com as part of their\n",
    "word-2-vec tutorial which closely follows the Tensorflow code.\n",
    "\n",
    "I have also modified some to suit our use case.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Extract the file as a list of words.\n",
    "    NOTE: this is modified from original function in TF  \n",
    "    tutorialwhich expected a zipped input file.\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        data = tf.compat.as_str(f.read()).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"\n",
    "    Process raw inputs into a dataset.\n",
    "    Creates vocabulary from top n words indexed by rank.\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data_index = 0\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    \"\"\"\n",
    "    Function to generate a training batch for the skip-gram model.\n",
    "    NOTE: this wass modified from original function in TF  \n",
    "    tutorial by adventuresinML tutorial - mostly just renamed.\n",
    "    \"\"\"\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
