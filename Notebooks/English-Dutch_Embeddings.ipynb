{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English - Dutch Embeddings (3 versions)\n",
    "`w266 Final Project: Crosslingual Word Embeddings`\n",
    "\n",
    "Instead of traning on randomly substituted words, here we'll choose the translation that is closest to the context embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Base Paths__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MV_BASE = '/home/mmillervedam/Data'\n",
    "MI_BASE = '/home/miwamoto/Data'\n",
    "PROJ = '/home/mmillervedam/ProjectRepo'\n",
    "#PROJ = '/Users/mona/OneDrive/repos/final_proj/W266-Fall-2017-Final-Project'\n",
    "GTT_BASE = PROJ + '/BaselineModels/data/ground_truth_translations/'\n",
    "\n",
    "# directory to save pickled embeddings\n",
    "SAVE_TO = MV_BASE + '/embeddings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Globals__ - _the parameters below fully determine all 3 models in this NB_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "LANG = ('en','nl')\n",
    "FULL_TEXT = \"/home/miwamoto/en_nl_shuf.txt\"\n",
    "VOCAB_INDEX = MV_BASE + '/vocab/en_nl_small.pkl'\n",
    "PANLEX = MI_BASE + '/panlex/en_nl_dict.pkl'\n",
    "GTT_PATH = GTT_BASE + \"%s-%s-clean.csv\" % (LANG[1], LANG[0])\n",
    "\n",
    "# Model\n",
    "EMBEDDING_SIZE = 200\n",
    "\n",
    "# Training\n",
    "nBATCHES = 100000 # <<< 1 epoch with our 1 million sentence corpus\n",
    "BATCH_SIZE = 48\n",
    "WINDOW_SIZE = 4\n",
    "MAX_EPOCHS = 5 # fail safe\n",
    "ALPHA = 0.5 # authors use a much smaller learning rate but train longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import Corpus, BilingualVocabulary, batch_generator, get_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load corpus\n",
    "raw_data = Corpus(FULL_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load panlex dictionary\n",
    "with open(PANLEX,'rb') as f:\n",
    "    translations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocab = BilingualVocabulary([], languages = LANG)\n",
    "with open(VOCAB_INDEX,'rb') as f:\n",
    "    vocab.load_from_index(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded 437931 panlex translations\n",
      "... loaded 20003 word ('en', 'nl') vocabulary\n"
     ]
    }
   ],
   "source": [
    "# confirmations\n",
    "print('... loaded %s panlex translations'%(len(translations)))\n",
    "print('... loaded %s word %s vocabulary'%(vocab.size,vocab.language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... test word ids: [3, 226, 10010, 10065]\n"
     ]
    }
   ],
   "source": [
    "# Validation Words (for training printout)\n",
    "TEST_WORDS = vocab.to_ids(['en_the','en_last', 'nl_voor', 'nl_aantal'])\n",
    "print('... test word ids:', TEST_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded 93854 ground truth translations.\n"
     ]
    }
   ],
   "source": [
    "# Ground Truth Translations\n",
    "GTT_DF = pd.read_csv(GTT_PATH, names = [LANG[1], LANG[0]], sep=' ', header=None)\n",
    "print('... loaded %s ground truth translations.'%(len(GTT_DF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loaded 0 evaluation words.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Words (for reporting recall)\n",
    "eval_words = [w for w in get_common_words(vocab) if w.startswith(LANG[1])]\n",
    "EVAL_IDS = vocab.to_ids(eval_words)\n",
    "print('... loaded %s evaluation words.' % (len(EVAL_IDS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Random Translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_random\n",
    "\n",
    "# create model\n",
    "model_1 = BiW2V_random(bilingual_dict = translations,\n",
    "                       vocab = vocab, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model_1.BuildCoreGraph()\n",
    "model_1.BuildTrainingGraph()\n",
    "model_1.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fresh data generator\n",
    "DATA_GENERATOR = batch_generator(raw_data, vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(20003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.00147580852509\n",
      "   [en_the] closest:  en_boycott, en_grades, en_uefa, en_minnesota, en_perhaps, en_punt, nl_ballet, en_banking,\n",
      "   [en_last] closest:  en_sabbath, en_swords, en_orthodoxy, en_organisation, nl_schoon, en_lab, nl_stal, nl_doetinchem,\n",
      "   [nl_voor] closest:  en_coup, nl_born, nl_grip, nl_statuten, en_rather, nl_zand, en_rebellion, en_branches,\n",
      "   [nl_aantal] closest:  nl_oosterhout, en_mushroom, en_session, nl_ingesloten, nl_king, en_fire, nl_duiven, nl_opzicht,\n",
      "... STEP 10000 : Average Loss : 4.26000624292\n",
      "... STEP 20000 : Average Loss : 3.88555000093\n",
      "   [en_the] closest:  en_a, en_his, en_minnesota, en_grades, en_boycott, en_uefa, nl_westwood, en_garland,\n",
      "   [en_last] closest:  en_sabbath, en_swords, en_orthodoxy, en_lab, en_organisation, nl_schoon, nl_stal, en_obscure,\n",
      "   [nl_voor] closest:  nl_eerst, nl_het, nl_van, en_coup, nl_in, en_rather, nl_variatie, en_tale,\n",
      "   [nl_aantal] closest:  nl_oosterhout, en_mushroom, en_session, nl_ingesloten, nl_king, en_fire, nl_duiven, en_rome,\n",
      "... STEP 30000 : Average Loss : 3.80247880238\n",
      "... STEP 40000 : Average Loss : 3.74967385331\n",
      "   [en_the] closest:  en_a, en_his, en_boycott, en_grades, en_minnesota, en_their, en_uefa, en_garland,\n",
      "   [en_last] closest:  en_sabbath, en_swords, en_orthodoxy, en_lab, en_organisation, nl_schoon, en_obscure, nl_stal,\n",
      "   [nl_voor] closest:  nl_eerst, nl_in, nl_uit, nl_is, nl_van, nl_het, en_coup, en_rather,\n",
      "   [nl_aantal] closest:  nl_oosterhout, en_session, en_mushroom, nl_ingesloten, en_rome, nl_king, en_fire, nl_duiven,\n",
      "... STEP 50000 : Average Loss : 3.72885221913\n",
      "... STEP 60000 : Average Loss : 3.64384757075\n",
      "   [en_the] closest:  en_a, en_his, en_their, en_this, nl_het, en_an, nl_de, en_boycott,\n",
      "   [en_last] closest:  en_sabbath, en_swords, en_orthodoxy, en_lab, en_organisation, nl_schoon, en_capture, en_obscure,\n",
      "   [nl_voor] closest:  nl_eerst, nl_uit, nl_in, nl_is, nl_van, en_coup, nl_het, nl_door,\n",
      "   [nl_aantal] closest:  nl_oosterhout, en_session, nl_ingesloten, en_mushroom, nl_king, en_rome, en_fire, nl_soort,\n",
      "... STEP 70000 : Average Loss : 3.66751910747\n",
      "... STEP 80000 : Average Loss : 3.65162452517\n",
      "   [en_the] closest:  en_a, nl_het, en_his, en_this, nl_de, en_their, en_an, en_its,\n",
      "   [en_last] closest:  en_sabbath, en_lab, en_swords, en_orthodoxy, en_organisation, nl_schoon, en_capture, en_obscure,\n",
      "   [nl_voor] closest:  nl_uit, nl_eerst, nl_in, nl_door, nl_is, en_coup, nl_van, nl_variatie,\n",
      "   [nl_aantal] closest:  nl_soort, nl_oosterhout, en_session, en_rome, nl_king, nl_ingesloten, en_mushroom, en_fire,\n",
      "... STEP 90000 : Average Loss : 3.64857963815\n",
      "... STEP 100000 : Average Loss : 3.62429616221\n",
      "   [en_the] closest:  en_a, nl_het, en_his, en_this, en_their, en_its, en_an, nl_de,\n",
      "   [en_last] closest:  en_sabbath, en_lab, en_swords, en_orthodoxy, en_capture, en_organisation, nl_schoon, nl_driemaal,\n",
      "   [nl_voor] closest:  nl_uit, nl_eerst, nl_in, nl_door, nl_is, en_coup, nl_aan, nl_variatie,\n",
      "   [nl_aantal] closest:  nl_soort, nl_oosterhout, en_session, en_rome, nl_ingesloten, nl_king, en_mushroom, nl_geldig,\n",
      "... Training Complete\n",
      "... 100000 batches trained in 282.619189024 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "model_1.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = ALPHA)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context \n",
    "filename = SAVE_TO + '/en_nl_rand_100K_V_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_1.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# word\n",
    "filename = SAVE_TO + '/en_nl_rand_100K_U_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_1.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Most Common Target Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_mle\n",
    "\n",
    "# create model\n",
    "model_2 = BiW2V_mle(bilingual_dict = translations,\n",
    "                       vocab = vocab, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model_2.BuildCoreGraph()\n",
    "model_2.BuildTrainingGraph()\n",
    "model_2.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fresh data generator\n",
    "DATA_GENERATOR = batch_generator(raw_data, vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(20003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.00148056278229\n",
      "   [en_the] closest:  en_heats, nl_continu, nl_vijftig, en_sioux, nl_restaureren, en_enduring, nl_burggraaf, nl_jaartelling,\n",
      "   [en_last] closest:  nl_procureur, nl_jagers, nl_normaal, en_wade, nl_onderzoek, nl_leeuw, en_gum, en_skip,\n",
      "   [nl_voor] closest:  en_ghana, nl_plafond, en_bilingual, nl_grotendeels, nl_stuart, en_appearances, en_joaquin, nl_luisteren,\n",
      "   [nl_aantal] closest:  nl_roer, en_silesia, en_irrigation, nl_kwalificeren, nl_jommeke, en_medley, nl_afgelegen, nl_limiet,\n",
      "... STEP 10000 : Average Loss : 4.04762265734\n",
      "... STEP 20000 : Average Loss : 3.6590352628\n",
      "   [en_the] closest:  en_a, en_heats, en_and, en_passive, nl_continu, nl_restaureren, en_manitoba, en_enduring,\n",
      "   [en_last] closest:  nl_procureur, nl_jagers, nl_normaal, en_wade, nl_onderzoek, nl_leeuw, en_gum, en_skip,\n",
      "   [nl_voor] closest:  nl_in, en_ghana, nl_plafond, en_bilingual, en_prospect, en_redundant, nl_grotendeels, en_anton,\n",
      "   [nl_aantal] closest:  nl_roer, en_silesia, nl_jommeke, en_medley, en_irrigation, nl_kwalificeren, nl_limiet, nl_afgelegen,\n",
      "... STEP 30000 : Average Loss : 3.58041426963\n",
      "... STEP 40000 : Average Loss : 3.52019006448\n",
      "   [en_the] closest:  en_a, en_an, en_heats, en_its, nl_continu, en_passive, en_enduring, en_his,\n",
      "   [en_last] closest:  nl_jagers, nl_normaal, nl_procureur, en_wade, nl_onderzoek, nl_bovenkant, en_skip, nl_leeuw,\n",
      "   [nl_voor] closest:  nl_in, en_ghana, nl_plafond, nl_is, en_bilingual, en_prospect, en_anton, en_circus,\n",
      "   [nl_aantal] closest:  nl_roer, en_silesia, nl_jommeke, nl_eerst, en_medley, nl_limiet, nl_afgelegen, en_annually,\n",
      "... STEP 50000 : Average Loss : 3.48856138529\n",
      "... STEP 60000 : Average Loss : 3.39445415405\n",
      "   [en_the] closest:  en_a, en_an, en_its, en_this, en_his, en_their, en_heats, en_enduring,\n",
      "   [en_last] closest:  nl_jagers, nl_normaal, nl_procureur, nl_bovenkant, nl_onderzoek, en_wade, en_skip, nl_leeuw,\n",
      "   [nl_voor] closest:  nl_in, nl_is, en_ghana, nl_plafond, en_circus, nl_efficiëntie, nl_op, en_prospect,\n",
      "   [nl_aantal] closest:  nl_eerst, nl_roer, en_silesia, nl_jommeke, nl_is, en_medley, nl_limiet, nl_blokken,\n",
      "... STEP 70000 : Average Loss : 3.4178840347\n",
      "... STEP 80000 : Average Loss : 3.42026182629\n",
      "   [en_the] closest:  en_a, en_its, en_this, en_an, en_their, en_his, en_enduring, en_heats,\n",
      "   [en_last] closest:  nl_jagers, nl_normaal, nl_bovenkant, nl_onderzoek, nl_procureur, nl_verloop, en_nas, en_wade,\n",
      "   [nl_voor] closest:  nl_in, nl_op, nl_is, en_ghana, nl_efficiëntie, en_anton, en_circus, en_prospect,\n",
      "   [nl_aantal] closest:  nl_eerst, nl_roer, nl_is, en_silesia, nl_jommeke, en_gasoline, nl_blokken, nl_limiet,\n",
      "... STEP 90000 : Average Loss : 3.40379472343\n",
      "... STEP 100000 : Average Loss : 3.3625613836\n",
      "   [en_the] closest:  en_a, en_its, en_this, en_an, en_their, en_his, en_enduring, en_heats,\n",
      "   [en_last] closest:  nl_bovenkant, nl_jagers, nl_normaal, nl_verloop, en_nas, en_construct, en_skip, en_clash,\n",
      "   [nl_voor] closest:  nl_in, nl_op, nl_is, en_ghana, nl_efficiëntie, en_for, en_circus, en_prospect,\n",
      "   [nl_aantal] closest:  nl_eerst, nl_is, nl_roer, nl_soort, nl_jommeke, en_silesia, en_gasoline, nl_blokken,\n",
      "... Training Complete\n",
      "... 100000 batches trained in 282.897243023 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "model_2.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = ALPHA)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context \n",
    "filename = SAVE_TO + '/en_nl_mle_100K_V_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_2.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# word\n",
    "filename = SAVE_TO + '/en_nl_mle_100K_U_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_2.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: Closest Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TF graph created for BiW2V model.\n",
      "... TF graph created for BiW2V training.\n",
      "... TF graph created for BiW2V validation.\n"
     ]
    }
   ],
   "source": [
    "from models import BiW2V_nn\n",
    "\n",
    "# create model\n",
    "model_3 = BiW2V_nn(bilingual_dict = translations,\n",
    "                   vocab = vocab, H = EMBEDDING_SIZE)\n",
    "\n",
    "# intialize TF graphs\n",
    "model_3.BuildCoreGraph()\n",
    "model_3.BuildTrainingGraph()\n",
    "model_3.BuildValidationGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fresh data generator\n",
    "DATA_GENERATOR = batch_generator(raw_data, vocab, BATCH_SIZE, WINDOW_SIZE, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Model Initialized\n",
      "\t <tf.Variable 'Embedding_Layer/ContextEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/WordEmbeddings:0' shape=(20003, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'Hidden_Layer/b:0' shape=(20003,) dtype=float32_ref>\n",
      "... Starting Training\n",
      "... STEP 0 : Average Loss : 0.0290604896545\n",
      "   [en_the] closest:  en_hospitality, en_communicate, en_nichols, nl_vastleggen, en_transformation, nl_heilige, nl_schuin, nl_verloor,\n",
      "   [en_last] closest:  nl_aveyron, nl_valentine, nl_koersen, nl_aruba, en_suburban, en_armenians, en_virgil, en_blank,\n",
      "   [nl_voor] closest:  en_correspondent, nl_duisburg, nl_steenbergen, nl_fleming, nl_achterste, en_combine, nl_stephen, nl_juventus,\n",
      "   [nl_aantal] closest:  nl_estisch, nl_plato, nl_wijd, en_ms., en_voyage, en_belong, en_obsession, en_genetic,\n",
      "... STEP 500 : Average Loss : 5.9849984839\n",
      "... STEP 1000 : Average Loss : 5.24351522017\n",
      "   [en_the] closest:  en_hospitality, nl_verloor, nl_laser, en_acronym, en_nichols, nl_krim, nl_heilige, nl_vastleggen,\n",
      "   [en_last] closest:  nl_aveyron, nl_koersen, nl_valentine, nl_aruba, en_suburban, en_armenians, en_virgil, en_blank,\n",
      "   [nl_voor] closest:  en_correspondent, nl_duisburg, nl_steenbergen, nl_fleming, en_combine, nl_achterste, nl_stephen, nl_juventus,\n",
      "   [nl_aantal] closest:  nl_estisch, nl_plato, nl_wijd, en_ms., en_voyage, en_belong, en_obsession, en_genetic,\n",
      "... STEP 1500 : Average Loss : 5.05810768485\n",
      "... STEP 2000 : Average Loss : 4.94261519122\n",
      "   [en_the] closest:  en_hospitality, en_nichols, nl_laser, nl_verloor, en_acronym, nl_krim, nl_heilige, nl_smelten,\n",
      "   [en_last] closest:  nl_aveyron, nl_koersen, nl_valentine, nl_aruba, en_suburban, en_armenians, en_virgil, en_blank,\n",
      "   [nl_voor] closest:  en_correspondent, nl_duisburg, nl_steenbergen, nl_fleming, en_combine, nl_achterste, en_suspicion, nl_juventus,\n",
      "   [nl_aantal] closest:  nl_estisch, nl_plato, nl_wijd, en_voyage, en_ms., en_belong, en_obsession, en_genetic,\n",
      "... STEP 2500 : Average Loss : 4.86860204268\n",
      "... STEP 3000 : Average Loss : 4.727818753\n",
      "   [en_the] closest:  en_hospitality, en_nichols, nl_laser, en_acronym, nl_verloor, nl_krim, nl_heilige, nl_enige,\n",
      "   [en_last] closest:  nl_aveyron, nl_koersen, nl_valentine, nl_aruba, en_suburban, en_armenians, en_virgil, en_blank,\n",
      "   [nl_voor] closest:  en_correspondent, nl_duisburg, nl_steenbergen, nl_fleming, en_combine, nl_achterste, nl_waarden, en_suspicion,\n",
      "   [nl_aantal] closest:  nl_estisch, nl_plato, nl_wijd, en_voyage, en_ms., en_belong, en_obsession, en_genetic,\n",
      "... STEP 3500 : Average Loss : 4.71035161519\n",
      "... STEP 4000 : Average Loss : 4.68220962667\n",
      "   [en_the] closest:  en_hospitality, en_nichols, en_acronym, nl_verloor, nl_laser, nl_heilige, nl_krim, nl_enige,\n",
      "   [en_last] closest:  nl_aveyron, nl_koersen, nl_valentine, en_suburban, nl_aruba, en_armenians, en_virgil, en_blank,\n",
      "   [nl_voor] closest:  en_correspondent, nl_duisburg, nl_het, nl_steenbergen, nl_fleming, en_combine, nl_achterste, nl_waarden,\n",
      "   [nl_aantal] closest:  nl_estisch, nl_plato, nl_wijd, en_voyage, en_ms., en_belong, en_obsession, en_genetic,\n",
      "... STEP 4500 : Average Loss : 4.64071756458\n",
      "... STEP 5000 : Average Loss : 4.58630378056\n",
      "   [en_the] closest:  en_nichols, en_hospitality, en_acronym, nl_verloor, nl_krim, nl_heilige, nl_laser, nl_enige,\n",
      "   [en_last] closest:  nl_aveyron, nl_koersen, nl_valentine, en_suburban, nl_aruba, en_armenians, en_virgil, en_blank,\n",
      "   [nl_voor] closest:  en_correspondent, nl_het, nl_duisburg, nl_steenbergen, nl_fleming, en_combine, nl_waarden, nl_achterste,\n",
      "   [nl_aantal] closest:  nl_estisch, nl_plato, nl_wijd, en_voyage, en_ms., en_belong, en_obsession, en_genetic,\n",
      "... Training Complete\n",
      "... 5000 batches trained in 2032.36627007 seconds\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "nBATCHES = 5000 # Takes too long w/ nn so we'll only do 5K\n",
    "start = time.time()\n",
    "model_3.train(nBATCHES, DATA_GENERATOR, TEST_WORDS, learning_rate = ALPHA)\n",
    "tot = (time.time() - start)\n",
    "print('... {} batches trained in {} seconds'.format(nBATCHES, tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context \n",
    "filename = SAVE_TO + '/en_nl_nn_5K_V_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_2.context_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# word\n",
    "filename = SAVE_TO + '/en_nl_nn_5K_U_dec19.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(model_2.word_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
