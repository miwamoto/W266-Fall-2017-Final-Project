{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Wiki Data\n",
    "` w266 Final Project: Crosslingual Word Embeddings`   \n",
    "\n",
    "The code in this notebook and the supporting file __`parsing.py`__ build on the helper functions provided in the TensorFlow Word2Vec tutorial to develop a set of data handling functions for use with the data relevant to Duong et al's paper. Ideally I'll develop a scalable solution for tokenizing, prepending language indicators (eg. `en_`) and extracting sentences in two langauges to create traning data that includes sentences from two languages. I also hope to develop a batch iterator modeled after the one in A4. Depending on the available tools I may end up needing to look at using a distributed system (Spark?) for preprocessing the English corpus which is ~ 9GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##    Note: Start Jupyter with \n",
    "##      jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filepaths\n",
    "\n",
    "## Maya's paths\n",
    "#BASE = '/Users/mmillervedam/Documents/MIDS/w266' #'/home/mmillervedam/' \n",
    "#PROJ = '/Users/mmillervedam/Documents/MIDS/w266/FinalProject'#'/home/mmillervedam/ProjectRepo'\n",
    "\n",
    "## Roseanna's paths\n",
    "\n",
    "\n",
    "## Mona's local paths\n",
    "BASE = '/Users/mona/OneDrive/repos/Data' #'/home/mmillervedam/Data'\n",
    "PROJ = '/Users/mona/OneDrive/repos/final_proj/W266-Fall-2017-Final-Project'#'/home/mmillervedam/ProjectRepo'\n",
    "## Mona's gc paths\n",
    "BASE = '/home/miwamoto' #'/home/mmillervedam/Data'\n",
    "PROJ = '/home/miwamoto/W266-Fall-2017-Final-Project'#'/home/mmillervedam/ProjectRepo'\n",
    "\n",
    "\n",
    "## Repo paths\n",
    "\n",
    "FPATH_EN = BASE + '/Data/test/wiki_en_10K.txt' # first 10000 lines from wiki dump\n",
    "FPATH_ES = BASE + '/Data/test/wiki_es_10K.txt' # first 10000 lines from wiki dump\n",
    "EN_ES_DICT = PROJ +'/XlingualEmb/data/dicts/en.es.panlex.all.processed'\n",
    "EN_IT_DICT  = PROJ +'/XlingualEmb/data/dicts/en.it.panlex.all.processed'\n",
    "EN_IT_RAW = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'\n",
    "EN_IT_RAW = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'\n",
    "FULL_EN = BASE + '/Data/en/full.txt'\n",
    "FULL_ES = BASE + '/Data/es/full.txt'\n",
    "FULL_IT = BASE + '/Data/it/full.txt'\n",
    "FULL_FR = BASE + '/Data/fr/full.txt'\n",
    "FULL_JA = BASE + '/Data/ja/full.txt'\n",
    "FULL_NL = BASE + '/Data/nl/full.txt'\n",
    "\n",
    "DPATH = PROJ +'/XlingualEmb/data/dicts/en.es.panlex.all.processed'\n",
    "EN_IT = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'\n",
    "\n",
    "## Large datasets\n",
    "FULL_EN_ES = \"./shuffled_files/en_es_shuf.txt\"\n",
    "FULL_EN_IT = \"./shuffled_files/en_it_shuf.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desired Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_[[877881]]\r\n",
      "it_[[879362]]\r\n",
      "it_in it_un it_remoto it_passato it_aveva it_progettato it_, it_per it_conto it_dei it_demoniazzi it_silastici it_di it_striterax it_, it_una it_bomba it_in it_grado it_di it_collegare it_simultaneamente it_tutti it_i it_nuclei it_di it_tutte it_le it_stelle it_, it_creando it_così it_un'immensa it_supernova it_che it_avrebbe it_distrutto it_l'universo it_, it_secondo it_i it_desideri it_dei it_demoniazzi it_silastici it_.\r\n",
      "it_krikkitesi it_i it_krikkitesi it_sono it_una it_razza it_aliena it_che it_per it_miliardi it_di it_anni it_aveva it_vissuto it_senza it_la it_minima it_consapevolezza it_dell'esistenza it_di it_altri it_mondi it_o it_altre it_specie it_.\r\n",
      "en_as en_the en_patron en_of en_delphi en_( en_pythian en_apollo en_) en_, en_apollo en_was en_an en_oracular en_god en_— en_the en_prophetic en_deity en_of en_the en_delphic en_oracle en_.\r\n",
      "it_all'inizio it_del it_2006 it_ha it_pubblicato it_il it_suo it_primo it_singolo it_solista it_, it_nell'angolo it_, it_con it_la it_partecipazione it_dello it_stesso it_zero it_, it_che it_ha it_collaborato it_anche it_alla it_stesura it_del it_testo it_.\r\n",
      "en_medieval en_muslim en_scholars en_regularly en_described en_aristotle en_as en_the en_\" en_first en_teacher en_\" en_.\r\n",
      "it_[[876688]]\r\n",
      "en_roman en_gladiatorial en_games en_often en_referenced en_classical en_mythology en_, en_and en_this en_seems en_to en_reference en_achilles en_' en_fight en_with en_penthesilea en_but en_gives en_it en_an en_extra en_twist en_of en_achilles en_being en_\" en_played en_\" en_by en_a en_woman en_.\r\n",
      "it_l'arrangiamento it_è it_opera it_di it_demo it_morselli it_.\r\n"
     ]
    }
   ],
   "source": [
    "# take a look at what Duong et al trained on for reference\n",
    "!head -n 10 {EN_IT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ There are no UNK tokens here and punctuation is included as its own token. However words are lowercased and the language marker is prepended. Also note that sentences from the two languages have been shuffled together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Code\n",
    "I've put the parsing functions in their own python script for ease of access and shared editing. The scrips can be found in the shared repo at: __`/Notebooks/parsing.py`__. Here's a quick overview of the methods it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import Corpus, Vocabulary, batch_generator, make_bilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Class with helper methods to read from a Corpus.\n",
      "    Intended to facilitate working with multiple corpora at once.\n",
      "    Init Args:\n",
      "        path - (str) filepath of the raw data\n",
      "        prefix - (str) optional language prefix to prepend when reading\n",
      "    Methods:\n",
      "        gen_tokens - generator factory for tokens in order\n",
      "        gen_sentences - generator factory for sentences in order\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Corpus.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This class is based heavily on code provided in a4 of MIDS w266, Fall 2017.\n",
      "    Init Args:\n",
      "        tokens    - iterable of tokens to count\n",
      "        wordset   - (optional) limit vocabulary to these words\n",
      "        size      - (optional) integer, number of vocabulary words\n",
      "    Attributes:\n",
      "        self.index   - dictionary of {id : type}\n",
      "        self.size    - integer, number of words in total\n",
      "        self.types   - dictionary of {type : id}\n",
      "        self.wordset - set of types\n",
      "        self.language- order of languages in the index\n",
      "    Methods:\n",
      "        self.to_ids(words) - returns list of ids for the word list\n",
      "        self.to_words(ids) - returns list of words for the id list\n",
      "        self.sentence_to_ids(sentence) - returns list of ids with start & end\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Vocabulary.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function to iterate repeated over a corpus delivering\n",
      "    batch_size arrays of ids and context_labels for CBOW.\n",
      "\n",
      "    Args:\n",
      "        corpus - an instance of Corpus()\n",
      "        vocabulary - an instance of Vocabulary()\n",
      "        batch_size - int, number of words to serve at once\n",
      "        bag_window - context distance for CBOW training\n",
      "        max_epochs - int(default = None) stop generating\n",
      "\n",
      "    Yields:\n",
      "        batch: np.array of dim: (batch_size, 2*bag_window)\n",
      "               Represents set of context words.\n",
      "        labels: np.array of dim: (batch_size, 1)\n",
      "               Represents center words to predict/translate.\n",
      "\n",
      "    WARNING: this generator will go on ad infinitum unless\n",
      "    you specify max_epochs or explicitly break.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(batch_generator.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/miwamoto/Data/test/wiki_en_10K.txt\n"
     ]
    }
   ],
   "source": [
    "print(FPATH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# english test corpus\n",
    "en_test = Corpus(FPATH_EN, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_[[12]]\n",
      "en_anarchism\n",
      "en_is\n",
      "en_often\n",
      "en_defined\n",
      "en_as\n",
      "en_a\n",
      "en_political\n",
      "en_philosophy\n",
      "en_which\n"
     ]
    }
   ],
   "source": [
    "# demo generator\n",
    "idx = 1\n",
    "for tok in en_test.gen_tokens():\n",
    "    print(tok)\n",
    "    idx += 1\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# english vocabulary\n",
    "en_vocab = Vocabulary(en_test.gen_tokens(), size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look\n",
    "#en_vocab.types\n",
    "#en_vocab.index\n",
    "#en_vocab.wordset\n",
    "en_vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " en_anarchism en_is en_often en_defined en_as en_a en_political en_philosophy en_which en_holds en_the en_state en_to en_be en_undesirable en_, en_unnecessary en_, en_or en_harmful en_.\n",
      "[0, 209, 11, 93, 598, 13, 10, 186, 267, 28, 2, 3, 58, 9, 30, 2, 4, 2, 4, 25, 2, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "# translate the first sentence into indexes\n",
    "idx = 0\n",
    "for sent in en_test.gen_sentences():\n",
    "    if idx == 1:\n",
    "        print(sent)\n",
    "        print(en_vocab.sentence_to_ids(sent))\n",
    "        break\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT WINDOWS: [[0, 0, 1, 1], [0, 0, 11, 93], [0, 209, 93, 598], [209, 11, 598, 13]]\n",
      "CENTER WORDS: [2, 209, 11, 93]\n",
      "CONTEXT WINDOWS: [[11, 93, 13, 10], [93, 598, 10, 186], [598, 13, 186, 267], [13, 10, 267, 28]]\n",
      "CENTER WORDS: [598, 13, 10, 186]\n",
      "CONTEXT WINDOWS: [[10, 186, 28, 2], [186, 267, 2, 3], [267, 28, 3, 58], [28, 2, 58, 9]]\n",
      "CENTER WORDS: [267, 28, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# demo batch iterator\n",
    "batch_size = 4\n",
    "bag_window = 2\n",
    "idx = 0\n",
    "for batch, labels in batch_generator(en_test, en_vocab, batch_size, bag_window):\n",
    "    print(\"CONTEXT WINDOWS:\", batch)\n",
    "    print(\"CENTER WORDS:\", labels)\n",
    "    idx += 1\n",
    "    if idx > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "['<s>', '<s>', '</s>', '</s>'] --> ['<unk>']\n",
      "['<s>', '<s>', 'en_is', 'en_often'] --> ['en_anarchism']\n",
      "['<s>', 'en_anarchism', 'en_often', 'en_defined'] --> ['en_is']\n",
      "['en_anarchism', 'en_is', 'en_defined', 'en_as'] --> ['en_often']\n",
      "Batch 2:\n",
      "['en_is', 'en_often', 'en_as', 'en_a'] --> ['en_defined']\n",
      "['en_often', 'en_defined', 'en_a', 'en_political'] --> ['en_as']\n",
      "['en_defined', 'en_as', 'en_political', 'en_philosophy'] --> ['en_a']\n",
      "['en_as', 'en_a', 'en_philosophy', 'en_which'] --> ['en_political']\n",
      "Batch 3:\n",
      "['en_a', 'en_political', 'en_which', '<unk>'] --> ['en_philosophy']\n",
      "['en_political', 'en_philosophy', '<unk>', 'en_the'] --> ['en_which']\n",
      "['en_philosophy', 'en_which', 'en_the', 'en_state'] --> ['<unk>']\n",
      "['en_which', '<unk>', 'en_state', 'en_to'] --> ['en_the']\n"
     ]
    }
   ],
   "source": [
    "# demo batch iterator w/ readible format\n",
    "batch_size = 4\n",
    "bag_window = 2\n",
    "idx = 0\n",
    "for batch, labels in batch_generator(en_test, en_vocab, batch_size, bag_window):\n",
    "    print(\"Batch %s:\"%(idx+1))\n",
    "    for context, wrd in zip(batch,labels):\n",
    "        print(en_vocab.to_words(context), \"-->\", en_vocab.to_words([wrd]))\n",
    "    idx += 1\n",
    "    if idx > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`QUESTION:`__ What do we do for context w/ the start and end words? I need to go back and check Mona's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10000  259843 1461734 /Users/mona/OneDrive/repos/Data/test/wiki_en_10K.txt\r\n"
     ]
    }
   ],
   "source": [
    "# confirm that batch generator will reload\n",
    "!wc {FPATH_EN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filmography Tarkovsky is mainly known as a director of films .\r\n"
     ]
    }
   ],
   "source": [
    "# last sentence\n",
    "!tail -n 1 {FPATH_EN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12]]\r\n",
      "Anarchism is often defined as a political philosophy which holds the state to be undesirable , unnecessary , or harmful .\r\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "!head -n 2 {FPATH_EN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ `~65001 batches per epoch` (in this test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 65000:\n",
      "['<unk>', 'en_tarkovsky', '<unk>', 'en_known'] --> ['en_is']\n",
      "['en_tarkovsky', 'en_is', 'en_known', 'en_as'] --> ['<unk>']\n",
      "['en_is', '<unk>', 'en_as', 'en_a'] --> ['en_known']\n",
      "['<unk>', 'en_known', 'en_a', '<unk>'] --> ['en_as']\n",
      "Batch 65001:\n",
      "['en_known', 'en_as', '<unk>', 'en_of'] --> ['en_a']\n",
      "['en_as', 'en_a', 'en_of', 'en_films'] --> ['<unk>']\n",
      "['en_a', '<unk>', 'en_films', 'en_.'] --> ['en_of']\n",
      "['<unk>', 'en_of', 'en_.', '</s>'] --> ['en_films']\n",
      "Batch 65002:\n",
      "['en_of', 'en_films', '</s>', '</s>'] --> ['en_.']\n",
      "['<s>', '<s>', '</s>', '</s>'] --> ['<unk>']\n",
      "['<s>', '<s>', 'en_is', 'en_often'] --> ['en_anarchism']\n",
      "['<s>', 'en_anarchism', 'en_often', 'en_defined'] --> ['en_is']\n",
      "Batch 65003:\n",
      "['en_anarchism', 'en_is', 'en_defined', 'en_as'] --> ['en_often']\n",
      "['en_is', 'en_often', 'en_as', 'en_a'] --> ['en_defined']\n",
      "['en_often', 'en_defined', 'en_a', 'en_political'] --> ['en_as']\n",
      "['en_defined', 'en_as', 'en_political', 'en_philosophy'] --> ['en_a']\n"
     ]
    }
   ],
   "source": [
    "# print the 64952-4rd batch (should be the same as above)\n",
    "idx = 0\n",
    "for batch, labels in batch_generator(en_test, en_vocab, 4, 2):\n",
    "    idx += 1\n",
    "    if idx < 65000:\n",
    "        continue\n",
    "    elif idx > 65003:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Batch %s:\"%(idx))\n",
    "        for context, wrd in zip(batch,labels):\n",
    "            print(en_vocab.to_words(context), \"-->\", en_vocab.to_words([wrd]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Full Datasets\n",
    "## Prepare English Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/miwamoto/Data/en/full.txt\n"
     ]
    }
   ],
   "source": [
    "print(FULL_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# english full corpus\n",
    "en_text = Corpus(FULL_EN, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to split -  578.537941 seconds\n",
      "6946 files written\n"
     ]
    }
   ],
   "source": [
    "# minimum sentence length = 3\n",
    "min_sentence_length = 3\n",
    "en_text.split_file(min_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6946\n"
     ]
    }
   ],
   "source": [
    "# Each split has 10K sentences\n",
    "print(en_text.splits)\n",
    "#en_text.splits = 6946"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may need to change the upper limit of open files\n",
    "the soft limit imposed by the current configuration\n",
    "the hard limit imposed by the operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft limit is  10000\n",
      "Hard limit is  1048576\n"
     ]
    }
   ],
   "source": [
    "import resource\n",
    "\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print('Soft limit is ', soft)\n",
    "print('Hard limit is ', hard)\n",
    "\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (10000, hard))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1048576)\n",
      "Time to shuffle -  10.776336 seconds\n",
      "0 sentences skipped\n"
     ]
    }
   ],
   "source": [
    "# Randomly draw from split files\n",
    "# This take a LONG time for a 9GB file\n",
    "en_text.draw_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 miwamoto miwamoto 991M Dec 18 21:48 ./shuffled_files/en_shuffled.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Quick check - shuffled file size\n",
    "!ls -lh ./shuffled_files/en_shuffled.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " en_reception en_the en_allmusic en_review en_by en_steve en_huey en_awarded en_the en_album en_5 en_stars en_and en_calling en_it en_\" en_a en_superbly en_sensuous en_blend en_of en_lusty en_blues en_swagger en_and en_achingly en_romantic en_ballads en_... en_a en_quiet en_, en_sorely en_underrated en_masterpiece en_\" en_.\r\n",
      " en_astrid en_is en_a en_member en_of en_the en_crimean en_royal en_knights en_.\r\n",
      " en_although en_it en_ran en_reasonably en_well en_, en_the en_engine en_was en_fuel en_inefficient en_, en_extremely en_noisy en_, en_tended en_to en_overheat en_and en_, en_if en_sufficient en_cooling en_water en_was en_not en_applied en_, en_seize en_up en_.\r\n",
      " en_the en_hebrew en_bible en_refers en_uncritically en_to en_slavery en_as en_an en_established en_institution en_.\r\n",
      " en_sandoval en_sued en_under en_title en_vi en_of en_the en_civil en_rights en_act en_of en_1964 en_.\r\n"
     ]
    }
   ],
   "source": [
    "!head -n5 ./shuffled_files/en_shuffled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './split_files/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# remove files if not needed\n",
    "!rm ./split_files/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Spanish Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/miwamoto/Data/es/full.txt\n"
     ]
    }
   ],
   "source": [
    "print(FULL_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spanish full corpus\n",
    "es_text = Corpus(FULL_ES, 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to split -  126.983595 seconds\n",
      "1441 files written\n"
     ]
    }
   ],
   "source": [
    "# Split full file using min_sentence_length\n",
    "\n",
    "min_sentence_length = 3\n",
    "es_text.split_file(min_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1441\n"
     ]
    }
   ],
   "source": [
    "print(es_text.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1048576)\n",
      "Time to shuffle -  9.490429 seconds\n",
      "0 sentences skipped\n"
     ]
    }
   ],
   "source": [
    "es_text.draw_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 miwamoto miwamoto 1.1G Dec 18 22:06 ./shuffled_files/es_shuffled.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Quick check - shuffled file size\n",
    "!ls -lh ./shuffled_files/es_shuffled.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " es_el es_hecho es_de es_que es_acacio es_se es_hubiera es_presentado es_hasta es_entonces es_como es_un es_defensor es_de es_la es_verdadera es_ortodoxia es_es es_curioso es_.\r\n",
      " es_en es_1162 es_sancho es_vi es_de es_navarra es_se es_lanza es_a es_la es_conquista es_de es_la es_rioja es_, es_conquistando es_en es_1163 es_logroño es_, es_navarrete es_, es_entrena es_, es_ausejo es_, es_resa es_, es_ocón es_, es_autol es_, es_quel es_, es_grañón es_, es_pazuengos es_y es_treviana es_.\r\n",
      " es_en es_él es_se es_ubican es_la es_facultad es_de es_ciencias es_económicas es_y es_empresariales es_, es_la es_escuela es_universitaria es_de es_estudios es_empresariales es_, es_la es_facultad es_de es_derecho es_, es_la es_facultad es_de es_ciencias es_políticas es_y es_sociales es_, es_la es_facultad es_de es_filosofía es_, es_la es_facultad es_de es_psicología es_, es_la es_facultad es_de es_filología es_, es_facultad es_de es_geografía es_e es_historia es_, es_la es_facultad es_de es_ciencias es_de es_la es_información es_, es_la es_biblioteca es_general es_, es_el es_aulario es_de es_guajara es_, es_la es_comisión es_de es_doctorado es_y es_un es_edificio es_de es_servicios es_al es_alumnado es_ull es_- es_cajacanarias es_en es_el es_cual es_se es_ubica es_la es_tienda es_de es_la es_universidad es_.\r\n",
      " es_johnson es_solanke es_tomori es_es es_importante es_en es_la es_historia es_de es_ikeja es_.\r\n",
      " es_los es_romanos es_no es_utilizaban es_para es_estas es_empalizadas es_troncos es_fuertes es_, es_sino es_estacas es_del es_grosor es_del es_brazo es_pero es_con es_varias es_ramas es_laterales es_que es_se es_podían es_enlazar es_unas es_con es_otras es_.\r\n"
     ]
    }
   ],
   "source": [
    "!head -n5 ./shuffled_files/es_shuffled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove files if not needed\n",
    "!rm ./split_files/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Italian Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/miwamoto/Data/it/full.txt\n"
     ]
    }
   ],
   "source": [
    "print(FULL_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Italian full corpus\n",
    "it_text = Corpus(FULL_IT, 'it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to split -  102.788192 seconds\n",
      "1152 files written\n"
     ]
    }
   ],
   "source": [
    "# Split full file using min_sentence_length\n",
    "\n",
    "min_sentence_length = 3\n",
    "it_text.split_file(min_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1048576)\n",
      "Time to shuffle -  9.534405 seconds\n",
      "0 sentences skipped\n"
     ]
    }
   ],
   "source": [
    "it_text.draw_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 miwamoto miwamoto 1.2G Dec 18 22:18 ./shuffled_files/it_shuffled.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Quick check - shuffled file size\n",
    "!ls -lh ./shuffled_files/it_shuffled.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it_lo it_stereoscopio it_ad it_ingrandimento it_variabile it_è it_un it_particolare it_stereoscopio it_idoneo it_a it_variare it_l'ingrandimento it_del it_modello it_ottico it_tridimensionale it_osservato it_.\r\n",
      " it_il it_suo it_simbolo it_è it_ni it_.\r\n",
      " it_in it_quella it_squadra it_già it_giocava it_il it_triestino it_nereo it_rocco it_: it_è it_stato it_prima it_un it_importante it_giocatore it_, it_poi it_allenatore it_.\r\n",
      " it_atena it_trasformò it_anche it_la it_parte it_inferiore it_del it_loro it_corpo it_in it_modo it_tale it_da it_renderle it_impossibilitate it_ad it_avere it_rapporti it_sessuali it_con it_un it_uomo it_.\r\n",
      " it_file:copia it_della it_madonna it_di it_albinea it_di it_correggio it_2 it_. it_jpg it_| it_dettaglio it_bibliografia it_collegamenti it_esterni\r\n"
     ]
    }
   ],
   "source": [
    "!head -n5 ./shuffled_files/it_shuffled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove files if not needed\n",
    "!rm ./split_files/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare French Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/miwamoto/Data/fr/full.txt\n"
     ]
    }
   ],
   "source": [
    "print(FULL_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Italian full corpus\n",
    "fr_text = Corpus(FULL_FR, 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to split -  152.248704 seconds\n",
      "1797 files written\n"
     ]
    }
   ],
   "source": [
    "# Split full file using min_sentence_length\n",
    "\n",
    "min_sentence_length = 3\n",
    "fr_text.split_file(min_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(fr_text.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1048576)\n",
      "Time to shuffle -  9.720246 seconds\n",
      "0 sentences skipped\n"
     ]
    }
   ],
   "source": [
    "fr_text.draw_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 miwamoto miwamoto 1.1G Dec 18 22:22 ./shuffled_files/fr_shuffled.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Quick check - shuffled file size\n",
    "!ls -lh ./shuffled_files/fr_shuffled.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fr_seuls fr_babylone fr_, fr_l fr_' fr_urartu fr_, fr_l fr_' fr_Élam fr_et fr_l fr_' fr_Égypte fr_peuvent fr_un fr_temps fr_caresser fr_l'idée fr_de fr_rivaliser fr_avec fr_lui fr_, fr_mais fr_ils fr_sont fr_finalement fr_tous fr_vaincus fr_.\r\n",
      " fr_comme fr_dans fr_le fr_reste fr_de fr_la fr_région fr_, fr_à fr_la fr_fin fr_du fr_se fr_déroule fr_la fr_guerre fr_de fr_vendée fr_, fr_qui fr_marque fr_de fr_son fr_empreinte fr_le fr_pays fr_tout fr_entier fr_.\r\n",
      " fr_le fr_propulseur fr_est fr_un fr_moteur fr_fusée fr_à fr_carburant fr_solide fr_.\r\n",
      " fr_le fr_dernier fr_kilomètre fr_très fr_raide fr_permet fr_à fr_david fr_de fr_la fr_fuente fr_d'attaquer fr_et fr_de fr_remporter fr_sa fr_première fr_victoire fr_de fr_la fr_saison fr_.\r\n",
      " fr_en fr_france fr_, fr_de fr_nombreuses fr_chaussures fr_de fr_femmes fr_furent fr_par fr_conséquent fr_pourvues fr_de fr_semelles fr_compensées fr_en fr_bois fr_.\r\n"
     ]
    }
   ],
   "source": [
    "!head -n5 ./shuffled_files/fr_shuffled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove files if not needed\n",
    "!rm ./split_files/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dutch Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/miwamoto/Data/nl/full.txt\n"
     ]
    }
   ],
   "source": [
    "print(FULL_NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import Corpus, Vocabulary, batch_generator, make_bilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Italian full corpus\n",
    "nl_text = Corpus(FULL_NL, 'nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to split -  77.246617 seconds\n",
      "1067 files written\n"
     ]
    }
   ],
   "source": [
    "# Split full file using min_sentence_length\n",
    "\n",
    "min_sentence_length = 3\n",
    "nl_text.split_file(min_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067\n"
     ]
    }
   ],
   "source": [
    "print(nl_text.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Soft limit is ', 10000)\n",
      "('Hard limit is ', 1048576)\n"
     ]
    }
   ],
   "source": [
    "import resource\n",
    "\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print('Soft limit is ', soft)\n",
    "print('Hard limit is ', hard)\n",
    "\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (10000, hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1048576)\n",
      "Time to shuffle -  7.817511 seconds\n",
      "0 sentences skipped\n"
     ]
    }
   ],
   "source": [
    "nl_text.draw_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 miwamoto miwamoto 809M Dec 19 03:29 ./shuffled_files/nl_shuffled.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Quick check - shuffled file size\n",
    "!ls -lh ./shuffled_files/nl_shuffled.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nl_tesseract nl_wordt nl_tegenwoordig nl_ontwikkeld nl_door nl_google nl_en nl_uitgegeven nl_onder nl_de nl_apache nl_- nl_licentie nl_2.0 nl_.\r\n",
      " nl_in nl_ronde nl_2 nl_maakte nl_felipe nl_aguilar nl_64 nl_waarna nl_hij nl_samen nl_met nl_george nl_murray nl_aan nl_de nl_leiding nl_stond nl_.\r\n",
      " nl_de nl_eigenaar nl_van nl_het nl_150 nl_miljoen nl_dollar nl_kostende nl_gebouw nl_is nl_bentleyforbes nl_.\r\n",
      " nl_taxonomie nl_tipularia nl_wordt nl_tegenwoordig nl_samen nl_met nl_de nl_geslachten nl_calypso nl_en nl_corallorhiza nl_en nl_nog nl_enkele nl_ander nl_tot nl_de nl_tribus nl_calypsoeae nl_gerekend nl_.\r\n",
      " nl_pierre nl_hazette nl_( nl_marneffe nl_, nl_15 nl_maart nl_1939 nl_) nl_is nl_een nl_belgisch nl_politicus nl_voor nl_de nl_prl nl_.\r\n"
     ]
    }
   ],
   "source": [
    "!head -n5 ./shuffled_files/nl_shuffled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove files if not needed\n",
    "!rm ./split_files/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Japanese Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/miwamoto/Data/ja/full.txt\n"
     ]
    }
   ],
   "source": [
    "print(FULL_JA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Italian full corpus\n",
    "ja_text = Corpus(FULL_JA, 'ja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to split -  118.577906 seconds\n",
      "2096 files written\n"
     ]
    }
   ],
   "source": [
    "# Split full file using min_sentence_length\n",
    "\n",
    "min_sentence_length = 3\n",
    "ja_text.split_file(min_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2096\n"
     ]
    }
   ],
   "source": [
    "print(ja_text.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1048576)\n",
      "Time to shuffle -  5.354531 seconds\n",
      "0 sentences skipped\n"
     ]
    }
   ],
   "source": [
    "ja_text.draw_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 miwamoto miwamoto 390M Dec 18 22:26 ./shuffled_files/ja_shuffled.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Quick check - shuffled file size\n",
    "!ls -lh ./shuffled_files/ja_shuffled.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ja_ノー ja_ブル ja_節 ja_北米 ja_西部 ja_高地\r\n",
      " ja_江川 ja_大会 ja_通算 ja_60 ja_奪 ja_三振 ja_記録\r\n",
      " ja_1970 ja_年 ja_空気 ja_ばね ja_式 ja_車体 ja_傾斜 ja_制御 ja_装置 ja_試験 ja_行う ja_時 ja_クハ ja_1658 ja_使用 ja_台車 ja_空気 ja_ばね ja_式 ja_振り子 ja_台車 ja_fs ja_080 ja_形 ja_採用 ja_三菱 ja_三菱電機 ja_電機 ja_自動 ja_振子 ja_制御 ja_装置 ja_組み合わせる\r\n",
      " ja_2001 ja_年 ja_10 ja_月 ja_2002 ja_年 ja_9 ja_月 ja_2 ja_名 ja_後継 ja_番組\r\n",
      " ja_江東 ja_区立 ja_東川 ja_小学校\r\n"
     ]
    }
   ],
   "source": [
    "!head -n5 ./shuffled_files/ja_shuffled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove files if not needed\n",
    "!rm ./split_files/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine corpora for bilingual text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_bilingual(en_text, it_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_bilingual(en_text, es_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_bilingual(en_text, fr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_bilingual(en_text, ja_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bilingual(en_text, nl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " en_reception en_the en_allmusic en_review en_by en_steve en_huey en_awarded en_the en_album en_5 en_stars en_and en_calling en_it en_\" en_a en_superbly en_sensuous en_blend en_of en_lusty en_blues en_swagger en_and en_achingly en_romantic en_ballads en_... en_a en_quiet en_, en_sorely en_underrated en_masterpiece en_\" en_.\r\n",
      " nl_tesseract nl_wordt nl_tegenwoordig nl_ontwikkeld nl_door nl_google nl_en nl_uitgegeven nl_onder nl_de nl_apache nl_- nl_licentie nl_2.0 nl_.\r\n",
      " en_astrid en_is en_a en_member en_of en_the en_crimean en_royal en_knights en_.\r\n",
      " nl_in nl_ronde nl_2 nl_maakte nl_felipe nl_aguilar nl_64 nl_waarna nl_hij nl_samen nl_met nl_george nl_murray nl_aan nl_de nl_leiding nl_stond nl_.\r\n",
      " en_although en_it en_ran en_reasonably en_well en_, en_the en_engine en_was en_fuel en_inefficient en_, en_extremely en_noisy en_, en_tended en_to en_overheat en_and en_, en_if en_sufficient en_cooling en_water en_was en_not en_applied en_, en_seize en_up en_.\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 ./shuffled_files/en_nl_shuf.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with dictionary wordset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load wordset from dict\n",
    "pld = pd.read_csv(DPATH, sep='\\t', names = ['en', 'es'], dtype=str)\n",
    "en_set = set(pld.en.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356410"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look\n",
    "len(en_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create vocab\n",
    "en_vocab = Vocabulary(en_test.gen_tokens(), wordset = en_set, size = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13561\n",
      "13561\n"
     ]
    }
   ],
   "source": [
    "# take a look - NOTE: the test set has a small vocabulary!\n",
    "print(len(en_vocab.wordset))\n",
    "print(en_vocab.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing with full spanish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# real corpus\n",
    "es_data = Corpus(FULL_ES, 'es')\n",
    "es_set = set(pld.es.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# vocabulary trainied on full corpus\n",
    "es_vocab = Vocabulary(es_data.gen_tokens(), wordset = es_set, size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(es_vocab.wordset))\n",
    "print(es_vocab.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with full english data\n",
    "I am still having memory problems w/ the full file. I think the next steps are 1) try a larger instance and 2) go back to the paper to see if we really need all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Polyglot nonsense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ First time you run this on a new machine you'll need to make sure you've installed [polyglot](http://polyglot.readthedocs.io/en/latest/Installation.html):\n",
    "```\n",
    "sudo apt-get install libicu-dev\n",
    "pip install polyglot\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import polyglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACK! (see readme for more info on what I've tried to fix this )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from polyglot.detect import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from polyglot.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob = \"[[12]] Anarchism is often defined as a political philosophy which holds the state to be undesirable , unnecessary , or harmful .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "550px",
    "left": "0px",
    "right": "940px",
    "top": "110px",
    "width": "222px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
