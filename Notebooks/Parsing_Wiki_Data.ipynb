{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Wiki Data\n",
    "` w266 Final Project: Crosslingual Word Embeddings`   \n",
    "\n",
    "The code in this notebook and the supporting file __`parsing.py`__ build on the helper functions provided in the TensorFlow Word2Vec tutorial to develop a set of data handling functions for use with the data relevant to Duong et al's paper. Ideally I'll develop a scalable solution for tokenizing, prepending language indicators (eg. `en_`) and extracting sentences in two langauges to create traning data that includes sentences from two languages. I also hope to develop a batch iterator modeled after the one in A4. Depending on the available tools I may end up needing to look at using a distributed system (Spark?) for preprocessing the English corpus which is ~ 9GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/nlp/lib/python2.7/site-packages/matplotlib/font_manager.py:280: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "BASE = '~/Documents/MIDS/w266/Data' #'/home/mmillervedam/Data'\n",
    "PROJ = '~/Documents/MIDS/w266/FinalProject'#'/home/mmillervedam/ProjectRepo'\n",
    "FPATH_EN = BASE + '/test/wiki_en_10K.txt' # first 10000 lines from wiki dump\n",
    "FPATH_ES = BASE + '/test/wiki_es_10K.txt' # first 10000 lines from wiki dump\n",
    "#FULL_EN = BASE + '/en/full.txt'\n",
    "#FULL_ES = BASE + '/es/full.txt'\n",
    "DPATH = PROJ +'/XlingualEmb/data/dicts/en.es.panlex.all.processed'\n",
    "EN_IT = PROJ + '/XlingualEmb/data/mono/en_it.shuf.10k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desired Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_[[877881]]\r\n",
      "it_[[879362]]\r\n",
      "it_in it_un it_remoto it_passato it_aveva it_progettato it_, it_per it_conto it_dei it_demoniazzi it_silastici it_di it_striterax it_, it_una it_bomba it_in it_grado it_di it_collegare it_simultaneamente it_tutti it_i it_nuclei it_di it_tutte it_le it_stelle it_, it_creando it_così it_un'immensa it_supernova it_che it_avrebbe it_distrutto it_l'universo it_, it_secondo it_i it_desideri it_dei it_demoniazzi it_silastici it_.\r\n",
      "it_krikkitesi it_i it_krikkitesi it_sono it_una it_razza it_aliena it_che it_per it_miliardi it_di it_anni it_aveva it_vissuto it_senza it_la it_minima it_consapevolezza it_dell'esistenza it_di it_altri it_mondi it_o it_altre it_specie it_.\r\n",
      "en_as en_the en_patron en_of en_delphi en_( en_pythian en_apollo en_) en_, en_apollo en_was en_an en_oracular en_god en_— en_the en_prophetic en_deity en_of en_the en_delphic en_oracle en_.\r\n",
      "it_all'inizio it_del it_2006 it_ha it_pubblicato it_il it_suo it_primo it_singolo it_solista it_, it_nell'angolo it_, it_con it_la it_partecipazione it_dello it_stesso it_zero it_, it_che it_ha it_collaborato it_anche it_alla it_stesura it_del it_testo it_.\r\n",
      "en_medieval en_muslim en_scholars en_regularly en_described en_aristotle en_as en_the en_\" en_first en_teacher en_\" en_.\r\n",
      "it_[[876688]]\r\n",
      "en_roman en_gladiatorial en_games en_often en_referenced en_classical en_mythology en_, en_and en_this en_seems en_to en_reference en_achilles en_' en_fight en_with en_penthesilea en_but en_gives en_it en_an en_extra en_twist en_of en_achilles en_being en_\" en_played en_\" en_by en_a en_woman en_.\r\n",
      "it_l'arrangiamento it_è it_opera it_di it_demo it_morselli it_.\r\n"
     ]
    }
   ],
   "source": [
    "# take a look at what Duong et al trained on for reference\n",
    "!head -n 10 {EN_IT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ There are no UNK tokens here and punctuation is included as its own token. However words are lowercased and the language marker is prepended. Also note that sentences from the two languages have been shuffled together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Code\n",
    "I've put the parsing functions in their own python script for ease of access and shared editing. The scrips can be found in the shared repo at: __`/Notebooks/parsing.py`__. Here's a quick overview of the methods it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing import Corpus, Vocabulary, batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Class with helper methods to read from a Corpus.\n",
      "    Intended to facillitate working with multiple corpora at once.\n",
      "    Init Args:\n",
      "        path - (str) filepath of the raw data\n",
      "        lang - (str) optional language prefix to prepend when reading\n",
      "    Methods:\n",
      "        gen_tokens - generator factory for tokens in order\n",
      "        gen_sentences - generator factory for sentences in order\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Corpus.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This class is based heavily on code provided in a4 of MIDS w266, Fall 2017.\n",
      "    Init Args:\n",
      "        tokens    - iterable of tokens to count\n",
      "        wordset   - (optional) limit vocabulary to these words\n",
      "        size      - (optional) integer, number of vocabulary words\n",
      "    Attributes:\n",
      "        self.index   - dictionary of {id : type} \n",
      "        self.size    - integer, number of words in total\n",
      "        self.types   - dictionary of {type : id}\n",
      "        self.wordset - set of types\n",
      "    Methods:\n",
      "        self.to_ids(words) - returns list of ids for the word list\n",
      "        self.to_words(ids) - returns list of words for the id list\n",
      "        self.sentence_to_ids(sentence) - returns list of ids with start & end\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Vocabulary.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function to iterate repeated over a corpus delivering\n",
      "    batch_size arrays of ids and context_labels for CBOW.\n",
      "    \n",
      "    Args:\n",
      "        corpus - an instance of Corpus()\n",
      "        vocabulary - an instance of Vocabulary()\n",
      "        batch_size - int, number of words to serve at once\n",
      "        bag_window - context distance for CBOW training\n",
      "        max_epochs - int(default = None) stop generating\n",
      "        \n",
      "    Yields:\n",
      "        batch: np.array of dim: (batch_size, 2*bag_window - 1)\n",
      "               Represents set of context words.\n",
      "        labels: np.array of dim: (batch_size, 1)\n",
      "               Represents center words to predict/translate.\n",
      "        \n",
      "    WARNING: this generator will go on ad infinitum unless\n",
      "    you specify max_epochs or explicitly break.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(batch_generator.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english test corpus\n",
    "en_test = Corpus(FPATH_EN, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_[[12]]\n",
      "en_anarchism\n",
      "en_is\n",
      "en_often\n",
      "en_defined\n",
      "en_as\n",
      "en_a\n",
      "en_political\n",
      "en_philosophy\n",
      "en_which\n"
     ]
    }
   ],
   "source": [
    "# demo generator\n",
    "idx = 1\n",
    "for tok in en_test.gen_tokens():\n",
    "    print(tok)\n",
    "    idx += 1\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english vocabulary\n",
    "en_vocab = Vocabulary(en_test.gen_tokens(), size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look\n",
    "#en_vocab.types\n",
    "#en_vocab.index\n",
    "#en_vocab.wordset\n",
    "en_vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " en_anarchism en_is en_often en_defined en_as en_a en_political en_philosophy en_which en_holds en_the en_state en_to en_be en_undesirable en_, en_unnecessary en_, en_or en_harmful en_.\n",
      "[0, 209, 11, 93, 598, 13, 10, 186, 267, 28, 2, 3, 58, 9, 30, 2, 4, 2, 4, 25, 2, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "# translate the first sentence into indexes\n",
    "idx = 0\n",
    "for sent in en_test.gen_sentences():\n",
    "    if idx == 1:\n",
    "        print(sent)\n",
    "        print(en_vocab.sentence_to_ids(sent))\n",
    "        break\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT WINDOWS: [[1], [11, 93], [0, 209, 93, 598], [209, 11, 598, 13]]\n",
      "CENTER WORDS: [2, 209, 11, 93]\n",
      "CONTEXT WINDOWS: [[11, 93, 13, 10], [93, 598, 10, 186], [598, 13, 186, 267], [13, 10, 267, 28]]\n",
      "CENTER WORDS: [598, 13, 10, 186]\n",
      "CONTEXT WINDOWS: [[10, 186, 28, 2], [186, 267, 2, 3], [267, 28, 3, 58], [28, 2, 58, 9]]\n",
      "CENTER WORDS: [267, 28, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# demo batch iterator\n",
    "batch_size = 4\n",
    "bag_window = 2\n",
    "idx = 0\n",
    "for batch, labels in batch_generator(en_test, en_vocab, batch_size, bag_window):\n",
    "    print(\"CONTEXT WINDOWS:\", batch)\n",
    "    print(\"CENTER WORDS:\", labels)\n",
    "    idx += 1\n",
    "    if idx > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "['</s>'] --> ['<unk>']\n",
      "['en_is', 'en_often'] --> ['en_anarchism']\n",
      "['<s>', 'en_anarchism', 'en_often', 'en_defined'] --> ['en_is']\n",
      "['en_anarchism', 'en_is', 'en_defined', 'en_as'] --> ['en_often']\n",
      "Batch 2:\n",
      "['en_is', 'en_often', 'en_as', 'en_a'] --> ['en_defined']\n",
      "['en_often', 'en_defined', 'en_a', 'en_political'] --> ['en_as']\n",
      "['en_defined', 'en_as', 'en_political', 'en_philosophy'] --> ['en_a']\n",
      "['en_as', 'en_a', 'en_philosophy', 'en_which'] --> ['en_political']\n",
      "Batch 3:\n",
      "['en_a', 'en_political', 'en_which', '<unk>'] --> ['en_philosophy']\n",
      "['en_political', 'en_philosophy', '<unk>', 'en_the'] --> ['en_which']\n",
      "['en_philosophy', 'en_which', 'en_the', 'en_state'] --> ['<unk>']\n",
      "['en_which', '<unk>', 'en_state', 'en_to'] --> ['en_the']\n"
     ]
    }
   ],
   "source": [
    "# demo batch iterator w/ readible format\n",
    "batch_size = 4\n",
    "bag_window = 2\n",
    "idx = 0\n",
    "for batch, labels in batch_generator(en_test, en_vocab, batch_size, bag_window):\n",
    "    print(\"Batch %s:\"%(idx+1))\n",
    "    for context, wrd in zip(batch,labels):\n",
    "        print(en_vocab.to_words(context), \"-->\", en_vocab.to_words([wrd]))\n",
    "    idx += 1\n",
    "    if idx > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`QUESTION:`__ What do we do for context w/ the start and end words? I need to go back and check Mona's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10000  259807 1461734 /home/mmillervedam/Data/test/wiki_en_10K.txt\r\n"
     ]
    }
   ],
   "source": [
    "# confirm that batch generator will reload\n",
    "!wc {FPATH_EN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filmography Tarkovsky is mainly known as a director of films .\r\n"
     ]
    }
   ],
   "source": [
    "# last sentence\n",
    "!tail -n 1 {FPATH_EN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12]]\r\n",
      "Anarchism is often defined as a political philosophy which holds the state to be undesirable , unnecessary , or harmful .\r\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "!head -n 2 {FPATH_EN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ `~65001 batches per epoch` (in this test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 65000:\n",
      "['<unk>', 'en_tarkovsky', '<unk>', 'en_known'] --> ['en_is']\n",
      "['en_tarkovsky', 'en_is', 'en_known', 'en_as'] --> ['<unk>']\n",
      "['en_is', '<unk>', 'en_as', 'en_a'] --> ['en_known']\n",
      "['<unk>', 'en_known', 'en_a', '<unk>'] --> ['en_as']\n",
      "Batch 65001:\n",
      "['en_known', 'en_as', '<unk>', 'en_of'] --> ['en_a']\n",
      "['en_as', 'en_a', 'en_of', 'en_films'] --> ['<unk>']\n",
      "['en_a', '<unk>', 'en_films', 'en_.'] --> ['en_of']\n",
      "['<unk>', 'en_of', 'en_.', '</s>'] --> ['en_films']\n",
      "Batch 65002:\n",
      "['en_of', 'en_films', '</s>'] --> ['en_.']\n",
      "['</s>'] --> ['<unk>']\n",
      "['en_is', 'en_often'] --> ['en_anarchism']\n",
      "['<s>', 'en_anarchism', 'en_often', 'en_defined'] --> ['en_is']\n",
      "Batch 65003:\n",
      "['en_anarchism', 'en_is', 'en_defined', 'en_as'] --> ['en_often']\n",
      "['en_is', 'en_often', 'en_as', 'en_a'] --> ['en_defined']\n",
      "['en_often', 'en_defined', 'en_a', 'en_political'] --> ['en_as']\n",
      "['en_defined', 'en_as', 'en_political', 'en_philosophy'] --> ['en_a']\n"
     ]
    }
   ],
   "source": [
    "# print the 64952-4rd batch (should be the same as above)\n",
    "idx = 0\n",
    "for batch, labels in batch_generator(en_test, en_vocab, 4, 2):\n",
    "    idx += 1\n",
    "    if idx < 65000:\n",
    "        continue\n",
    "    elif idx > 65003:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Batch %s:\"%(idx))\n",
    "        for context, wrd in zip(batch,labels):\n",
    "            print(en_vocab.to_words(context), \"-->\", en_vocab.to_words([wrd]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with dictionary wordset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load wordset from dict\n",
    "pld = pd.read_csv(DPATH, sep='\\t', names = ['en', 'es'], dtype=str)\n",
    "en_set = set(pld.en.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356410"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look\n",
    "len(en_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create vocab\n",
    "en_vocab = Vocabulary(en_test.gen_tokens(), wordset = en_set, size = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13561\n",
      "13561\n"
     ]
    }
   ],
   "source": [
    "# take a look - NOTE: the test set has a small vocabulary!\n",
    "print(len(en_vocab.wordset))\n",
    "print(en_vocab.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing with full spanish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real corpus\n",
    "es_data = Corpus(FULL_ES, 'es')\n",
    "es_set = set(pld.es.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# vocabulary trainied on full corpus\n",
    "es_vocab = Vocabulary(es_data.gen_tokens(), wordset = es_set, size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(es_vocab.wordset))\n",
    "print(es_vocab.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with full english data\n",
    "I am still having memory problems w/ the full file. I think the next steps are 1) try a larger instance and 2) go back to the paper to see if we really need all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Polyglot nonsense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ First time you run this on a new machine you'll need to make sure you've installed [polyglot](http://polyglot.readthedocs.io/en/latest/Installation.html):\n",
    "```\n",
    "sudo apt-get install libicu-dev\n",
    "pip install polyglot\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import polyglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACK! (see readme for more info on what I've tried to fix this )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.detect import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob = \"[[12]] Anarchism is often defined as a political philosophy which holds the state to be undesirable , unnecessary , or harmful .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "550px",
    "left": "0px",
    "right": "940px",
    "top": "110px",
    "width": "222px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
